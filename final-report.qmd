---
title: "Bayesian Methods of Variable Selection"
author: "Dav King"
format: 
  pdf:
    echo: false
    warning: false
    message: false
bibliography: references.bib
csl: apa.csl
header-includes:
  - \usepackage{amssymb}
  - \usepackage{amsmath}
  - \usepackage{mathtools}
---

```{r}
#| label: libraries

library(VGAM)
library(tidyverse)
library(latex2exp)
library(viridis)
library(knitr)
library(kableExtra)
```


# Introduction

Consider the traditional regression setting:


\begin{equation}
  \mathbf{Y} = \beta_0 \mathbf{1} + \mathbf{X} \beta + \epsilon
\end{equation}


In this setting, $\beta$ is the $p \times 1$ vector of predictors, $\mathbf{X}$ is the $n \times p$ feature matrix, $\mathbf{Y}$ is the $n \times 1$ vector of responses, $\beta_0$ is the intercept term, and $\epsilon$ represents random, irreducible, zero-mean noise. This process is known as the **data-generative model**, i.e., the mechanism by which we believe the response variable $\mathbf{Y}$ is generated. Often, we have many potential predictors, but believe that only a small handful are in the true data-generative model. This is a concept known as **sparsity**, where the true predictors are **signals** and the remaining predictors are **noise**. Our goal is to separate the signals from the noise and identify the underlying data-generative model.

To accomplish this, we wish to control the number of predictors by performing **variable selection**. By identifying the subset of true signals, we can reduce computational complexity, minimize model variance, and improve interpretability. This is especially useful in situations where we care more about interpretation than prediction: An understanding of the data-generative model allows us to determine how response variables are created. As computational power and data availability skyrocket, variable selection has developed highly meaningful applications in numerous fields, such as genomics and the social sciences.

Separating signals from noise is not a small feat, however. The naive answer is to try all possible combinations of variables while solving this problem, known as best subset selection (note: we typically exclude $\beta_0$ from the variable selection process). However, this method is computationally intractable except in situations where $p$ is very small. For a model with $p$ parameters, there are $2^p$ possible model formulations, composing the model space $\mathcal{M}$. Searching over all of $\mathcal{M}$ quickly becomes infeasible as $p$ increases. Another possible solution is the use of greedy algorithms, such as forward- and back-selection. However, these processes can miss important effects that arise only when related predictors are included in the model together. Other methods that penalize predictors, such as LASSO regression, have become standard across industries, but they are often not aggressive enough to select a sufficiently small subset of predictors or lead to bias in coefficient estimates.

One very active domain of variable selection research draws from the Bayesian perspective on statistics. At the core of Bayesian statistics is Bayes' theorem, which describes how beliefs about a probability should be updated according to new information:

\begin{equation}
  \mathbb{P}(B | A) = \frac{\mathbb{P}(A | B) \mathbb{P}(B)}{\mathbb{P}(A)}
\end{equation}

As discussed below, Bayesian statistics allows the statistician to impose some structure on the model based on their prior beliefs about its format, which can help stabilize calculations and ensure an output of the desired form. It also can search over high-probability regions of the model space much more effectively than frequentist approaches. Many papers have compared different aspects of Bayesian variable selection to one another [@lu2022; @rockova2013], but few if any have compared across the different branches of Bayesian variable selection. Thus, this paper introduces a variety of Bayesian variable selection methods, and performs a brief experiment to compare across these branches.


## Research Questions

The central research questions for this paper are as follows:

\begin{enumerate}
  \item What methods of Bayesian variable selection exist, and how do they relate?
  \item Which methods perform best in each of the following settings? \begin{align*}
  &n >> p \\
  &n > p \\
  &p > n \\
  &p >> n 
  \end{align*}
  \item What are the computation time/accuracy tradeoffs?
\end{enumerate}


# Bayesian Modeling

## Introduction

\begin{equation}
  p(\beta | \mathbf{Y}) = \frac{p(\mathbf{Y} | \beta) p(\beta)}{\int_\beta p(\mathbf{Y} | \beta) p(\beta) d\beta}
\end{equation}

In a Bayesian modeling setting, we use Bayes' theorem to generate a probability distribution of the model parameters. In this equation, $p(\mathbf{Y} | \beta)$ is the data generative model, or the process by which we believe the response variable is generated. This is generally represented with a likelihood function. $p(\beta)$ is the prior distribution, representing our beliefs about the structure of $\beta$ before viewing any of the data. This is a useful concept in Bayesian variable selection, as we will see later. $\int_\beta p(\mathbf{Y} | \beta) p(\beta) d\beta$ is the normalizing constant which ensures the posterior distribution integrates to 1, and it can usually be absorbed into proportionality and thus omitted from these calculations. Finally, $p(\beta | \mathbf{Y})$ is the posterior distribution, reflecting our beliefs about $\beta$ after seeing the data. In other words, the posterior distribution reflects an update in our beliefs about $\beta$ from our prior beliefs, based on the data likelihood function.


## Markov Chain Monte Carlo

Let $\beta$ and $\mathbf{Y}$ be as defined above, and let $\theta$ represent all other parameters in our modeling setup. We would like to calculate the joint posterior distribution $p(\beta, \mathbf{Y}, \theta)$. In many cases, posterior distributions are difficult or impossible to compute by hand. Thus, we estimate the joint posterior by sampling repeatedly from the full conditional distributions (i.e., the distribution of one parameter given everything else) of all involved parameters. This is known as a **Markov Chain Monte Carlo (MCMC)** algorithm. It is conducted as follows: Initialize values $\beta^{(1)}$, $\mathbf{Y}^{(1)}$, and $\theta^{(1)}$. Then, at every step $s$, 

\begin{enumerate}
  \item Sample $\beta^{(s + 1)}$ from the full conditional $p(\beta | \mathbf{Y}^{(s)}, \theta^{(s)})$.
  \item Sample $\mathbf{Y}^{(s + 1)}$ from the full conditional $p(\mathbf{Y} | \beta^{(s + 1)}, \theta^{(s)})$.
  \item Sample $\theta^{(s + 1)}$ from the full conditional $p(\theta | \beta^{(s + 1)}, \mathbf{Y}^{(s + 1)})$.
\end{enumerate}

After repeating this algorithm for long enough, it will eventually converge to the joint posterior. Usually, iterations on the order of tens of thousands are sufficient for the sampler to converge to a reasonable estimate of the joint posterior. The order in which these variables are sampled does not matter, and can be shuffled at different steps. The Markov Chain is memoryless - each sample only depends on the step immediately before it.

There are many different ways that values can be sampled in this process, but two of the most famous algorithms are Gibbs and Metropolis-Hastings. In a Gibbs sampler, the full conditional of each variable is known and easy to compute, and so new values are simply drawn from that full conditional and accepted with probability 1. In a Metropolis-Hastings algorithm, the full conditional of each variable is harder to draw from, so the following steps are used:

\begin{enumerate}
  \item Propose a new value of the parameter of interest from a proposal distribution, usually centered around the current parameter value.
  \item Calculate the likelihood ratio $\alpha = \ell(\text{proposal}) / \ell(\text{current})$.
  \item Accept the proposal with probability $\text{min}\{1, \alpha\}$. Otherwise, retain the current value.
\end{enumerate}

With a Metropolis-Hastings sampler, if the proposed value is more likely than the current value, it is always accepted; otherwise, it is accepted with probability corresponding to the likelihood ratio. This ensures that the sampler continues to explore parameter space, while making progress towards the regions of high posterior density. Note that it is possible to use both Gibbs and Metropolis-Hastings sampling for different parameters within the same MCMC sampler.


# Variable Selection: Non-Prior Methods

The first branch of Bayesian variable selection is general methods around the problem setup, without modifying the priors on the $\beta$s. These still take place within the framework of MCMC sampling, but impose additional structure or variables to reach the desired outcome.


## Bayesian Model Selection

Define a binary variable $\gamma = \gamma_1, \dots, \gamma_p$. This characterizes a sub-model $\mathcal{M}_\gamma$,

\begin{equation}
  \mathbf{Y} = \beta_0 \mathbf{1} + \mathbf{X}_\gamma \beta_\gamma + \epsilon,
\end{equation}

where $\gamma_j = 1$ indicates that $\beta_j$ is included in $\mathcal{M}_\gamma$, and $\gamma_j = 0$ indicates that $\beta_j$ is excluded [@lu2022]. The different specifications of $\gamma$ characterize the entire model space $\mathcal{M}$. We can place a variety of priors on $\gamma$, but Bernoulli priors are the most common.

In our MCMC sampler, we sample $\gamma$ along with $\beta$ and $\mathbf{Y}$. Typically, we sample $\gamma$ with a procedure like Metropolis-Hastings, so that we can accept or reject the new choice of $\gamma$ based on the model likelihood ratios (however, even Gibbs sampling will converge). There are a number of different criteria that can be used for comparing models, such as the Bayesian information criterion (BIC) or Chib's marginal likelihood estimator [@lu2022].

Regardless of how new values of $\gamma$ are sampled, the sampler identifies probable models intuitively: More probable models will appear in the posterior distribution of $\gamma$ more frequently. In fact, this procedure often converges to one (or a few) very strong candidate models rapidly, even when only exploring a small fraction of model space. This makes Bayesian model selection a very useful procedure, since it is able to dramatically cut down on the amount of computation required to identify highly probable models (with some risk of failing to reach the areas of high posterior density, if the procedure is done poorly). This is also a very simple procedure, and it requires relatively little computation work, making it one of the fastest algorithms considered here. However, it does have its downsides: The selection of only one model specification may place too much emphasis on random noise in the data sample, while dropping other highly probable models from consideration.


## Bayesian Model Averaging

Bayesian model averaging follows the exact same procedure as BMS: Define $\gamma$ as a latent binary indicator variable, sample this along with $\beta$ and $\mathbf{Y}$, and find a posterior distribution of model probability. However, there is one key difference. In BMS, we identify and select a single model specification with the highest posterior probability. In BMA, we simply average over the coefficients of every sampled model, letting $\beta_j = 0$ if $\gamma_j = 0$. This results, more-or-less, in posterior estimates for $\beta$ that are weighted by the probability of each model specification $\gamma$. Though unintuitive, this procedure often results in highly accurate estimates of the true data-generative model [@hoeting1999].

BMA, like BMS, has relatively little computational demand, and is widely applicable in a variety of situations. BMA also has the additional benefit of weighting $\beta$ coefficients by model probabilities, allowing the model specification to be influenced by a variety of highly probable model specifications instead of just one. However, there may be situations where the averaging does not produce the desired result, depending on the circumstances of the data. It may also have difficulty converging to the correct values if initialized poorly. This procedure is useful for its low computational demand and incorporation of information from the full (sampled) model space.


## Bayesian Subset Selection

Bayesian subset selection is very similar to the frequentist problem of best subset selection, with a few key differences. First, instead of focusing on the singular best subset, BSS identifies a family of "acceptable" subsets, or those within some tolerable $\epsilon$ of the best subset. This family is more likely to include the subset representing the true data generative model, which could be obscured by patterns in the random noise. Second, it draws response estimates from the posterior predictive distribution of a Bayesian model, rather than a simple OLS regression model. This allows for a better quantification of uncertainty in the variable selection process, and establishes the optimal coefficients for any subset of predictors. Third, it uses a more efficient branch-and-bound algorithm to explore only the promising subsets, and only considers subsets up to a maximum size. Fourth, it summarizes across the acceptable subsets to generate its "best" subset, including the "best" (by cross-validation) predictive subset, the smallest acceptable subset, and metrics on variable co-importance. All of these are improvements over frequentist best subset selection [@kowal2022].

However, as promising as BSS is with its improvements on the frequentist optimal procedure, it still has drawbacks. First, even with the computational benefits of the branch-and-bound algorithm, it is still computationally intractible above a certain number of predictors, which restricts the algorithm to subsets of an arbitrary maximum size (potentially eliminating the true best subsets from consideration). Since there is no efficient implementation of this in R, it is excluded from this analysis. Additionally, this means it is not much more computationally tractable than frequentist best subset selection, where the branch-and-bound algorithm can also be applied. This puts BSS's computational complexity far behind BMS and BMA, which do not need to search over the full model space to identify highly probable models. Still, the Bayesian BSS method is very promising and offers a lot of advantages. Research in the coming years will likely make BSS one of the top methods of Bayesian variable selection, particularly as searching over candidate subsets becomes more efficient.


# Variable Selection: Prior-Based Methods

The second branch of Bayesian variable selection follows a specific framework: Suppose we have some prior beliefs about the structure of our $\beta$s. Specifically, we believe that not all of them are included in the true data-generative model, and many of their coefficients must be zero. We can place a prior distribution on the $\beta$s accordingly, reflecting this belief. This produces a posterior distribution of our desired form, where many of the $\beta$s are set to (near) zero. We can also introduce additional structure with this method, such as placing priors on different groups of $\beta$s, but that is beyond the scope of this paper.

There are two major classes of priors used for variable selection: spike-and-slab priors and shrinkage priors.


## Spike-and-Slab Priors

A spike-and-slab prior is a mixture distribution: A combination of two distributions, where $\beta_j$ is drawn from one or the other according to some Bernoulli probability. Thus, a latent indicator variable $\gamma$ is still used, but instead of explicitly denoting whether a specific $\beta_j$ is included in the model, it denotes whether the prior distribution for $\beta_j$ is drawn from the spike or the slab distribution.

Spike-and-slab priors have the general form

\begin{equation}
  \beta_j | \gamma_j \sim (1 - \gamma_j) \phi_0(\beta_j) + \gamma_j \phi_1(\beta_j),
\end{equation}

where $\phi_0(\beta_j)$ is a concentrated "spike" distribution that pulls some of the $\beta$s to (near) zero, while $\phi_1(\beta_j)$ is a diffuse "slab" distribution that allows the remaining $\beta$s to attain their true coefficients [@lu2022]. By sampling $\gamma$ in the MCMC sampler, this specification returns a joint posterior where the signal coefficients are near their true values while the noise coefficients are near zero, with posterior inclusion probabilities ($\gamma$) that reflect the most probable model specifications. This allows for a little more uncertainty in the estimation of coefficients than BMS does, since near-zero coefficients do not have to be pulled to exactly zero. However, there is still the disadvantage of needing to include $\gamma$ to sample over the model space $\mathcal{M}$, which could struggle to converge to regions of true high posterior density.


### Stochastic Search Variable Selection

The best-known implementation of the spike-and-slab prior is stochastic search variable selection [@george1993]. This follows the procedure outlined above, embedding the full model selection process in a hierarchical Bayesian MCMC scheme. In SSVS, the prior is represented as a mixture of normals:

\begin{equation}
  \beta_j | \gamma_j \sim (1 - \gamma_j) N(0, \tau_j^2) + \gamma_j N(0, c_j^2 \tau_j^2)
\end{equation}

Prior distributions can be placed on $\tau$ and $c$, or they can be held constant. A visual depiction of the spike-and-slab distribution is shown in Figure 1, though the spike is usually more concentrated and the slab more diffuse than depicted.

```{r}
#| label: spike and slab distribution

tau <- 1
c <- 3

set.seed(481)
spike <- rnorm(100000, 0, tau^2)
slab <- rnorm(100000, 0, c^2 * tau^2)

data.frame(slab, spike) %>% 
  pivot_longer(everything()) %>% 
  mutate(name = factor(name, levels = c("slab", "spike"))) %>% 
  ggplot(aes(x = value, fill = name)) +
  geom_density(alpha = 0.5) +
  theme_bw() +
  labs(x = TeX("$\\beta$"), y = "Density", fill = "Distribution",
       title = "Plot of SSVS Spike-and-Slab Distribution",
       subtitle = TeX("$\\tau = 1, c = 3$"),
       caption = "Figure 1") +
  theme(plot.title = element_text(hjust = 0.5),
        plot.subtitle = element_text(hjust = 0.5),
        panel.grid = element_blank(),
        legend.position = "bottom") +
  scale_fill_viridis(discrete = TRUE, direction = -1) +
  coord_cartesian(xlim = c(-30, 30))
```


### Normal Mixture of Inverse Gamma

The normal mixture of inverse gamma expands on SSVS by moving the spike-and-slab formulation down one level in the hierarchy, to place it on the variance instead [@albert1993; @rockova2012]. The hierarchal specification of this model is

\begin{align}
  \beta_j | \eta_j &\sim N(0, \eta_j) \\
  \eta_j | \gamma_j &\sim (1 - \gamma_j) \text{IG} \left ( a, \frac{\nu_0}{b} \right ) + \gamma_j \text{IG} \left ( a, \frac{\nu_1}{b} \right ),
\end{align}

where IG is the inverse-gamma distribution, and $\nu_1 >> \nu_0$. A prior, typically Bernoulli, is placed on $\gamma$. By moving the sampling down one level in the hierarchy, we avoid having to put as much weight on the hyperparameters $c$ and $\tau$, often leading to better performance [@lu2022]. However, due to a lack of an efficient implementation in R, we omit NMIG from the experiment in this paper.


## Shrinkage Priors

Unlike spike-and-slab priors, which are mixture distributions, shrinkage priors are single-point, continuous distributions. Their goal is to pull some of the $\beta$s towards zero, while minimally penalizing the signal coefficients. Shrinkage priors have the benefit of being fully continuous, which means that they do not require a latent variable $\gamma$ to search over $\mathcal{M}$, and this makes shrinkage priors less likely to leave highly probable model specifications unexplored. However, this has its tradeoffs: approaches using shrinkage priors often have dramatically increased computation time, and research has suggested that they may not select variables aggressively enough or may overly penalize the signal coefficients in the process (i.e., they may be less effective at discerning between signal and noise predictors).


### LASSO

In frequentist statistics, LASSO regression is a well-know regression setting that expands on OLS regression to penalize the $\ell_1$ norm of the $\beta$s, using a hyperparameter $\lambda$ to control the shrinkage:

\begin{equation}
  \underset{\beta}{\text{min}} (\mathbf{Y} - \mathbf{X}\beta)^T (\mathbf{Y} - \mathbf{X} \beta) + \lambda \sum_{j = 1}^{p} |\beta_j|
\end{equation}

In the Bayesian analogue to LASSO regression [@park2008], we place a conditional Laplace prior on $\beta$:

\begin{equation}
  p(\beta | \sigma^2) = \prod_{j = 1}^{p} \frac{\lambda}{2\sqrt{\sigma^2}} \text{exp} \left \{ \frac{-\lambda |\beta_j|}{\sqrt{\sigma^2}} \right \}
\end{equation}

This results in the hierarchical specification

\begin{align}
  \beta_j | \tau_j &\sim N(0, \sigma^2 \tau_j^2) \\
  \tau_j | \lambda &\sim \text{exp}(\lambda^2 / 2)
\end{align}

Under this distribution, there is a high probability of the $\beta$s falling at or near zero, which increases with $\lambda$ as shown in Figure 2. In this setting, we can also place a prior on $\lambda$ and sample it along with $\beta$. The result is a posterior where many of the $\beta$s are at or near 0, and careful prior specification can ensure that the posterior distribution is unimodal [@park2008]. This can also be generalized to other cases, such as bridge regression.

```{r}
#| label: LASSO distribution

lambda <- c(0.5, 1, 2, 4)

N_samples <- 100000

lasso_values <- matrix(NA, nrow = N_samples, ncol = length(lambda))

for(l in 1:length(lambda)){
  lasso_values[,l] <- rlaplace(N_samples, 0, 1 / lambda[l])
}


data.frame(lasso_values) %>%
  pivot_longer(everything(), names_to = "lambda_number") %>% 
  mutate(lambda_number = as.integer(substring(lambda_number, 2))) %>% 
  mutate(lambda = lambda[lambda_number]) %>% 
  mutate(lambda = factor(lambda)) %>% 
  mutate(lambda = fct_rev(lambda)) %>% 
  ggplot(aes(x = value, fill = lambda)) +
  geom_density(alpha = 0.5) +
  coord_cartesian(xlim = c(-10, 10)) +
  theme_bw() +
  labs(x = TeX("$\\beta$"), y = "Density", fill = TeX("$\\lambda$"),
       title = "Plot of Laplace (LASSO Prior) Distribution",
       caption = "Figure 2") +
  theme(plot.title = element_text(hjust = 0.5),
        panel.grid = element_blank(),
        legend.position = "bottom") +
  scale_fill_viridis(discrete = TRUE, option = "B") +
  guides(fill = guide_legend(reverse = TRUE))
```


### Normal-Gamma

While the Bayesian LASSO was one of the earliest methods of Bayesian variable selection and remains one of the best-used, it is not without its criticisms. The use of only a single hyperparameter reduces flexibility, meaning that either the LASSO will fail to perform adequate variable selection or the signal coefficients will be unnecessarily reduced. Thus, the normal-gamma prior arises [@griffin2010]. The normal-gamma specification is:

\begin{align}
  \beta_j | \tau_j &\sim N(0, \tau_j^2) \\
  \tau_j^2 | \lambda, \xi &\sim \text{gamma}(\lambda, 1/(2\xi^2))
\end{align}

The introduction of the extra hyperparameter $\xi$ and the gamma prior on the variance $\tau_j^2$ allows this distribution to have a lot of mass close to zero, while maintaining heavy tails for the coefficients. This is generally an improvement on LASSO, though it does require the introduction of an extra parameter (potentially increasing computational complexity) and does not always outperform the LASSO in applied settings.


### Horseshoe

Taking a slightly different approach, horseshoe shrinkage priors fall into a class known as **global-local** shrinkage priors [@carvalho2010]. These priors use a global hyperparameter to shrink all coefficients towards zero, while maintaining a local hyperparameter to adjust the scale of shrinkage for some coefficients at the local level. The hierarchical representation of the regression model under a horseshoe prior is

\begin{align}
  \beta_j | \eta_j &\sim N(0, \eta_j^2) \\
  \eta_j | \tau &\sim C^+(0, \tau) \\ 
  \tau | \sigma &\sim C^+(0, \sigma),
\end{align}

where $C^+$ is a half-Cauchy distribution (restricted to positive values). In this specification, $\eta_j$ is the local shrinkage parameter, while $\tau$ is the global shrinkage parameter. With some integration, this results in a shrinkage coefficient $\kappa_j = 1 / (1 + \eta_j^2)$. The half-Cauchy prior on $\eta_j$ implies a Beta(1/2, 1/2) marginal distribution for $\kappa_j$, which gives the horseshoe its name.

```{r}
#| label: horseshoe distribution
#| fig-align: center

set.seed(523)
ggplot(mapping = aes(x = rbeta(1000000, 1/2, 1/2))) +
  geom_density(fill = "darkgrey", alpha = 0.5) +
  theme_bw() +
  labs(x = TeX("$\\kappa_j$"), y = "Density",
       title = "Plot of Horseshoe Shrinkage Coefficient Distribution",
       caption = "Figure 3") +
  theme(plot.title = element_text(hjust = 0.5),
        panel.grid = element_blank())
```

The distribution for $\kappa_j$, depicted in Figure 3, places a lot of mass near 0 (implying near-zero shrinkage, maintaining signals) and 1 (implying near-total shrinkage, reducing noise). This specification forces most of the coefficients to be pulled to zero, while allowing the remaining coefficients to reach their true values. It is a slightly different specification than LASSO/NG, but one that is widely used and often performs better in practice.

### Other Approaches

Several other shrinkage priors have been proposed. The horseshoe+ prior places another latent variable in the hierarchy for another level of local shrinkage [@bhadra2017]. The Dirichlet-Laplace prior draws local shrinkage coefficients from the joint Dirichlet distribution [@bhattacharya2014]. The SSLASSO prior hybridizes spike-and-slab and shrinkage priors by drawing from a mixture of Laplace distributions [@rockova2018]. While all of these approaches are promising and have their own strengths, they are excluded from further analysis here for simplicity.


# Variable Selection: Expectation-Maximization Variable Selection

## Overview

The third branch of Bayesian variable selection takes a completely different approach. Though all previous methods use the MCMC algorithm, EMVS is an algorithm that is formulated differently. EMVS is highly useful in high dimensional settings ($p >> n$), since it runs in a tiny fraction of the time that MCMC requires and can effectively identify the high-probability sparse models [@rockova2013]. EMVS is anchored by the SSVS spike-and-slab approach [@george1993], but it can be extended into other settings as well.


## Algorithm

The EMVS algorithm is very straightforward. Begin by specifying the full Bayesian regression setting, typically using a spike-and-slab prior. Then, repeat two steps until convergence:

 - **Expectation (E) Step:** Calculate the posterior inclusion probabilities (i.e., expectations) for each $\beta_j$ given current parameter estimates.
 - **Maximization (M) Step:** Update the parameter estimates by maximizing the objective function of all model parameters, given the data.
 
More formal specifications of the equations required to execute this algorithm can be found in [@rockova2013]. In practice, this algorithm generally converges very quickly - in the experiment below, all specifications of the algorithm converged within 15 iterations.


## Deterministic Annealing

If the true posterior is multi-modal, it is very possible that poor initialization of the EMVS algorithm will lead to incorrect estimates of the posterior, as EMVS gets trapped in local modes. One potential solution to this is deterministic annealing. Instead of maximizing the objective function, we maximize a tempered version known as the negative free energy function. We can raise the "temperature", which smooths away the local modes of the negative free energy function, allowing only the true signals to shine through and giving us some robustness against poor initialization. As we progress through the algorithm, we lower the temperature progressively until its effects are completely removed, allowing us to view an equation that accurately resembles the true posterior [@rockova2013]. Some form of cross-validation is required to find the optimal values of initial temperature, but since the EMVS algorithm converges so quickly, this is still much faster than MCMC sampling in most cases.


# Experiment

## Methodology

In order to compare these methods against each other, I performed a brief experiment. This experiment was performed using synthetic data in order to be able to control and evaluate the experiment more effectively, though this does limit the generalizability of this experiment as real-world data may not match this setting. Data were generated using the `simulate_lm()` function from the R package `BayesSubsets` [@R-BayesSubsets], with some modifications to have a little more diversity in coefficients. All data were generated with $n = 100$, for the sake of runtime when performing this experiment (greater amounts of data would change performance somewhat, but since the signal-to-noise ratio was fixed in these models, it likely would not have proven to be a huge change). Details of the model specifications, including the total number of predictors, the total number of significant predictors, and the true coefficients, can be found in the table below. 

```{r}
#| label: model specification table

rbind(
  c(Model = "Model 1", n = 100, p = 10, p_sig = 3, Coef = "3, 2, -2"),
  c(Model = "Model 2", n = 100, p = 95, p_sig = 8, Coef = "3, 2, -2, 1, -1, -1, -1, -1"),
  c(Model = "Model 3", n = 100, p = 105, p_sig = 8, Coef = "3, 2, -2, 1, -1, -1, -1, -1"),
  c(Model = "Model 4", n = 100, p = 1000, p_sig = 8, Coef = "3, 2, -2, 1, -1, -1, -1, -1")
) %>% 
  kable()
```

In this experiment, I focused only on the sparse signal setting. I did also consider the dense signal setting, but all of these algorithms performed poorly in that setting, so they are omitted from further exploration. Partial findings from the dense signal setting can be found in Appendix B.

All analysis for this experiment was performed in R. I ran the modeling process for all four models with each of the variable selection methods. MCMC samplers discarded 1,000 burn-in samples and then were run for 10,000 or 30,000 iterations, depending on computational demand. Runtimes are only reported in terms of orders of magnitude, as getting accurate timing on these tests would require numerous trials. Where applicable, I used ROC curves to determine the optimal thresholds (in posterior inclusion probability) for each method, based on true $\beta$ values. While this is a luxury afforded to us in the synthetic setting that is not present in the real world, most of these methods have forms of thresholding based on intersection points in the posterior distribution. Thus, this is not an unreasonable extension, and it allows us to compare the upper limit of performance between these methods.

I evaluated these approaches on four criteria: MSE in terms of estimated coefficients for the true (nonzero) signal coefficients, MSE in terms of estimated coefficients for all (including zero) coefficients, MSE in terms of the response variable on the training set, and MSE in terms of the response variable on a testing set (data generated with the same specification and a different random seed). I also explored true/false positive/negative rates, for research problems in which one is more costly than another. All code to produce these results can be found in `experiment.qmd`, although some manipulation may be required to get the code to run sequentially. All results are printed as tables in Appendix A.


## Results

### Model 1: $n >> p$

This setting is not particularly interesting to look at. All methods performed perfectly, ran within seconds, and had very similar MSE rates. Since this is not the setting where we are most curious about these algorithms' performance, there is nothing much to say about which method performs best here.

### Model 2: $n > p$

In this setting, the methods begin to differentiate. First, while BMS, BMA, and SSVS still run on the order of seconds (and EMVS on the order of milliseconds), the shrinkage prior based approaches have already slowed down, taking several minutes to run. Second, BMA is more or less useless by this point - it wants to include far too many predictors in the model, and its response test MSE is alarmingly large, indicating overfitting. Overall, most other methods performed comparably in this setting. Horseshoe did the best job of selecting true positive variables, estimating overall coefficients, and minimizing Test MSE, while EMVS performed best at estimating the signal coefficients (though it also showed signs of overfitting on the training data).

### Model 3: $p > n$

Though there is a difference between "$n$ slightly bigger than $p$" and "$p$ slightly bigger than $n$", there was very little change in the performance of these methods between the two settings. With more predictors, the shrinkage prior methods got closer to the performance of EMVS at predicting true signal coefficients, and continued to be the best performing in terms of variable selection, overall coefficient estimation, and test MSE with negligible increases in runtime complexity.

### Model 4: $p >> n$

This is the true high-dimensional setting, so it is perhaps our biggest area of focus here. There are a few things that stand out immediately. First, all of the algorithms have increased dramatically in runtime, except for EMVS, which runs in milliseconds even in this setting. Second, almost every method selects only true signal coefficients, with minimal to no false positives among the remaining 992 predictors. Third, the shrinkage prior methods are indiscernible from EMVS in terms of their ability to estimate coefficients, and the test MSE is comparable, though the EMVS with deterministic annealing performs best (by a little). All of these methods have highly accurate performance, given the circumstances. Thus, although researchers have suggested that EMVS is a far more effective algorithm in the high dimensional setting, these results do not imply that it has much performance improvement over other methods (outside of runtime).


# Discussion

Overall, there is no way to coin one "best" performing algorithm based on these results. Each has its own strengths and drawbacks, and everything is dependent on the situation. Overall, in medium-dimensional settings ($p \approx n$), the shrinkage priors seem to perform the best in terms of test MSE and overall coefficient accuracy, while EMVS performs the best in terms of signal coefficient accuracy and runtime complexity (the latter by a grand margin). In high-dimensional settings, the performance is nearly indistinguishable between EMVS and the shrinkage priors, except that EMVS runs several orders of magnitude faster and can be tuned more effectively as a result (deterministic annealing yielded estimates from EMVS that had the smallest test MSE). In low-dimensional settings, all algorithms performed equally well. Thus, while there is no overall winner, BMA/BMS and SSVS certainly appear to be unilaterally worse options, and EMVS with its near-optimal performance and lightning-fast runtime is certainly an intriguing new option for Bayesian high-dimensional variable selection that warrants extensive future study.

There are a number of limitations to the setting of this experiment. Synthetic data is inherently imperfect, since it may not reflect real-world situations, and some aspects of this study relied too heavily on the nature of the synthetic data. There are a number of variables that went unexplored, including which algorithms perform better as the signal-to-noise ratio (set to 1 in this study) changes, and how runtime scales as the number of observations increases. The low-observation setting ($n = 100$) is also concerning: While it was necessary for the sake of computation time, it may have misleading impacts on this problem and does not allow us to generalize about which algorithms would perform best with different amounts of data. There are also a number of other very promising methods developed in recent years (some mentioned above) that were excluded from this analysis due to difficulty of implementation, which could mean that the best performing algorithms were not considered here.

Overall, though, this paper shows a robust comparison of Bayesian variable selection techniques, one that compares across branches of thinking in a manner that few if any papers have yet addressed. This paper compares and contrasts the strengths and weaknesses of various approaches, and demonstrates their capabilities in a carefully controlled experiment. Bayesian variable selection is an area of very active research right now, with most of these methods developed within the past 15 years. In another 15, who knows how far our knowledge in this field may advance? The possibilities are endless.

*Note: all notes and code used in this project can be found at github.com/davmking/bayesian-variable-selection. The research nodes are in theory-notes.qmd, experiment files in experiment.qmd, Chat GPT citations in chat-gpt-prompts.txt, references in references.bib, and source code for this report in final-report.qmd. Note that this code relies on saved objects that are not pushed to GitHub; they can be created using code in experiment.qmd.*


\newpage

# Appendix A: Model Results

Note: Bayesian model selection does not produce coefficient estimates in the same manner that the other approaches do: the identification of a strong candidate model is its most important feature. Thus, it is excluded from an analysis of MSE here.

## Key

**Algorithm Names:**

 - BMS: Bayesian Model Selection
 - BMA: Bayesian Model Averaging
 - SSVS: Stochastic Search Variable Selection
 - EMVS: Expectation-Maximization Variable Selection
 - EMVS_DA: EMVS with Deterministic Annealing
 
**Name Qualifiers:**

 - $p = ...$: the threshold of posterior inclusion probability at which any given $\beta_j$ is included in the model, selected through error minimization from an ROC curve. If not specified, $p = 0.5$.
 - $t = ...$: the temperature coefficient used in deterministic annealing, selected through test set error minimization. If not specified, $t = 1$.
 
**Variable Names:**

 - Size: the number of predictors included in the model (based on the posterior inclusion probability thresholds)
 - TPR: true positive rate
 - FNR: false negative rate
 - FPR: false positive rate
 - TNR: true negative rate
 - MSE Coef Signal: MSE between the model's estimates and the truth for coefficients that are actually non-zero
 - MSE Coef All: MSE between the model's estimates and the truth for all coefficients, including those which are zero
 - MSE Train Y: MSE in the response variable on the training set
 - MSE Test Y: MSE in the response variable on the testing set
 

## Model 1: $n >> p$

$n = 100$, $p = 10$, $p\_sig = 3$

```{r}
#| label: model 1 table

load("model_0_rates.RData")
model_0_rates %>% 
  as.data.frame() %>% 
  mutate(Size = as.numeric(Size),
         TPR = as.numeric(TPR),
         FNR = as.numeric(FNR),
         FPR = as.numeric(FPR),
         TNR = as.numeric(TNR),
         MSE_Nonzero = as.numeric(MSE_Nonzero),
         MSE_All = as.numeric(MSE_All),
         MSE_Resp = as.numeric(MSE_Resp),
         MSE_Resp_Test = as.numeric(MSE_Resp_Test)) %>% 
  select(Name, Size, TPR, FNR, FPR, TNR, Runtime) %>% 
  #kable(digits = 2, #format = "html",
        #col.names = c("Name", "Size", "TPR", "FNR", "FPR", "TNR", "Time",
                      #"MSE\nCoef\nSignal", "MSE\nCoef\nAll", "MSE\nTrain\nY",
                      #"MSE\nTest\nY"),
        #longtable = TRUE)
  kable(digits = 3, longtable = TRUE)

model_0_rates %>% 
  as.data.frame() %>% 
  mutate(Size = as.numeric(Size),
         TPR = as.numeric(TPR),
         FNR = as.numeric(FNR),
         FPR = as.numeric(FPR),
         TNR = as.numeric(TNR),
         MSE_Nonzero = as.numeric(MSE_Nonzero),
         MSE_All = as.numeric(MSE_All),
         MSE_Resp = as.numeric(MSE_Resp),
         MSE_Resp_Test = as.numeric(MSE_Resp_Test)) %>% 
  select(Name, Size, MSE_Nonzero, MSE_All, MSE_Resp, MSE_Resp_Test) %>% 
  filter(Name != "BMS") %>% 
  kable(digits = 2,
        col.names = c("Name", "Size", "MSE Coef Signal", "MSE Coef All",
                      "MSE Train Y", "MSE Test Y"),
        longtable = TRUE)

load("model_coef_0.RData")
model_coef_0
```

## Model 2: $n > p$

$n = 100$, $p = 95$, $p\_sig = 8$

```{r}
#| label: model 2 table

load("model_3_rates.RData")
model_3_rates %>% 
  as.data.frame() %>% 
  mutate(Size = as.numeric(Size),
         TPR = as.numeric(TPR),
         FNR = as.numeric(FNR),
         FPR = as.numeric(FPR),
         TNR = as.numeric(TNR),
         MSE_Nonzero = as.numeric(MSE_Nonzero),
         MSE_All = as.numeric(MSE_All),
         MSE_Resp = as.numeric(MSE_Resp),
         MSE_Resp_Test = as.numeric(MSE_Resp_Test)) %>% 
  select(Name, Size, TPR, FNR, FPR, TNR, Runtime) %>% 
  #kable(digits = 2, #format = "html",
        #col.names = c("Name", "Size", "TPR", "FNR", "FPR", "TNR", "Time",
                      #"MSE\nCoef\nSignal", "MSE\nCoef\nAll", "MSE\nTrain\nY",
                      #"MSE\nTest\nY"),
        #longtable = TRUE)
  kable(digits = 3, longtable = TRUE)

model_3_rates %>% 
  as.data.frame() %>% 
  mutate(Size = as.numeric(Size),
         TPR = as.numeric(TPR),
         FNR = as.numeric(FNR),
         FPR = as.numeric(FPR),
         TNR = as.numeric(TNR),
         MSE_Nonzero = as.numeric(MSE_Nonzero),
         MSE_All = as.numeric(MSE_All),
         MSE_Resp = as.numeric(MSE_Resp),
         MSE_Resp_Test = as.numeric(MSE_Resp_Test)) %>% 
  select(Name, Size, MSE_Nonzero, MSE_All, MSE_Resp, MSE_Resp_Test) %>% 
  filter(Name != "BMS") %>% 
  kable(digits = 2,
        col.names = c("Name", "Size", "MSE Coef Signal", "MSE Coef All",
                      "MSE Train Y", "MSE Test Y"),
        longtable = TRUE)

load("model_coef_3.RData")
model_coef_3
```

## Model 3: $p > n$

$n = 100$, $p = 105$, $p\_sig = 8$

```{r}
#| label: model 3 table

load("model_4_rates.RData")
model_4_rates %>% 
  as.data.frame() %>% 
  mutate(Size = as.numeric(Size),
         TPR = as.numeric(TPR),
         FNR = as.numeric(FNR),
         FPR = as.numeric(FPR),
         TNR = as.numeric(TNR),
         MSE_Nonzero = as.numeric(MSE_Nonzero),
         MSE_All = as.numeric(MSE_All),
         MSE_Resp = as.numeric(MSE_Resp),
         MSE_Resp_Test = as.numeric(MSE_Resp_Test)) %>% 
  select(Name, Size, TPR, FNR, FPR, TNR, Runtime) %>% 
  #kable(digits = 2, #format = "html",
        #col.names = c("Name", "Size", "TPR", "FNR", "FPR", "TNR", "Time",
                      #"MSE\nCoef\nSignal", "MSE\nCoef\nAll", "MSE\nTrain\nY",
                      #"MSE\nTest\nY"),
        #longtable = TRUE)
  kable(digits = 3, longtable = TRUE)

model_4_rates %>% 
  as.data.frame() %>% 
  mutate(Size = as.numeric(Size),
         TPR = as.numeric(TPR),
         FNR = as.numeric(FNR),
         FPR = as.numeric(FPR),
         TNR = as.numeric(TNR),
         MSE_Nonzero = as.numeric(MSE_Nonzero),
         MSE_All = as.numeric(MSE_All),
         MSE_Resp = as.numeric(MSE_Resp),
         MSE_Resp_Test = as.numeric(MSE_Resp_Test)) %>% 
  select(Name, Size, MSE_Nonzero, MSE_All, MSE_Resp, MSE_Resp_Test) %>% 
  filter(Name != "BMS") %>% 
  kable(digits = 2,
        col.names = c("Name", "Size", "MSE Coef Signal", "MSE Coef All",
                      "MSE Train Y", "MSE Test Y"),
        longtable = TRUE)

load("model_coef_4.RData")
model_coef_4
```

## Model 4: $p >> n$

$n = 100$, $p = 1000$, $p\_sig = 8$

```{r}
#| label: model 4 table

load("model_5_rates.RData")
model_5_rates %>% 
  as.data.frame() %>% 
  mutate(Size = as.numeric(Size),
         TPR = as.numeric(TPR),
         FNR = as.numeric(FNR),
         FPR = as.numeric(FPR),
         TNR = as.numeric(TNR),
         MSE_Nonzero = as.numeric(MSE_Nonzero),
         MSE_All = as.numeric(MSE_All),
         MSE_Resp = as.numeric(MSE_Resp),
         MSE_Resp_Test = as.numeric(MSE_Resp_Test)) %>% 
  select(Name, Size, TPR, FNR, FPR, TNR, Runtime) %>% 
  #kable(digits = 2, #format = "html",
        #col.names = c("Name", "Size", "TPR", "FNR", "FPR", "TNR", "Time",
                      #"MSE\nCoef\nSignal", "MSE\nCoef\nAll", "MSE\nTrain\nY",
                      #"MSE\nTest\nY"),
        #longtable = TRUE)
  kable(digits = 3, longtable = TRUE)

model_5_rates %>% 
  as.data.frame() %>% 
  mutate(Size = as.numeric(Size),
         TPR = as.numeric(TPR),
         FNR = as.numeric(FNR),
         FPR = as.numeric(FPR),
         TNR = as.numeric(TNR),
         MSE_Nonzero = as.numeric(MSE_Nonzero),
         MSE_All = as.numeric(MSE_All),
         MSE_Resp = as.numeric(MSE_Resp),
         MSE_Resp_Test = as.numeric(MSE_Resp_Test)) %>% 
  select(Name, Size, MSE_Nonzero, MSE_All, MSE_Resp, MSE_Resp_Test) %>% 
  filter(Name != "BMS") %>% 
  kable(digits = 2,
        col.names = c("Name", "Size", "MSE Coef Signal", "MSE Coef All",
                      "MSE Train Y", "MSE Test Y"),
        longtable = TRUE)

load("model_coef_5.RData")
model_coef_5
```

\newpage

# Appendix B: Dense Setting

In addition to the sparse signal setting, I briefly considered the dense signal setting. One set of models was sufficient to know that none of these algorithms are very effective at identifying signals in a dense signal setting. In this setting, $n = 100, p = 95, p\_sig = 75$. The table below depicts the outcomes from each model in this dense setting. It mostly follows from the tables above, but has not been fully updated. This is here purely for those interested, and is not meant as true supplemental material.

```{r}
#| label: dense model table

load("model_2_rates.RData")
model_2_rates %>% 
  as.data.frame() %>% 
  mutate(Size = as.numeric(Size),
         TPR = as.numeric(TPR),
         FNR = as.numeric(FNR),
         FPR = as.numeric(FPR),
         TNR = as.numeric(TNR),
         MSE_Nonzero = as.numeric(MSE_Nonzero),
         MSE_All = as.numeric(MSE_All)) %>% 
  kable(digits = 2, longtable = TRUE,
        col.names = c("Name", "Size", "TPR", "FNR", "FPR", "TNR",
                      "Runtime", "MSE Coef Signal", "MSE Coef All"))
```

\newpage

# Appendix C: R Packages Used

 - `BayesVarSel`: Bayesian model selection [@R-BayesVarSel].
 - `BMS`: Bayesian model averaging [@R-BMS].
 - `BoomSpikeSlab`: SSVS [@R-BoomSpikeSlab].
 - `bayeslm`: Bayesian linear modeling [@R-bayeslm].
 - `BayesSubset`: Bayesian subset selection [@R-BayesSubsets].
 - `monomvn`: Shrinkage priors [@R-monomvn].
 - `tidyverse`: Data handling [@R-tidyverse].
 - `knitr`: Table formatting [@R-knitr].
 - `kableExtra`: Table formatting [@R-kableExtra].
 - `pROC`: ROC curves [@R-pROC].
 - `EMVS`: EMVS algorithm (installed from CRAN archive version 1.2.1) [@R-EMVS].
 - `viridis`: Colorblind-friendly color palettes [@R-viridis].
 - `VGAM`: Sampling from Laplace distribution [@R-VGAM].
 - `latex2exp`: Adding latex to plot labels [@R-latex2exp].



\newpage

# References


