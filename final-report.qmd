---
title: "Bayesian Methods of Variable Selection"
author: "Dav King"
format: pdf
bibliography: references.bib
csl: apa.csl
header-includes:
  - \usepackage{amssymb}
  - \usepackage{amsmath}
---

# Introduction

*Note: all notes and code used in this project can be found at github.com/davmking/bayesian-variable-selection.*

Consider the traditional regression setting:


\begin{equation}
  \mathbf{Y} = \mathbf{X} \beta + \epsilon
\end{equation}


In this setting, $\beta$ is the $p \times 1$ vector of predictors, $\mathbf{X}$ is the $n \times p$ feature matrix, $\mathbf{Y}$ is the $n \times 1$ vector of responses, and $\epsilon$ is the $n \times 1$ vector of random, irreducible, zero-mean noise. $p(\mathbf{Y} | \beta)$ is known as the **data-generative model**, i.e., the mechanism by which we believe the response variable $\mathbf{Y}$ is actually generated. Often, we have a great number of potential predictors, but believe that only a small handful of them are truly a part of the data-generative model. This is a statistical concept known as **sparsity**, where the predictors in the true data-generative model are **signals** and the remaining predictors are **noise**. Our goal is to separate the signals from the noise in order to truly understand the underlying data generative model.

Our goal, therefore, is to control the number of predictors by performing **variable selection**. By identifying only the subset of signals, we can reduce computational complexity, minimize variance in the model, and improve interpretability. This is especially useful in situations where we care more about interpretation than prediction: An understanding of the actual data-generative model allows us to understand how response variables come about, which can have highly meaningful applications in real-world settings. As computational power and data availability have simultaneously increased, variable selection processes have developed major applications in fields such as genomics and the social sciences.

Separating signals from noise is not a small feat, however. Though it is tempting to try all possible combinations of variables while solving this problem (known as best subset selection), this method is computationally intractable except in situations where $p$ is very small. For a model with $p$ parameters, there are $2^p$ possible model formulations, which compose the model space $\mathcal{M}$. Searching over all of $\mathcal{M}$ quickly becomes impossible, at least with modern computing technology. Another possible solution is greedy algorithms such as forward- and back-selection, but these processes can miss important effects that arise when multiple related predictors are included in the model together. Other methods that penalize predictors, such as LASSO regression, have become standard across industries, but they are often not aggressive enough to select a sufficiently small subset of variables or lead to bias in the model's coefficient predictions.

One very active domain of research around these problems draws from the Bayesian perspective on statistics. At the core of Bayesian statistics is Bayes' theorem, which describes how beliefs about a random variable should be updated after encountering new information:

\begin{equation}
  \mathbb{P}(B | A) = \frac{\mathbb{P}(A | B) \mathbb{P}(B)}{\mathbb{P}(A)}
\end{equation}

As will be discussed below, Bayesian statistics allows for the statistician to impose some structure on the model based on their prior beliefs about its format, which can help stabilize calculations and ensure an output of the desired form. It also can search over high-probability regions of the model space much more effectively than frequentist approaches. Many papers have compared different aspects of Bayesian model averaging to one another (e.g., [@lu2022], [@rockova2013]), but few if any have compared different domains of Bayesian variable selection to one another. Thus, this paper describes the foundation of Bayesian variable selection methods, and performs a brief experiment to compare approaches across domains.


## Research Questions

The central research questions for this paper were as follows:

\begin{enumerate}
  \item What methods of Bayesian variable selection exist, and how do they relate to one another?
  \item Which methods perform best in each of the following settings? \begin{align*}
  n >> p \\
  n > p \\
  p > n \\
  p >> n 
  \end{align*}
  \item What are the computation time/accuracy tradeoffs?
\end{enumerate}


# Bayesian Modeling

## Introduction

\begin{equation}
  \mathbb{P}(\beta | \mathbf{Y}) = \frac{\mathbb{P}(\mathbf{Y} | \beta) \mathbb{P}(\beta)}{\int_\beta \mathbb{P}(\mathbf{Y} | \beta) \mathbb{P}(\beta) d\beta}
\end{equation}

In a Bayesian modeling setting, we use Bayes' theorem to generate estimates of the model parameters. In this equation, $\mathbb{P}(\mathbf{Y} | \beta)$ is the data generative model, or the process by which we believe the response variable is generated. This is generally represented with a likelihood function. $\mathbb{P}(\beta)$ is the prior distribution, representing our beliefs about the structure of $\beta$ before viewing any of the data. This is a useful concept in Bayesian variable selection, as we will see later. $\int_\beta \mathbb{P}(\mathbf{Y} | \beta) \mathbb{P}(\beta) d\beta$ is the normalizing constant which ensures the posterior distribution integrates to 1, and it can usually be absorbed into proportionality and thus omitted from these calculations. Finally, $\mathbb{P}(\beta | \mathbf{Y})$ is the posterior distribution, reflecting our beliefs about $\beta$ after seeing the data - in other words, reflecting an update in our beliefs about $\beta$ from our prior beliefs, based on the data likelihood function.


## Markov Chain Monte Carlo

Let $\beta$ and $\mathbf{Y}$ be as defined above, and let $\theta$ represent all other parameters in our modeling setup. In this setting, we would like to calculate the joint posterior distribution $p(\beta, \mathbf{Y}, \theta)$. In many cases, posterior distributions are difficult or impossible to compute by hand. In this case, we estimate the joint posterior by sampling repeatedly from the full conditional distributions of all involved parameters (full conditional means the distribution of one parameter given everything else). This is known as a **Markov Chain Monte Carlo (MCMC)** algorithm. It is conducted as follows: Initialize your values $\beta^{(1)}$, $\mathbf{Y}^{(1)}$, and $\theta^{(1)}$. Then, for every step $s$, 

\begin{enumerate}
  \item Sample $\beta^{(s + 1)}$ from the full conditional $p(\beta | \mathbf{Y}^{(s)}, \theta^{(s)})$.
  \item Sample $\mathbf{Y}^{(s + 1)}$ from the full conditional $p(\mathbf{Y} | \beta^{(s + 1)}, \theta^{(s)})$.
  \item Sample $\theta^{(s + 1)}$ from the full conditional $p(\theta | \beta^{(s + 1)}, \mathbf{Y}^{(s + 1)})$.
\end{enumerate}

After repeating this algorithm for long enough, it will eventually converge to the joint posterior (reasonable initialization is key, or it may not reach the true joint posterior before the heat death of the universe). Usually, iterations on the order of tens of thousands are sufficient for the sampler to converge and give a reasonable estimate of the joint posterior. There are a couple things of importance here. First, the order in which these variables are sampled does not matter, and they can even be re-shuffled randomly at different steps. Second, the Markov Chain is memoryless - each sample only depends on the step immediately before it.

There are many different ways that values can be sampled in this process, but two of the most famous algorithms are Gibbs and Metropolis-Hastings. In a Gibbs sampler, the full conditional of each variable is known and easy to compute, and so new values are simply drawn from that full conditional and accepted with probability 1. In a Metropolis-Hastings algorithm, the full conditional of each variable is harder to draw from, so the following steps are used:

\begin{enumerate}
  \item Propose a new value of the parameter of interest from a proposal distribution, usually centered at around the current parameter value.
  \item Calculate the likelihood ratio $\alpha = \ell(\text{proposal}) / \ell(\text{current})$.
  \item Accept the likelihood ratio with probability $\text{min}\{1, \alpha\}$.
\end{enumerate}

With a Metropolis-Hastings sampler, if the proposed value is more likely than the current value, it is always accepted; otherwise, it is accepted with probability corresponding to the likelihood ratio (which ensures that the sampler continues to explore parameter space). Note that it is possible to use both Gibbs and Metropolis-Hastings sampling for different parameters within the same MCMC sampler.


# Non-Prior Methods

The first domain of Bayesian variable selection is general methods revolving around the problem setup, without modifying the priors placed on the $\beta$s. These still take place within the framework of MCMC sampling, but introduce additional structure or variables in order to reach the desired outcomes.


## Bayesian Model Selection

Define a variable $\gamma = \gamma_1, \dots, \gamma_p$. This characterizes a sub-model $\mathcal{M}_\gamma$,

\begin{equation}
  \mathbf{Y} = \beta_0 \mathbf{1} + \mathbf{X}_\gamma \beta_\gamma + \epsilon,
\end{equation}

where $\gamma_j = 1$ indicates that $\beta_j$ is included in $\mathcal{M}_\gamma$, and $\gamma_j = 0$ indicates that $\beta_j$ is not [@lu2022]. The different specifications of $\gamma$ characterize the entire model space $\mathcal{M}$. We can place a variety of priors on $\gamma$, but Bernoulli priors are among the most common.

In our MCMC sampler, we sample $\gamma$ along with $\beta$ and $\mathbf{Y}$. Typically, we sample $\gamma$ with a procedure like Metropolis-Hastings, so that we can accept or reject the new choice of $\gamma$ based on the model likelihood ratios (however, even Gibbs sampling will converge). There are a number of different criteria that can be used for comparing models, such as the Bayesian information criterion (BIC) or Chib's marginal likelihood estimator [@lu2022].

Regardless of how new values of $\gamma$ are sampled, the sampler is able to identify probable models intuitively: More probable models will appear in the posterior distribution of $\gamma$ more frequently. In fact, this procedure often converges to one (or a few) very strong candidate models rapidly, even when only exploring a small fraction of model space. This makes Bayesian model selection a very useful procedure, since it is able to dramatically cut down on the amount of computation required to identify highly probable models (with some risk of failing to reach the areas of true high posterior density, if the procedure is done poorly). This is also a very simple procedure, and it requires relatively little computation work, making it one of the fastest algorithms considered here. However, it does have some downsides: The selection of only one model specification may place too much emphasis on random noise in the data sample, while other highly probable models are dropped from consideration. Still, this procedure is very effective and widely used.


## Bayesian Model Averaging

Bayesian model averaging follows the exact same procedure as BMS: Define $\gamma$ as a latent binary indicator variable, sample this along with $\beta$ and $\mathbf{Y}$, and find the highly probable models. However, there is one key difference. In BMS, we look to identify the model with the highest posterior probability, and determine that to be our selected model. In BMA, we simply average over the coefficients of every sampled model, letting $\beta_j = 0$ if $\gamma_j = 0$. This results, more-or-less, in posterior estimates for $\beta$ that are weighted by the probability of each model specification $\gamma$. Though unintuitive, this procedure often results in highly accurate estimates of the true model specification $\gamma$ [@hoeting1999].

BMA, like BMS, has relatively little computational demand, and is widely applicable in a variety of situations. BMA also has the additional benefit of weighting $\beta$ coefficients by model probabilities, which allows the model specification to be influenced by a variety of highly probable model specifications instead of just one. However, there may be situations where the averaging does not produce the desired result, depending on the circumstances of the data. It may also have difficulty converging to the correct values if initialized poorly. This procedure is useful for its low computational demand and incorporation of information from the full (sampled) model space.


## Bayesian Subset Selection

Bayesian subset selection is very similar to the frequentist problem of best subset selection, with a few key differences. First, instead of focusing on the singular best subset, BSS identifies a family of "acceptable" subsets, or those within some tolerable $\epsilon$ of the best subset. This allows for the identification of near-optimal subsets, which may represent the true data generative model but might not be recognized under best subset selection because of patterns in the random noise. Second, it draws response estimates from the posterior predictive distribution of a Bayesian model, rather than simply the estimates from an OLS regression model over the subset. This allows for a better quantification of uncertainty in the variable selection process, and establishes the optimal coefficients for any subset of predictors. Third, it uses a more efficient branch-and-bound algorithm to explore only the promising subsets, and only considers subsets up to a maximum size. Fourth, it summarizes across the acceptable subsets to generate the "best" subset, including the "best" (by cross-validation) predictive subset, the smallest acceptable subset, and metrics on variable co-importance, all of which the frequentist best subset selection lacks [@kowal2022].

However, as promising as BSS is with its many strengths over frequentist methods and its ability to compare subsets of models in the true spirit of variable selection, it still has its drawbacks. First, even with the computational benefits of the branch-and-bound algorithm, it is still computationally intractible above a certain number of predictors, which restricts the algorithm to subsets of an arbitrary maximum size (and may eliminate the true best subsets from consideration). Since there is no efficient implementation of this in R, it is excluded from this analysis. Additionally, this means it is not much more computationally tractable than frequentist best subset selection, where the branch-and-bound algorithm can also be applied. This puts BSS far behind BMS and BMA, which do not need to search over the full model space to identify promising models. Still, the Bayesian BSS method is very promising (and still very new), and offers a lot of advantages. Research in the coming years will likely make this one of the clearly preferred methods of Bayesian variable selection, particularly as searching over candidate subsets becomes more efficient.


# Prior Methods

## Spike-and-Slab Priors

### Stochastic Search Variable Selection

### Normal Mixture of Inverse Gamma


## Shrinkage Priors

### LASSO

### Normal-Gamma

### Horseshoe


# Expectation-Maximization Variable Selection

## Overview

## Algorithm

## Deterministic Annealing


# Experiment

## Methodology

## Results


# Discussion


\newpage

# References


\newpage

# Appendix


