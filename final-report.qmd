---
title: "Bayesian Methods of Variable Selection"
author: "Dav King"
format: 
  pdf:
    echo: false
    warning: false
    message: false
bibliography: references.bib
csl: apa.csl
header-includes:
  - \usepackage{amssymb}
  - \usepackage{amsmath}
---

```{r}
#| label: libraries

library(VGAM)
library(tidyverse)
library(latex2exp)
library(viridis)
```


# Introduction

*Note: all notes and code used in this project can be found at github.com/davmking/bayesian-variable-selection.*

Consider the traditional regression setting:


\begin{equation}
  \mathbf{Y} = \mathbf{X} \beta + \epsilon
\end{equation}


In this setting, $\beta$ is the $p \times 1$ vector of predictors, $\mathbf{X}$ is the $n \times p$ feature matrix, $\mathbf{Y}$ is the $n \times 1$ vector of responses, and $\epsilon$ is the $n \times 1$ vector of random, irreducible, zero-mean noise. $p(\mathbf{Y} | \beta)$ is known as the **data-generative model**, i.e., the mechanism by which we believe the response variable $\mathbf{Y}$ is actually generated. Often, we have a great number of potential predictors, but believe that only a small handful of them are truly a part of the data-generative model. This is a statistical concept known as **sparsity**, where the predictors in the true data-generative model are **signals** and the remaining predictors are **noise**. Our goal is to separate the signals from the noise in order to truly understand the underlying data generative model.

Our goal, therefore, is to control the number of predictors by performing **variable selection**. By identifying only the subset of signals, we can reduce computational complexity, minimize variance in the model, and improve interpretability. This is especially useful in situations where we care more about interpretation than prediction: An understanding of the actual data-generative model allows us to understand how response variables come about, which can have highly meaningful applications in real-world settings. As computational power and data availability have simultaneously increased, variable selection processes have developed major applications in fields such as genomics and the social sciences.

Separating signals from noise is not a small feat, however. Though it is tempting to try all possible combinations of variables while solving this problem (known as best subset selection), this method is computationally intractable except in situations where $p$ is very small. For a model with $p$ parameters, there are $2^p$ possible model formulations, which compose the model space $\mathcal{M}$. Searching over all of $\mathcal{M}$ quickly becomes impossible, at least with modern computing technology. Another possible solution is greedy algorithms such as forward- and back-selection, but these processes can miss important effects that arise when multiple related predictors are included in the model together. Other methods that penalize predictors, such as LASSO regression, have become standard across industries, but they are often not aggressive enough to select a sufficiently small subset of variables or lead to bias in the model's coefficient predictions.

One very active domain of research around these problems draws from the Bayesian perspective on statistics. At the core of Bayesian statistics is Bayes' theorem, which describes how beliefs about a random variable should be updated after encountering new information:

\begin{equation}
  \mathbb{P}(B | A) = \frac{\mathbb{P}(A | B) \mathbb{P}(B)}{\mathbb{P}(A)}
\end{equation}

As will be discussed below, Bayesian statistics allows for the statistician to impose some structure on the model based on their prior beliefs about its format, which can help stabilize calculations and ensure an output of the desired form. It also can search over high-probability regions of the model space much more effectively than frequentist approaches. Many papers have compared different aspects of Bayesian model averaging to one another (e.g., [@lu2022], [@rockova2013]), but few if any have compared different domains of Bayesian variable selection to one another. Thus, this paper describes the foundation of Bayesian variable selection methods, and performs a brief experiment to compare approaches across domains.


## Research Questions

The central research questions for this paper were as follows:

\begin{enumerate}
  \item What methods of Bayesian variable selection exist, and how do they relate to one another?
  \item Which methods perform best in each of the following settings? \begin{align*}
  n >> p \\
  n > p \\
  p > n \\
  p >> n 
  \end{align*}
  \item What are the computation time/accuracy tradeoffs?
\end{enumerate}


# Bayesian Modeling

## Introduction

\begin{equation}
  \mathbb{P}(\beta | \mathbf{Y}) = \frac{\mathbb{P}(\mathbf{Y} | \beta) \mathbb{P}(\beta)}{\int_\beta \mathbb{P}(\mathbf{Y} | \beta) \mathbb{P}(\beta) d\beta}
\end{equation}

In a Bayesian modeling setting, we use Bayes' theorem to generate estimates of the model parameters. In this equation, $\mathbb{P}(\mathbf{Y} | \beta)$ is the data generative model, or the process by which we believe the response variable is generated. This is generally represented with a likelihood function. $\mathbb{P}(\beta)$ is the prior distribution, representing our beliefs about the structure of $\beta$ before viewing any of the data. This is a useful concept in Bayesian variable selection, as we will see later. $\int_\beta \mathbb{P}(\mathbf{Y} | \beta) \mathbb{P}(\beta) d\beta$ is the normalizing constant which ensures the posterior distribution integrates to 1, and it can usually be absorbed into proportionality and thus omitted from these calculations. Finally, $\mathbb{P}(\beta | \mathbf{Y})$ is the posterior distribution, reflecting our beliefs about $\beta$ after seeing the data - in other words, reflecting an update in our beliefs about $\beta$ from our prior beliefs, based on the data likelihood function.


## Markov Chain Monte Carlo

Let $\beta$ and $\mathbf{Y}$ be as defined above, and let $\theta$ represent all other parameters in our modeling setup. In this setting, we would like to calculate the joint posterior distribution $p(\beta, \mathbf{Y}, \theta)$. In many cases, posterior distributions are difficult or impossible to compute by hand. In this case, we estimate the joint posterior by sampling repeatedly from the full conditional distributions of all involved parameters (full conditional means the distribution of one parameter given everything else). This is known as a **Markov Chain Monte Carlo (MCMC)** algorithm. It is conducted as follows: Initialize your values $\beta^{(1)}$, $\mathbf{Y}^{(1)}$, and $\theta^{(1)}$. Then, for every step $s$, 

\begin{enumerate}
  \item Sample $\beta^{(s + 1)}$ from the full conditional $p(\beta | \mathbf{Y}^{(s)}, \theta^{(s)})$.
  \item Sample $\mathbf{Y}^{(s + 1)}$ from the full conditional $p(\mathbf{Y} | \beta^{(s + 1)}, \theta^{(s)})$.
  \item Sample $\theta^{(s + 1)}$ from the full conditional $p(\theta | \beta^{(s + 1)}, \mathbf{Y}^{(s + 1)})$.
\end{enumerate}

After repeating this algorithm for long enough, it will eventually converge to the joint posterior (reasonable initialization is key, or it may not reach the true joint posterior before the heat death of the universe). Usually, iterations on the order of tens of thousands are sufficient for the sampler to converge and give a reasonable estimate of the joint posterior. There are a couple things of importance here. First, the order in which these variables are sampled does not matter, and they can even be re-shuffled randomly at different steps. Second, the Markov Chain is memoryless - each sample only depends on the step immediately before it.

There are many different ways that values can be sampled in this process, but two of the most famous algorithms are Gibbs and Metropolis-Hastings. In a Gibbs sampler, the full conditional of each variable is known and easy to compute, and so new values are simply drawn from that full conditional and accepted with probability 1. In a Metropolis-Hastings algorithm, the full conditional of each variable is harder to draw from, so the following steps are used:

\begin{enumerate}
  \item Propose a new value of the parameter of interest from a proposal distribution, usually centered at around the current parameter value.
  \item Calculate the likelihood ratio $\alpha = \ell(\text{proposal}) / \ell(\text{current})$.
  \item Accept the likelihood ratio with probability $\text{min}\{1, \alpha\}$.
\end{enumerate}

With a Metropolis-Hastings sampler, if the proposed value is more likely than the current value, it is always accepted; otherwise, it is accepted with probability corresponding to the likelihood ratio (which ensures that the sampler continues to explore parameter space). Note that it is possible to use both Gibbs and Metropolis-Hastings sampling for different parameters within the same MCMC sampler.


# Non-Prior Methods

The first domain of Bayesian variable selection is general methods revolving around the problem setup, without modifying the priors placed on the $\beta$s. These still take place within the framework of MCMC sampling, but introduce additional structure or variables in order to reach the desired outcomes.


## Bayesian Model Selection

Define a variable $\gamma = \gamma_1, \dots, \gamma_p$. This characterizes a sub-model $\mathcal{M}_\gamma$,

\begin{equation}
  \mathbf{Y} = \beta_0 \mathbf{1} + \mathbf{X}_\gamma \beta_\gamma + \epsilon,
\end{equation}

where $\gamma_j = 1$ indicates that $\beta_j$ is included in $\mathcal{M}_\gamma$, and $\gamma_j = 0$ indicates that $\beta_j$ is not [@lu2022]. The different specifications of $\gamma$ characterize the entire model space $\mathcal{M}$. We can place a variety of priors on $\gamma$, but Bernoulli priors are among the most common.

In our MCMC sampler, we sample $\gamma$ along with $\beta$ and $\mathbf{Y}$. Typically, we sample $\gamma$ with a procedure like Metropolis-Hastings, so that we can accept or reject the new choice of $\gamma$ based on the model likelihood ratios (however, even Gibbs sampling will converge). There are a number of different criteria that can be used for comparing models, such as the Bayesian information criterion (BIC) or Chib's marginal likelihood estimator [@lu2022].

Regardless of how new values of $\gamma$ are sampled, the sampler is able to identify probable models intuitively: More probable models will appear in the posterior distribution of $\gamma$ more frequently. In fact, this procedure often converges to one (or a few) very strong candidate models rapidly, even when only exploring a small fraction of model space. This makes Bayesian model selection a very useful procedure, since it is able to dramatically cut down on the amount of computation required to identify highly probable models (with some risk of failing to reach the areas of true high posterior density, if the procedure is done poorly). This is also a very simple procedure, and it requires relatively little computation work, making it one of the fastest algorithms considered here. However, it does have some downsides: The selection of only one model specification may place too much emphasis on random noise in the data sample, while other highly probable models are dropped from consideration. Still, this procedure is very effective and widely used.


## Bayesian Model Averaging

Bayesian model averaging follows the exact same procedure as BMS: Define $\gamma$ as a latent binary indicator variable, sample this along with $\beta$ and $\mathbf{Y}$, and find the highly probable models. However, there is one key difference. In BMS, we look to identify the model with the highest posterior probability, and determine that to be our selected model. In BMA, we simply average over the coefficients of every sampled model, letting $\beta_j = 0$ if $\gamma_j = 0$. This results, more-or-less, in posterior estimates for $\beta$ that are weighted by the probability of each model specification $\gamma$. Though unintuitive, this procedure often results in highly accurate estimates of the true model specification $\gamma$ [@hoeting1999].

BMA, like BMS, has relatively little computational demand, and is widely applicable in a variety of situations. BMA also has the additional benefit of weighting $\beta$ coefficients by model probabilities, which allows the model specification to be influenced by a variety of highly probable model specifications instead of just one. However, there may be situations where the averaging does not produce the desired result, depending on the circumstances of the data. It may also have difficulty converging to the correct values if initialized poorly. This procedure is useful for its low computational demand and incorporation of information from the full (sampled) model space.


## Bayesian Subset Selection

Bayesian subset selection is very similar to the frequentist problem of best subset selection, with a few key differences. First, instead of focusing on the singular best subset, BSS identifies a family of "acceptable" subsets, or those within some tolerable $\epsilon$ of the best subset. This allows for the identification of near-optimal subsets, which may represent the true data generative model but might not be recognized under best subset selection because of patterns in the random noise. Second, it draws response estimates from the posterior predictive distribution of a Bayesian model, rather than simply the estimates from an OLS regression model over the subset. This allows for a better quantification of uncertainty in the variable selection process, and establishes the optimal coefficients for any subset of predictors. Third, it uses a more efficient branch-and-bound algorithm to explore only the promising subsets, and only considers subsets up to a maximum size. Fourth, it summarizes across the acceptable subsets to generate the "best" subset, including the "best" (by cross-validation) predictive subset, the smallest acceptable subset, and metrics on variable co-importance, all of which the frequentist best subset selection lacks [@kowal2022].

However, as promising as BSS is with its many strengths over frequentist methods and its ability to compare subsets of models in the true spirit of variable selection, it still has its drawbacks. First, even with the computational benefits of the branch-and-bound algorithm, it is still computationally intractible above a certain number of predictors, which restricts the algorithm to subsets of an arbitrary maximum size (and may eliminate the true best subsets from consideration). Since there is no efficient implementation of this in R, it is excluded from this analysis. Additionally, this means it is not much more computationally tractable than frequentist best subset selection, where the branch-and-bound algorithm can also be applied. This puts BSS far behind BMS and BMA, which do not need to search over the full model space to identify promising models. Still, the Bayesian BSS method is very promising (and still very new), and offers a lot of advantages. Research in the coming years will likely make this one of the clearly preferred methods of Bayesian variable selection, particularly as searching over candidate subsets becomes more efficient.


# Prior Methods

Suppose we have some prior beliefs about the structure of our $\beta$s. Specifically, we believe that not all of them are included in the true data-generative model, and many of their coefficients must be zero. We can place a prior distribution on the $\beta$s accordingly, reflecting this belief. This produces a posterior distribution of our desired form, where many of the $\beta$s are set to (near) zero. We can also introduce additional structure with this method, such as placing priors on different groups of $\beta$s, but that goes beyond the scope of this paper.

There are two major classes of priors used for variable selection: spike-and-slab priors and shrinkage priors.


## Spike-and-Slab Priors

A spike-and-slab prior is a mixture distribution: A combination of two distributions, where $\beta_j$ is drawn from one or the other according to some Bernoulli probability. Thus, a latent indicator variable $\gamma$ is still used, but instead of explicitly denoting whether a specific $\beta_j$ is included in the model, it denotes whether the prior distribution for $\beta_j$ is drawn from the spike or the slab distribution.

Spike-and-slab priors have the general form

\begin{equation}
  \beta_j | \gamma_j \sim (1 - \gamma_j) \phi_0(\beta_j) + \gamma_j \phi_1(\beta_j),
\end{equation}

where $\phi_0(\beta_j)$ is a concentrated "spike" distribution that pulls some of the $\beta$s to (near) zero, while $\phi_1(\beta_j)$ is a diffuse "slab" distribution that allows the remaining $\beta$s to attain their true coefficients [@lu2022]. By continuing to sample $\gamma$ in the MCMC sampler, this specification returns a joint posterior where (in general) the signal coefficients are near their true values while the noise coefficients are near zero, with posterior inclusion probabilities ($\gamma$) that reflect the most probable model specifications. This allows for a little more uncertainty in the estimation of coefficients than BMS does, since near-zero coefficients do not have to be pulled to exactly zero. Overall, spike-and-slab approaches are well-studied, effective, and widely used. However, there is still the disadvantage of needing to include $\gamma$ to sample over the model space $\mathcal{M}$, which can result in either extreme computational complexity or the possibility that the sampler never reaches the areas of true high posterior density.


### Stochastic Search Variable Selection

The best-known implementation of the spike-and-slab prior is stochastic search variable selection [@george1993]. This follows the procedure outlined above, embedding the full model selection process in a hierarchical Bayesian MCMC scheme. In SSVS, the prior is represented as a mixture of normals:

\begin{equation}
  \beta_j | \gamma_j \sim (1 - \gamma_j) N(0, \tau_j^2) + \gamma_j N(0, c_j^2 \tau_j^2)
\end{equation}

Prior distributions can be placed on $\tau$ and $c$, or they can be held constant.

```{r}
#| label: spike and slab distribution

tau <- 1
c <- 3

set.seed(481)
spike <- rnorm(100000, 0, tau^2)
slab <- rnorm(100000, 0, c^2 * tau^2)

data.frame(slab, spike) %>% 
  pivot_longer(everything()) %>% 
  mutate(name = factor(name, levels = c("slab", "spike"))) %>% 
  ggplot(aes(x = value, fill = name)) +
  geom_density(alpha = 0.5) +
  theme_bw() +
  labs(x = TeX("$\\beta$"), y = "Density", fill = "Distribution",
       title = "Plot of SSVS Spike-and-Slab Distribution",
       subtitle = TeX("$\\tau = 1, c = 3$"),
       caption = "Note: spike is usually more concentrated and slab is usually more diffuse than depicted") +
  theme(plot.title = element_text(hjust = 0.5),
        plot.subtitle = element_text(hjust = 0.5),
        panel.grid = element_blank(),
        legend.position = "bottom") +
  scale_fill_viridis(discrete = TRUE, direction = -1) +
  coord_cartesian(xlim = c(-30, 30))
```


### Normal Mixture of Inverse Gamma

The normal mixture of inverse gamma expands on SSVS by moving the spike-and-slab formulation down one level in the hierarchy, to place it on the variance instead [@albert1993, @rockova2012]. The hierarchal specification of this model is

\begin{align}
  \beta_j | \eta_j &\sim N(0, \eta_j) \\
  \eta_j | \gamma_j &\sim (1 - \gamma_j) \text{IG} \left ( a, \frac{\nu_0}{b} \right ) + \gamma_j \text{IG} \left ( a, \frac{\nu_1}{b} \right ),
\end{align}

where IG is the inverse-gamma distribution, and $\nu_1 >> \nu_0$. A prior, typically Bernoulli, is placed on $\gamma$. By moving the sampling down one level in the hierarchy, we avoid having to put as much weight on the hyperparameters $c$ and $\tau$, often leading to better performance [@lu2022]. However, due to a lack of an efficient implementation in R, we omit NMIG from the experiment in this paper.


## Shrinkage Priors

Unlike spike-and-slab priors, which are mixture distributions, shrinkage priors are single-point, continuous distributions. Their goal is to pull some of the $\beta$s towards zero, while minimally penalizing the signal coefficients. Shrinkage priors have the benefit of being fully continuous, which means that they do not require a latent variable $\gamma$ to search over $\mathcal{M}$, and this makes shrinkage priors less likely to leave an entire set of highly probable model specifications unexplored. However, this has its tradeoffs: approaches using shrinkage priors often have dramatically increased computation time, and research has suggested that they may not select variables aggressively enough or may overly penalize the signal coefficients in the process (i.e., they may be less effective at discerning between signal and noise predictors).


### LASSO

In frequentist statistics, LASSO regression is a well-know regression setting that expands on OLS regression to penalize the $\ell_1$ norm of the $\beta$s, using a hyperparameter $\lambda$ to control the shrinkage:

\begin{equation}
  \underset{\beta}{\text{min}} (\mathbf{Y} - \mathbf{X}\beta)^T (\mathbf{Y} - \mathbf{X} \beta) + \lambda \sum_{j = 1}^{p} |\beta_j|
\end{equation}

In the Bayesian analogue to LASSO regression [@park2008], we place a conditional Laplace prior on $\beta$:

\begin{equation}
  p(\beta | \sigma^2) = \prod_{j = 1}^{p} \frac{\lambda}{2\sqrt{\sigma^2}} \text{exp} \left \{ \frac{-\lambda |\beta_j|}{\sqrt{\sigma^2}} \right \}
\end{equation}

```{r}
#| label: LASSO distribution

lambda <- c(0.5, 1, 2, 4)

N_samples <- 100000

lasso_values <- matrix(NA, nrow = N_samples, ncol = length(lambda))

for(l in 1:length(lambda)){
  lasso_values[,l] <- rlaplace(N_samples, 0, 1 / lambda[l])
}


data.frame(lasso_values) %>%
  pivot_longer(everything(), names_to = "lambda_number") %>% 
  mutate(lambda_number = as.integer(substring(lambda_number, 2))) %>% 
  mutate(lambda = lambda[lambda_number]) %>% 
  mutate(lambda = factor(lambda)) %>% 
  mutate(lambda = fct_rev(lambda)) %>% 
  ggplot(aes(x = value, fill = lambda)) +
  geom_density(alpha = 0.5) +
  coord_cartesian(xlim = c(-10, 10)) +
  theme_bw() +
  labs(x = TeX("$\\beta$"), y = "Density", fill = TeX("$\\lambda$"),
       title = "Plot of Laplace (LASSO Prior) Distribution") +
  theme(plot.title = element_text(hjust = 0.5),
        panel.grid = element_blank()) +
  scale_fill_viridis(discrete = TRUE, option = "B") +
  guides(fill = guide_legend(reverse = TRUE))
```

This results in the hierarchical specification

\begin{align}
  \beta_j | \tau_j &\sim N(0, \sigma^2 \tau_j^2) \\
  \tau_j | \lambda &\sim \text{exp}(\lambda^2 / 2)
\end{align}

Under this distribution, there is a high probability of the $\beta$s falling at or near zero, which increases with $\lambda$ as shown above. In this setting, we can also place a prior on $\lambda$ and sample it along with $\beta$. The result is a posterior where many of the $\beta$s are at or near 0, and careful prior specification can ensure that the posterior distribution is unimodal [@park2008]. This can also be generalized to other cases, such as bridge regression.


### Normal-Gamma

While the Bayesian LASSO was one of the earliest methods of Bayesian variable selection and remains one of the best-used, it is not without its criticisms. The use of only a single hyperparameter reduces flexibility, meaning that either the LASSO will fail to perform adequate variable selection or the signal coefficients will be unnecessarily reduced. Thus, the normal-gamma prior arises [@griffin2010]. The specification for the normal-gamma is

\begin{align}
  \beta_j | \tau_j &\sim N(0, \tau_j^2) \\
  \tau_j^2 | \lambda, \xi &\sim \text{gamma}(\lambda, 1/(2\xi^2))
\end{align}

The introduction of the extra hyperparameter $\xi$ and the gamma prior on the variance $\tau_j^2$ allows this distribution to have a lot of mass close to zero, while maintaining heavy tails for the coefficients. This is generally an improvement on LASSO, though it does require the introduction of an extra parameter (potentially increasing computational complexity) and does not always outperform the LASSO in applied settings.


### Horseshoe

Taking a slightly different approach, horseshoe priors fall into a class known as **global-local** shrinkage priors [@carvalho2010]. These priors use a global hyperparameter to shrink all coefficients towards zero, while maintaining a local hyperparameter to adjust the scale of shrinkage for some coefficients at the local level. The hierarchical representation of the regression model under a horseshoe prior is

\begin{align}
  \beta_j | \eta_j &\sim N(0, \eta_j^2) \\
  \eta_j | \tau &\sim C^+(0, \tau) \\ 
  \tau | \sigma &\sim C^+(0, \sigma),
\end{align}

where $C^+$ is a half-Cauchy distribution (restricted to positive values). In this specification, $\eta_j$ is the local shrinkage parameter, while $\tau$ is the global shrinkage parameter. With some integration, this results in a shrinkage coefficient $\kappa_j = 1 / (1 + \eta_j^2)$. The half-Cauchy prior on $\eta_j$ implies a Beta(1/2, 1/2) marginal distribution for $\kappa_j$, which gives the horseshoe its name.

```{r}
#| label: horseshoe distribution
#| fig-align: center

set.seed(523)
ggplot(mapping = aes(x = rbeta(1000000, 1/2, 1/2))) +
  geom_density(fill = "darkgrey", alpha = 0.5) +
  theme_bw() +
  labs(x = TeX("$\\kappa_j$"), y = "Density",
       title = "Plot of Horseshoe Prior Distribution") +
  theme(plot.title = element_text(hjust = 0.5),
        panel.grid = element_blank())
```

The distribution for $\kappa_j$ places a lot of mass near 0 (implying near-zero shrinkage, representing signals) and 1 (implying near-total shrinkage, representing noise). This specification allows for most of the coefficients to be pulled to zero, while allowing the remaining coefficients to reach their true values. It is a different specification than LASSO/NG, but one that is widely used and often performs better in practice.

### Other Approaches

There are several other shrinkage priors that have been proposed. The horseshoe+ prior places another latent variable in the hierarchy for another level of local shrinkage [@bhadra2017]. The Dirichlet-Laplace prior, another global-local shrinkage prior, draws local shrinkage coefficients from the joint Dirichlet distribution [@bhattacharya2014]. The SSLASSO prior hybridizes spike-and-slab and shrinkage priors by drawing from a mixture of Laplace distributions [@rockova2018]. While all of these approaches are promising and have their own strengths, they are excluded from further analysis here for the sake of time.


# Expectation-Maximization Variable Selection

## Overview

Though all other methods described here use the MCMC algorithm approach to Bayesian variable selection, EMVS is an algorithm that is formulated differently. EMVS is highly useful in high dimensional settings ($p >> n$), since it runs in a tiny fraction of the time that MCMC requires and can effectively identify the high-probability sparse models [@rockova2013]. EMVS is anchored by the SSVS spike-and-slab approach [@george1993], but it can be extended into other settings as well.


## Algorithm

The EMVS algorithm is very straightforward. Begin by specifying the full Bayesian regression setting, typically using a spike-and-slab prior such as SSVS. Then, repeat two steps until convergence:

 - **Expectation (E) Step:** Calculate the posterior inclusion probabilities (i.e., expectations) for each $\beta_j$ given current parameter updates.
 - **Maximization (M) Step:** Update the parameter estimates by maximizing the objective function of all model parameters, given the data.
 
More formal specifications of the equations required to execute this algorithm can be found in [@rockova2013]. In practice, this algorithm generally converges very quickly - in the experiment below, all specifications of the algorithm converged within 15 iterations, which is stellar performance.


## Deterministic Annealing

If the true posterior is multi-modal, it is very possible that poor initialization of the EMVS algorithm will lead to incorrect estimates of the posterior, as EMVS gets trapped in local modes. One potential solution to this is deterministic annealing. Instead of maximizing the objective function, we minimize a tempered version known as the negative free energy function. We can raise the "temperature", which smooths away the local modes of the negative free energy function, allowing only the true signals to shine through and giving us some robustness against poor initialization. As we progress through the algorithm, we lower the temperature progressively until its effects are completely removed, allowing us to view an equation that more truly resembles the posterior [@rockova2013]. Some form of cross-validation is required to find the optimal values of initial temperature, but since the EMVS algorithm converges so quickly, this is still much faster than MCMC sampling in most cases.


# Experiment

## Methodology

## Results


# Discussion


\newpage

# References

\newpage

# Appendix


