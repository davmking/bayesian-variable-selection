---
title: "Bayesian Methods of Variable Selection"
author: "Dav King"
format: 
  revealjs:
    css: styles.css
---

```{r}
#| label: libraries

library(VGAM)
library(tidyverse)
library(viridis)
library(patchwork)
library(latex2exp)
```


## Motivation: Why Variable Selection?

-   In modeling, we often have a vast number of possible predictors, but only anticipate a small handful are associated with the outcome variable.
-   Without controlling the number of predictors, we are likely to find significance in predictors that are actually unrelated to the outcome variable.
-   We also have much greater computational complexity with the high number of predictors.
-   By selecting a subset of predictors, we can often reduce variance in the model and create a much simpler, easy-to-interpret outcome.
-   This has particularly useful applications in fields like genomics and the social sciences.

## Frequentist Analogues

-   A tempting solution is to simply perform **best subset selection**, in order to identify the subset of predictors that minimizes RSS.
-   Problem: Combinatorial complexity.
-   There are some workarounds.
-   Another common method is **shrinkage models**, such as LASSO regression.

## Bayesian Statistics: Background

**Bayesian statistics** revolves around the use of Bayes' theorem to update our beliefs about a probability based on new information:

$$
  p(B | A) = \frac{p(A | B) p(B)}{p(A)}
$$

In **Bayesian modeling**, we use Bayes' theorem to update our beliefs about parameters based on A) our prior beliefs about the parameters and B) the likelihood function of the data:

$$
  p(\mathbf{\beta} | \mathbf{Y}) = \frac{p(\mathbf{Y} | \mathbf{\beta}) p(\mathbf{\beta})}{\int_\mathbf{\beta} p(\mathbf{Y} | \mathbf{\beta}) p(\mathbf{\beta}) d\mathbf{\beta}}
$$

## Bayesian Statistics Background

$$
  p(\mathbf{\beta} | \mathbf{Y}) = \frac{p(\mathbf{Y} | \mathbf{\beta}) p(\mathbf{\beta})}{\int_\mathbf{\beta} p(\mathbf{Y} | \mathbf{\beta}) p(\mathbf{\beta}) d\mathbf{\beta}}
$$

In this model,

 - $p(\mathbf{Y} | \mathbf{\beta})$ is the **Data Generative Model (DGM)**.
 - $p(\mathbf{\beta})$ is the **Prior Distribution**(**).
 - $\int_\mathbf{\beta} p(\mathbf{Y} | \mathbf{\beta}) p(\mathbf{\beta}) d\mathbf{\beta}$ is a **constant**, and we are usually not concerned with it.
 - $p(\mathbf{\beta} | \mathbf{Y})$ is the **Posterior Distribution**.
 
 
## Bayesian Statistics: Markov Chain Monte Carlo

 - Since many posterior distributions are hard or impossible to compute mathematically, we run a **Markov Chain Monte Carlo (MCMC)** algorithm:
  1. Initialize $\beta$ and $\mathbf{Y}$.
  2. Sample $\beta^{(s + 1)}$ based on $\mathbf{Y}^{(s)}$.
  3. Sample $\mathbf{Y}^{(s + 1)}$ based on (updated) $\mathbf{\beta}^{(s + 1)}$.
  4. Sample hyperparameters if needed.
<!-- - **Metropolis-Hastings Algorithm**: propose new values of $\mathbf{\beta}, \mathbf{Y}$, accept/reject based on the likelihood. -->

 - A key component of the MCMC process is that it is **memoryless** - that is, each value in the Markov Chain is only dependent on the sample immediately before it.
 
 
## Why Bayesian?

 - Compared to the frequentist setting, Bayesian approaches allow us to incorporate prior beliefs on the structure of the model.
 - This can help stabilize calculations and produce an outcome structured as we would like it to be.
 - It can also improve computational complexity.
 - Often, convergence is much faster in a Bayesian setting.
 - We can apply priors to groups of variables, etc.


## Research Questions

 1. What methods of Bayesian variable selection exists, and how do they relate to one another?
 2. Which methods perform best in each of the following settings?
 
 $$
  \begin{equation*}
    n >> p \\
    n > p \\
    p > n \\
    p >> n
  \end{equation*}
 $$
 
 3. What are the computation time/accuracy tradeoffs?
 

# Non Prior-Based Methods

## Bayesian Model Selection

Define a binary variable $\mathbf{\gamma} = \gamma_1, \dots, \gamma_p$, where $\gamma_i$ represents $\beta_i$ being included in the model $\mathcal{M}$. Then we can represent our entire model space $\mathcal{M}_\gamma$ as different realizations of $\gamma$. Place some prior distribution on $\gamma$, often a Bernoulli distribution.

When running the MCMC sampler, propose a new value of $\gamma$ at each step along with $\beta$ and $\mathbf{Y}$, and accept/reject the new model with a probability according to the likelihood ratio of the two models. Then we can generate a posterior distribution for $\gamma$, which shows us the regions of high posterior density, enabling us to see which $\beta$s have a higher probability of being included in the model. 

Often, this procedure converges quickly to one (or a few) strong candidate models, without having to explore the full combinatorial space.


## Bayesian Model Averaging

Bayesian model averaging (BMA) is very similar to Bayesian model selection, with one key difference. In Bayesian model selection, we are looking for the model with the highest posterior probability. In Bayesian model averaging, we simply average over *all* of the candidate models - letting $\beta_i = 0$ if it is not included in a specific candidate model. While unintuitive, this procedure frequently returns highly accurate results of the true coefficients, with (relatively) minimal computational demand.

 
# Prior-Based Methods

## Overview

 - The general idea of prior-based methods is that we have some prior belief about the structure of our $\beta$s.
 - In particular, we believe that not all of them are included in the true data-generative model.
 - We therefore place a prior distribution on the $\beta$s that reflects this belief.
 - We can add more complicated techniques, such as placing the priors on different groups of $\beta$s (beyond the scope of this project).
 
## Spike-And-Slab Priors 

A spike-and-slab prior is a mixture distribution - i.e., a combination of two distributions, drawing from one or the other with some Bernoulli probability. Spike-and-slab priors have the general form
 
$$
  \beta_j | \gamma_j \sim \gamma_j \phi_1(\beta_j) + (1 - \gamma_j) \phi_0(\beta_j),
$$

where

 - $\phi_1(\beta_j)$ is a diffuse "slab" distribution that allows the $\beta_j$ to reach their true coefficients
 - $\phi_0(\beta_j)$ is a concentrated "spike" distribution that pulls some of the $\beta_j$ to (near) zero
 - $\gamma$ is a latent binary indicator variable representing each of the $2^p$ possible models
 
## Stochastic Search Variable Selection

 - SSVS follows the same procedure of embedding the entire modeling process in a hierarchical Bayesian setup, including the model selection $\gamma$
 - The conditional distribution can be represented as a scale mixture of normals:
 
$$ 
  \beta_j | \gamma_j \sim (1 - \gamma_j) N(0, \tau_j^2) + \gamma_j N(0, c_j^2 \tau_j^2) 
$$

 - You can also place prior distributions on $\tau_j^2$ and $c_j$
 - Other priors, such as the normal mixture of inverse gammas (NMIG), extend this process
 
## Stochastic Search Variable Selection


```{r}
#| label: spike and slab distribution

tau <- 1
c <- 4

set.seed(481)
slab <- rnorm(100000, 0, tau^2)
spike <- rnorm(100000, 0, c^2 * tau^2)

data.frame(slab, spike) %>% 
  pivot_longer(everything()) %>% 
  mutate(name = factor(name, levels = c("spike", "slab"))) %>% 
  ggplot(aes(x = value, fill = name)) +
  geom_density(alpha = 0.5) +
  theme_bw() +
  labs(x = TeX("$\\beta$"), y = "Density", fill = "Distribution",
       title = "Plot of SSVS Spike-and-Slab Distribution",
       subtitle = TeX("$\\tau = 1, c = 4$")) +
  theme(plot.title = element_text(hjust = 0.5),
        plot.subtitle = element_text(hjust = 0.5),
        panel.grid = element_blank(),
        legend.position = "bottom",
        text = element_text(size = 16)) +
  scale_fill_viridis(discrete = TRUE, direction = -1) +
  scale_y_continuous(labels = scales::percent_format())
```

## Shrinkage Priors

Unlike spike-and-slab priors, shrinkage priors are single-point, continuous distributions. Their goal, much like shrinkage in frequentist regression, is to pull some of the $\beta$s towards zero, while minimally penalizing the remaining coefficients (i.e., allowing them to reach their true values).

This class of prior is generally represented as a **scale mixture of normals**, which is a mixture of normal distributions whose variances are random variables drawn from another distribution.


## LASSO Prior

The prior for the Bayesian analogue to LASSO regression is the Laplace, or double-exponential, prior. It is easy to see this when deriving the posterior. The LASSO prior assumes that the mixing distribution follows an exponential distribution. The hierarchical representation of the full conditional model under the LASSO prior is given below:

$$
\begin{align*}
  \beta_j | \tau_j &\sim N(0, \sigma^2 \tau_j^2) \\
  \tau_j^2 | \lambda &\sim \text{exp}(\lambda^2 / 2)
\end{align*}
$$

## LASSO Prior

```{r}
#| label: LASSO distribution

lambda <- c(0.5, 1, 2, 4)

N_samples <- 100000

lasso_values <- matrix(NA, nrow = N_samples, ncol = length(lambda))

for(l in 1:length(lambda)){
  lasso_values[,l] <- rlaplace(N_samples, 0, 1 / lambda[l])
}


data.frame(lasso_values) %>%
  pivot_longer(everything(), names_to = "lambda_number") %>% 
  mutate(lambda_number = as.integer(substring(lambda_number, 2))) %>% 
  mutate(lambda = lambda[lambda_number]) %>% 
  mutate(lambda = factor(lambda)) %>% 
  mutate(lambda = fct_rev(lambda)) %>% 
  ggplot(aes(x = value, fill = lambda)) +
  geom_density(alpha = 0.5) +
  coord_cartesian(xlim = c(-10, 10)) +
  theme_bw() +
  labs(x = TeX("$\\beta$"), y = "Density", fill = TeX("$\\lambda$"),
       title = "Plot of Laplace (LASSO Prior) Distribution") +
  theme(plot.title = element_text(hjust = 0.5),
        panel.grid = element_blank(),
        text = element_text(size = 16)) +
  scale_fill_viridis(discrete = TRUE, option = "B") +
  guides(fill = guide_legend(reverse = TRUE))
```


## Normal-Gamma Prior

While the LASSO is a well-known and often-used technique, empirical evidence has suggested that it may not be optimal. Specifically, the shrinkage effect may not be strong enough to bring coefficients to zero, but the tails may not be heavy enough to allow coefficients to reach their true values.

The normal-gamma prior attempts to solve these problems. It assumes that the mixing distribution follows a gamma distribution, rather than an exponential. This, along with the introduction of another hyperparameter $\xi$, allows the distribution to have a lot of mass close to zero, while maintaining heavy tails (particularly as $\lambda$ decreases). This is similar to a spike-and-slab distribution under the right formulation.

$$
\begin{align*}
  \beta_j | \tau_j &\sim N(0, \tau_j^2) \\
  \tau_j^2 &\sim \text{gamma}(\lambda, 1/(2\xi^2))
\end{align*}
$$


## Horseshoe Prior

The horseshoe class of priors uses a global hyperparameter to shrink all coefficients towards zero and a local hyperparameter to allow some coefficients to adjust the scale of shrinkage locally. These are known as **global-local** shrinkage priors. They can also be represented as scale mixtures of normals,

$$
\begin{align*}
  \beta_j | \lambda_j &\sim N(0, \lambda_j^2) \\
  \lambda_j | \tau &\sim C^+(0, \tau) \\
  \tau | \sigma &\sim C^+(0, \sigma)
\end{align*}
$$

In this case, $C^+(0, \tau)$ is a half-Cauchy distribution. $\lambda_j$ is referred to as the local shrinkage parameter, while $\tau$ is the global shrinkage parameter. With some integration, this yields the shrinkage coefficient $\kappa_j = 1 / (1 + \lambda_j^2)$. The half-Cauchy prior on $\lambda_j$ implies a Beta(1/2, 1/2) prior distribution for $\kappa_j$, which gives the horseshoe its name.

## Horseshoe Prior

```{r}
#| label: horseshoe distribution
#| fig-align: center

set.seed(523)
ggplot(mapping = aes(x = rbeta(100000, 1/2, 1/2))) +
  geom_density(fill = "darkgrey", alpha = 0.5) +
  theme_bw() +
  labs(x = TeX("$\\kappa_j$"), y = "Density",
       title = "Plot of Horseshoe Prior Distribution") +
  theme(plot.title = element_text(hjust = 0.5),
        panel.grid = element_blank(),
        text = element_text(size = 16))
```

The left side of the horseshoe, $\kappa_j \approx 0$, yields almost no shrinkage and represents signals, while the right side of the horseshoe, $\kappa_j \approx 1$, yields near-total shrinkage and represents noise.



# Expectation-Maximization Variable Selection

## Expectation-Maximization Variable Selection

EMVS is an alternative to Gibbs sampling, which is frequently much more computationally efficient and has been shown to have good performance in high-dimensional settings. Setting up the Bayesian regression setting using a spike-and-slab prior, the algorithm repeats two steps until convergence:

 - **Expectation (E) Step**: Calculate the posterior inclusion probabilities for each predictor based on current estimates of parameters.
 - **Maximization (M) Step**: Update parameter estimates by maximizing the expectation of the likelihood over (given) the data.
 
Researchers have suggested that the EMVS algorithm may perform much better when $p >> n$.


