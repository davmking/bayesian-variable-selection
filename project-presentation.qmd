---
title: "Bayesian Methods of Variable Selection"
author: "Dav King"
format: revealjs
---

## Motivation: Why Variable Selection?

 - In modeling, we often have a vast number of possible predictors, but only anticipate a small handful are associated with the outcome variable.
 - Without controlling the number of predictors, we are likely to find significance in predictors that are actually unrelated to the outcome variable.
 - We also have much greater computational complexity with the high number of predictors.
 - By selecting a subset of predictors, we can often reduce variance in the model and create a much simpler, easy-to-interpret outcome.
 - This has particularly useful applications in fields like genomics and the social sciences.

## Frequentist Analogues

 - A tempting solution is to simply perform **best subset selection**, in order to identify the subset of predictors that minimizes RSS.
  - Problem: Combinatorial complexity.
  - There are some workarounds.
 - Another common method is **shrinkage models**, such as LASSO regression.

## Bayesian Statistics: Background

**Bayesian statistics** revolves around the use of Bayes' theorem to update predictions of our parameters based on a data generative model and a prior belief about our parameters:

$$
  p(\mathbf{\beta} | \mathbf{Y}) = \frac{p(\mathbf{Y} | \mathbf{\beta}) p(\beta)}{\int_\beta p(\mathbf{Y} | \mathbf{\beta}) p(\mathbf{\beta}) d\mathbf{\beta}}
$$


## Research Questions

