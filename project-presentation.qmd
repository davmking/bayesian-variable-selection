---
title: "Bayesian Methods of Variable Selection"
author: "Dav King"
format: 
  revealjs:
    css: styles.css
---

## Motivation: Why Variable Selection?

-   In modeling, we often have a vast number of possible predictors, but only anticipate a small handful are associated with the outcome variable.
-   Without controlling the number of predictors, we are likely to find significance in predictors that are actually unrelated to the outcome variable.
-   We also have much greater computational complexity with the high number of predictors.
-   By selecting a subset of predictors, we can often reduce variance in the model and create a much simpler, easy-to-interpret outcome.
-   This has particularly useful applications in fields like genomics and the social sciences.

## Frequentist Analogues

-   A tempting solution is to simply perform **best subset selection**, in order to identify the subset of predictors that minimizes RSS.
-   Problem: Combinatorial complexity.
-   There are some workarounds.
-   Another common method is **shrinkage models**, such as LASSO regression.

## Bayesian Statistics: Background

**Bayesian statistics** revolves around the use of Bayes' theorem to update our beliefs about a probability based on new information:

$$
  p(B | A) = \frac{p(A | B) p(B)}{p(A)}
$$

In **Bayesian modeling**, we use Bayes' theorem to update our beliefs about parameters based on A) our prior beliefs about the parameters and B) the likelihood function of the data:

$$
  p(\mathbf{\beta} | \mathbf{Y}) = \frac{p(\mathbf{Y} | \mathbf{\beta}) p(\mathbf{\beta})}{\int_\mathbf{\beta} p(\mathbf{Y} | \mathbf{\beta}) p(\mathbf{\beta}) d\mathbf{\beta}}
$$

## Bayesian Statistics Background

$$
  p(\mathbf{\beta} | \mathbf{Y}) = \frac{p(\mathbf{Y} | \mathbf{\beta}) p(\mathbf{\beta})}{\int_\mathbf{\beta} p(\mathbf{Y} | \mathbf{\beta}) p(\mathbf{\beta}) d\mathbf{\beta}}
$$

In this model,

 - $p(\mathbf{Y} | \mathbf{\beta})$ is the **Data Generative Model (DGM)**.
 - $p(\mathbf{\beta})$ is the **Prior Distribution**(**).
 - $\int_\mathbf{\beta} p(\mathbf{Y} | \mathbf{\beta}) p(\mathbf{\beta}) d\mathbf{\beta}$ is a **constant**, and we are usually not concerned with it.
 - $p(\mathbf{\beta} | \mathbf{Y})$ is the **Posterior Distribution**.
 
 
## Bayesian Statistics: Markov Chain Monte Carlo

 - Since many posterior distributions are hard or impossible to compute mathematically, we run a **Markov Chain Monte Carlo (MCMC)** algorithm:
  1. Initialize $\beta$ and $\mathbf{Y}$.
  2. Sample $\beta^{(s + 1)}$ based on $\mathbf{Y}^{(s)}$.
  3. Sample $\mathbf{Y}^{(s + 1)}$ based on (updated) $\mathbf{\beta}^{(s + 1)}$.
  4. Sample hyperparameters if needed.
<!-- - **Metropolis-Hastings Algorithm**: propose new values of $\mathbf{\beta}, \mathbf{Y}$, accept/reject based on the likelihood. -->

 - A key component of the MCMC process is that it is **memoryless** - that is, each value in the Markov Chain is only dependent on the sample immediately before it.
 
 
## Why Bayesian?

 - Compared to the frequentist setting, Bayesian approaches allow us to incorporate prior beliefs on the structure of the model.
 - This can help stabilize calculations and produce an outcome structured as we would like it to be.
 - It can also improve computational complexity.
 - Often, convergence is much faster in a Bayesian setting.
 - We can apply priors to groups of variables, etc.


## Research Questions

 1. What methods of Bayesian variable selection exists, and how do they relate to one another?
 2. Which methods perform best in each of the following settings?
 
 $$
  \begin{equation*}
    n >> p \\
    n > p \\
    p > n \\
    p >> n
  \end{equation*}
 $$
 
 3. What are the computation time/accuracy tradeoffs?
 
# Prior-Based Methods

## Overview

 - The general idea of prior-based methods is that we have some prior belief about the structure of our $\beta$s.
 - In particular, we believe that not all of them are included in the true data-generative model.
 - We therefore place a prior distribution on the $\beta$s that reflects this belief.
 - We can add more complicated techniques, such as placing the priors on different groups of $\beta$s (beyond the scope of this project).
 
## Spike-And-Slab Priors 

A spike-and-slab prior is a mixture distribution - i.e., a combination of two distributions, drawing from one or the other with some Bernoulli probability. Spike-and-slab priors have the general form
 
$$
  \beta_j | \gamma_j \sim \gamma_j \phi_1(\beta_j) + (1 - \gamma_j) \phi_0(\beta_j),
$$

where

 - $\phi_1(\beta_j)$ is a diffuse "slab" distribution that allows the $\beta_j$ to reach their true coefficients
 - $\phi_0(\beta_j)$ is a concentrated "spike" distribution that pulls some of the $\beta_j$ to (near) zero
 - $\gamma$ is a latent binary indicator variable representing each of the $2^p$ possible models


## Shrinkage Priors



