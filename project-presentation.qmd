---
title: "Bayesian Methods of Variable Selection"
author: "Dav King"
format: 
  revealjs:
    css: styles.css
---

```{r}
#| label: libraries

library(VGAM)
library(tidyverse)
library(viridis)
library(patchwork)
library(latex2exp)
```


## Motivation: Why Variable Selection?

-   In modeling, we often have a vast number of possible predictors, but only anticipate a small handful are associated with the outcome variable.
-   Without controlling the number of predictors, we are likely to find significance in predictors that are actually unrelated to the outcome variable.
-   We also have much greater computational complexity with the high number of predictors.
-   By selecting a subset of predictors, we can often reduce variance in the model and create a much simpler, easy-to-interpret outcome.
-   This has particularly useful applications in fields like genomics and the social sciences.

## Frequentist Analogues

-   A tempting solution is to simply perform **best subset selection**, in order to identify the subset of predictors that minimizes RSS.
-   Problem: Combinatorial complexity.
-   There are some workarounds.
-   Another common method is **shrinkage models**, such as LASSO regression.

## Bayesian Statistics: Background

**Bayesian statistics** revolves around the use of Bayes' theorem to update our beliefs about a probability based on new information:

$$
  p(B | A) = \frac{p(A | B) p(B)}{p(A)}
$$

In **Bayesian modeling**, we use Bayes' theorem to update our beliefs about parameters based on A) our prior beliefs about the parameters and B) the likelihood function of the data:

$$
  p(\mathbf{\beta} | \mathbf{Y}) = \frac{p(\mathbf{Y} | \mathbf{\beta}) p(\mathbf{\beta})}{\int_\mathbf{\beta} p(\mathbf{Y} | \mathbf{\beta}) p(\mathbf{\beta}) d\mathbf{\beta}}
$$

## Bayesian Statistics Background

$$
  p(\mathbf{\beta} | \mathbf{Y}) = \frac{p(\mathbf{Y} | \mathbf{\beta}) p(\mathbf{\beta})}{\int_\mathbf{\beta} p(\mathbf{Y} | \mathbf{\beta}) p(\mathbf{\beta}) d\mathbf{\beta}}
$$

In this model,

 - $p(\mathbf{Y} | \mathbf{\beta})$ is the **Data Generative Model (DGM)**.
 - $p(\mathbf{\beta})$ is the **Prior Distribution**(**).
 - $\int_\mathbf{\beta} p(\mathbf{Y} | \mathbf{\beta}) p(\mathbf{\beta}) d\mathbf{\beta}$ is a **constant**, and we are usually not concerned with it.
 - $p(\mathbf{\beta} | \mathbf{Y})$ is the **Posterior Distribution**.
 
 
## Bayesian Statistics: Markov Chain Monte Carlo

 - Since many posterior distributions are hard or impossible to compute mathematically, we run a **Markov Chain Monte Carlo (MCMC)** algorithm:
  1. Initialize $\beta$ and $\mathbf{Y}$.
  2. Sample $\beta^{(s + 1)}$ based on $\mathbf{Y}^{(s)}$.
  3. Sample $\mathbf{Y}^{(s + 1)}$ based on (updated) $\mathbf{\beta}^{(s + 1)}$.
  4. Sample hyperparameters if needed.
<!-- - **Metropolis-Hastings Algorithm**: propose new values of $\mathbf{\beta}, \mathbf{Y}$, accept/reject based on the likelihood. -->

 - A key component of the MCMC process is that it is **memoryless** - that is, each value in the Markov Chain is only dependent on the sample immediately before it.
 
 
## Why Bayesian?

 - Compared to the frequentist setting, Bayesian approaches allow us to incorporate prior beliefs on the structure of the model.
 - This can help stabilize calculations and produce an outcome structured as we would like it to be.
 - It can also improve computational complexity.
 - Often, convergence is much faster in a Bayesian setting.
 - We can apply priors to groups of variables, etc.


## Research Questions

 1. What methods of Bayesian variable selection exists, and how do they relate to one another?
 2. Which methods perform best in each of the following settings?
 
 $$
  \begin{equation*}
    n >> p \\
    n > p \\
    p > n \\
    p >> n
  \end{equation*}
 $$
 
 3. What are the computation time/accuracy tradeoffs?
 

# Non Prior-Based Methods

## Bayesian Model Selection

Define a binary variable $\mathbf{\gamma} = \gamma_1, \dots, \gamma_p$, where $\gamma_i$ represents $\beta_i$ being included in the model $\mathcal{M}$. Then we can represent our entire model space $\mathcal{M}_\gamma$ as different realizations of $\gamma$. Place some prior distribution on $\gamma$, often a Bernoulli distribution.

When running the MCMC sampler, propose a new value of $\gamma$ at each step along with $\beta$ and $\mathbf{Y}$, and accept/reject the new model with a probability according to the likelihood ratio of the two models. Then we can generate a posterior distribution for $\gamma$, which shows us the regions of high posterior density, enabling us to see which $\beta$s have a higher probability of being included in the model. 

Often, this procedure converges quickly to one (or a few) strong candidate models, without having to explore the full combinatorial space.


## Bayesian Model Averaging

Bayesian model averaging (BMA) is very similar to Bayesian model selection, with one key difference. In Bayesian model selection, we are looking for the model with the highest posterior probability. In Bayesian model averaging, we simply average over *all* of the candidate models - letting $\beta_i = 0$ if it is not included in a specific candidate model. While unintuitive, this procedure frequently returns highly accurate results of the true coefficients, with (relatively) minimal computational demand.

 
# Prior-Based Methods

## Overview

 - The general idea of prior-based methods is that we have some prior belief about the structure of our $\beta$s.
 - In particular, we believe that not all of them are included in the true data-generative model.
 - We therefore place a prior distribution on the $\beta$s that reflects this belief.
 - We can add more complicated techniques, such as placing the priors on different groups of $\beta$s (beyond the scope of this project).
 
## Spike-And-Slab Priors 

A spike-and-slab prior is a mixture distribution - i.e., a combination of two distributions, drawing from one or the other with some Bernoulli probability. Spike-and-slab priors have the general form
 
$$
  \beta_j | \gamma_j \sim \gamma_j \phi_1(\beta_j) + (1 - \gamma_j) \phi_0(\beta_j),
$$

where

 - $\phi_1(\beta_j)$ is a diffuse "slab" distribution that allows the $\beta_j$ to reach their true coefficients
 - $\phi_0(\beta_j)$ is a concentrated "spike" distribution that pulls some of the $\beta_j$ to (near) zero
 - $\gamma$ is a latent binary indicator variable representing each of the $2^p$ possible models
 
## Stochastic Search Variable Selection

 - SSVS follows the same procedure of embedding the entire modeling process in a hierarchical Bayesian setup, including the model selection $\gamma$
 - The conditional distribution can be represented as a scale mixture of normals:
 
$$ 
  \beta_j | \gamma_j \sim (1 - \gamma_j) N(0, \tau_j^2) + \gamma_j N(0, c_j^2 \tau_j^2) 
$$

 - You can also place prior distributions on $\tau_j^2$ and $c_j$
 - Other priors, such as the normal mixture of inverse gammas (NMIG), extend this process
 
## Stochastic Search Variable Selection


```{r}
#| label: spike and slab distribution

tau <- 1
c <- 4

set.seed(481)
slab <- rnorm(100000, 0, tau^2)
spike <- rnorm(100000, 0, c^2 * tau^2)

data.frame(slab, spike) %>% 
  pivot_longer(everything()) %>% 
  mutate(name = factor(name, levels = c("spike", "slab"))) %>% 
  ggplot(aes(x = value, fill = name)) +
  geom_density(alpha = 0.5) +
  theme_bw() +
  labs(x = TeX("$\\beta$"), y = "Density", fill = "Distribution",
       title = "Plot of SSVS Spike-and-Slab Distribution",
       subtitle = TeX("$\\tau = 1, c = 4$")) +
  theme(plot.title = element_text(hjust = 0.5),
        plot.subtitle = element_text(hjust = 0.5),
        panel.grid = element_blank(),
        legend.position = "bottom",
        text = element_text(size = 16)) +
  scale_fill_viridis(discrete = TRUE, direction = -1) +
  scale_y_continuous(labels = scales::percent_format())
```

## Shrinkage Priors


# Expectation-Maximization Variable Selection


