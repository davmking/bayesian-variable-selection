---
title: "Bayesian Methods of Variable Selection"
author: "Dav King"
format: 
  revealjs:
    css: styles.css
---

```{r}
#| label: libraries

library(VGAM)
library(tidyverse)
library(viridis)
library(patchwork)
library(latex2exp)
library(knitr)
library(kableExtra)
```


## Variable Selection: Motivation

 - In modeling, we frequently have many possible predictors, but anticipate only a few are associated with the outcome
 - Solution: Control the number of predictors
   - Reduce computational complexity
   - Minimize model variance
   - Improve interpretability
 - Useful in fields like genomics \& social sciences
 - Common methods: Best subset selection, LASSO regression, etc.
 
# Bayesian Statistics
 
## Bayesian Statistics: Background

**Bayesian statistics** is a field of statistics that uses Bayes' theorem to update beliefs about a probability based on new information:

$$
  p(B | A) = \frac{p(A | B)p(B)}{p(A)}
$$

In **Bayesian modeling**, we update our beliefs about model parameters based on A) our prior beliefs about the parameters and B) the likelihood function of the data:

$$
  p(\beta | \mathbf{Y}) = \frac{p(\mathbf{Y} | \beta)p(\beta)}{\int_\beta p(\mathbf{Y} | \beta)p(\beta)d\beta}
$$


## Bayesian Statistics: Background

$$
  p(\beta | \mathbf{Y}) = \frac{p(\mathbf{Y} | \beta)p(\beta)}{\int_\beta p(\mathbf{Y} | \beta)p(\beta)d\beta}
$$

In this model,

 - $p(\mathbf{Y} | \beta)$ is the **Data Generative Model**.
 - $p(\beta)$ is the **Prior Distribution**(**).
 - $\int_\beta p(\mathbf{Y} | \beta) p(\beta) d\beta$ is a **constant**, which we can usually ignore.
 - $p(\mathbf{\beta} | \mathbf{Y})$ is the **Posterior Distribution**, which is our target.
 
 
## Bayesian Statistics: Markov Chain Monte Carlo

Since many posterior distributions are hard or impossible to compute by hand, we run a **Markov Chain Monte Carlo (MCMC)** algorithm to sample from the posterior:
 
  1. Initialize $\beta$ and $\mathbf{Y}$.
  2. Sample $\beta^{(s + 1)}$ based on $\mathbf{Y}^{(s)}$.
  3. Sample $\mathbf{Y}^{(s + 1)}$ based on (updated) $\mathbf{\beta}^{(s + 1)}$.
  4. Sample hyperparameters if needed.

A key attribute of the MCMC process is that it is **memoryless** - that is, each value in the Markov Chain is only dependent on the sample immediately before it.


## Why Bayesian?

 - Compared to the frequentist setting, Bayesian approaches allow us to incorporate prior beliefs on the structure of the model.
 - This helps stable calculations and produce an outcome of the desired form.
 - We can also sample over the regions of high probability in the model space much more quickly, often improving computational complexity.
 
 
## Research Questions

 1. What methods of Bayesian variable selection exists, and how do they relate to one another?
 2. Which methods perform best in each of the following settings?
 
$$
  \begin{equation*}
    n >> p \\
    n > p \\
    p > n \\
    p >> n
  \end{equation*}
$$
 
 3. What are the computation time/accuracy tradeoffs?
 
 
# Non Prior-Based Methods

## Bayesian Model Selection (BMS)

 - Let $\gamma = \gamma_1, \dots, \gamma_p$ be a binary indicator variable denoting whether $\beta_i$ is included in model $\mathcal{M}_\gamma$. Different instances of $\gamma$ represent the whole model space $\mathcal{M}$.
 - In the MCMC sampler, propose new values of $\gamma$ along with $\beta$ and $\mathbf{Y}$, and accept/reject based on model likelihood ratio.
 - More probable models will appear more frequently in the resulting posterior distribution of $\gamma$.
 - Often this procedure converges quickly to one (or a few) strong candidate models, despite only exploring a small portion of the model space.

## Bayesian Model Averaging (BMA)

 - Instead of identifying a single model with high posterior density, estimate model coefficients by averaging over all generated models.
   - If $\gamma_i = 0$, then $\beta_i = 0$ for this model.
 - This results in posterior estimates for $\beta$ that are weighted by the probability of each model specification $\gamma$.
 - While unintuitive, this procedure frequently generates highly accurate estimates of the true coefficients, with (relatively) minimal computational demand.
 
 
## Prior-Based Methods

 - Suppose we have some prior belief about the structure of our $\beta$s: specifically, we believe that not all of them are included in the true data-generative model.
 - We place a prior distribution on the $\beta$s that reflects this belief.
 - We can introduce more complicated techniques, such as placing different priors on specific groups of $\beta$s (beyond the scope of this project).
 
 
## Spike-And-Slab Priors

A spike-and-slab prior is a mixture distribution: A combination of two distributions, drawing from one or the other with some Bernoulli probability (according to $\gamma$). Spike-and-slab priors have the general form

$$
  \beta_j | \gamma_j \sim (1 - \gamma_j) \phi_0(\beta_j) + \gamma_j \phi_1(\beta_j),
$$

where

 - $\phi_0(\beta_j)$ is a concentrated "spike" distribution that pulls some of the $\beta$s to (near) zero.
 - $\phi_1(\beta_j)$ is a diffuse "slab" distribution that allows the remaining $\beta$s to attain their true coefficients.
 - A common technique, Stochastic Search Variable Selection (SSVS), represents the two distributions as a scale mixture of normals, and finds highly probable instances of $\gamma$.
 
 
## SSVS Prior Distribution


```{r}
#| label: spike and slab distribution

tau <- 1
c <- 3

set.seed(481)
slab <- rnorm(100000, 0, tau^2)
spike <- rnorm(100000, 0, c^2 * tau^2)

data.frame(slab, spike) %>% 
  pivot_longer(everything()) %>% 
  mutate(name = factor(name, levels = c("spike", "slab"))) %>% 
  ggplot(aes(x = value, fill = name)) +
  geom_density(alpha = 0.5) +
  theme_bw() +
  labs(x = TeX("$\\beta$"), y = "Density", fill = "Distribution",
       title = "Plot of SSVS Spike-and-Slab Distribution",
       subtitle = TeX("$\\tau = 1, c = 3$")) +
  theme(plot.title = element_text(hjust = 0.5),
        plot.subtitle = element_text(hjust = 0.5),
        panel.grid = element_blank(),
        legend.position = "bottom",
        text = element_text(size = 16)) +
  scale_fill_viridis(discrete = TRUE, direction = -1)
```



