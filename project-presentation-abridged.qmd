---
title: "Bayesian Methods of Variable Selection"
author: "Dav King"
format: 
  revealjs:
    css: styles.css
    slideNumber: true
---

```{r}
#| label: libraries

library(VGAM)
library(tidyverse)
library(viridis)
library(patchwork)
library(latex2exp)
library(knitr)
library(kableExtra)
```


## Variable Selection: Motivation

 - In modeling, we frequently have many possible predictors, but anticipate only a few are associated with the outcome
 - Solution: Control the number of predictors
   - Reduce computational complexity
   - Minimize model variance
   - Improve interpretability
 - Useful in fields like genomics \& social sciences
 - Common methods: Best subset selection, LASSO regression, etc.
 - **Problem:** With $p$ parameters, there are $2^p$ possible model formulations, known as the **model space** $\mathcal{M}$
 
# Bayesian Statistics
 
## Bayesian Statistics: Background

**Bayesian statistics** is a field of statistics that uses Bayes' theorem to update beliefs about a probability based on new information:

$$
  p(B | A) = \frac{p(A | B)p(B)}{p(A)}
$$

In **Bayesian modeling**, we update our beliefs about model parameters based on A) our prior beliefs about the parameters and B) the likelihood function of the data:

$$
  p(\beta | \mathbf{Y}) = \frac{p(\mathbf{Y} | \beta)p(\beta)}{\int_\beta p(\mathbf{Y} | \beta)p(\beta)d\beta}
$$


## Bayesian Statistics: Background

$$
  p(\beta | \mathbf{Y}) = \frac{p(\mathbf{Y} | \beta)p(\beta)}{\int_\beta p(\mathbf{Y} | \beta)p(\beta)d\beta}
$$

In this model,

 - $p(\mathbf{Y} | \beta)$ is the **Data Generative Model**.
 - $p(\beta)$ is the **Prior Distribution**(**).
 - $\int_\beta p(\mathbf{Y} | \beta) p(\beta) d\beta$ is a **constant**, which we can usually ignore.
 - $p(\mathbf{\beta} | \mathbf{Y})$ is the **Posterior Distribution**, which is our target.
 
 
## Bayesian Statistics: Markov Chain Monte Carlo

Since many posterior distributions are hard or impossible to compute by hand, we run a **Markov Chain Monte Carlo (MCMC)** algorithm to sample from the posterior:
 
  1. Initialize $\beta$ and $\mathbf{Y}$.
  2. Sample $\beta^{(s + 1)}$ based on $\mathbf{Y}^{(s)}$.
  3. Sample $\mathbf{Y}^{(s + 1)}$ based on (updated) $\mathbf{\beta}^{(s + 1)}$.
  4. Sample hyperparameters if needed.

This allows us to explore parameter space and identify regions of high posterior density. With the right initialization, it often converges very quickly.


## Why Bayesian?

 - Compared to the frequentist setting, Bayesian approaches allow us to incorporate prior beliefs on the structure of the model.
 - This helps stabilize calculations and produce an outcome of the desired form.
 - We can sample hyperparameters in the MCMC process, instead of doing cross-validation.
 - We can also identify regions of high probability in the model space (i.e., highly probable models) much more quickly than in the frequentist setting.
 
 
## Research Questions

 1. What methods of Bayesian variable selection exist, and how do they relate to one another?
 2. Which methods perform best in each of the following settings?
 
$$
  \begin{equation*}
    n >> p \\
    n > p \\
    p > n \\
    p >> n
  \end{equation*}
$$
 
 3. What are the computation time/accuracy tradeoffs?
 
 
# Non Prior-Based Methods

## Bayesian Model Selection (BMS)

 - Let $\gamma_i$ be a binary indicator variable denoting whether $\beta_i$ is included in model $\mathcal{M}_\gamma$. Different instances of $\gamma$ represent the whole model space $\mathcal{M}$.
 - In the MCMC sampler, propose new values of $\gamma$ along with $\beta$ and $\mathbf{Y}$, and accept/reject the proposal based on model likelihood.
 - More probable models will appear more frequently in the resulting posterior distribution of $\gamma$.
 - Often this procedure converges quickly to one (or a few) strong candidate models, despite only exploring a small portion of the model space.

## Bayesian Model Averaging (BMA)

 - Instead of identifying a single model with high posterior density, estimate model coefficients by averaging over all generated models.
   - If $\gamma_i = 0$, then $\beta_i = 0$ for this model.
 - This results in posterior estimates for $\beta$ that are weighted by the probability of each model specification $\gamma$.
 - While unintuitive, this procedure frequently generates highly accurate estimates of the true coefficients, with (relatively) minimal computational demand.
 
# Prior-Based Methods
 
## Prior-Based Methods

 - Suppose we have some prior belief about the structure of our $\beta$s: specifically, we believe that not all of them are included in the true data-generative model.
 - We place a prior distribution on the $\beta$s that reflects this belief.
 - We can introduce more complicated techniques, such as placing different priors on specific groups of $\beta$s (beyond the scope of this project).
 
 
## Spike-And-Slab Priors

A spike-and-slab prior is a mixture distribution: A combination of two distributions, drawing from one or the other with some Bernoulli probability (according to $\gamma$). Spike-and-slab priors have the general form

$$
  \beta_j | \gamma_j \sim (1 - \gamma_j) \phi_0(\beta_j) + \gamma_j \phi_1(\beta_j),
$$

where

 - $\phi_0(\beta_j)$ is a concentrated "spike" distribution that pulls some of the $\beta$s to (near) zero.
 - $\phi_1(\beta_j)$ is a diffuse "slab" distribution that allows the remaining $\beta$s to attain their true coefficients.
 - A common technique, Stochastic Search Variable Selection (SSVS), represents the two distributions as a scale mixture of normals, and finds highly probable instances of $\gamma$.
 
 
## SSVS Prior Distribution


```{r}
#| label: spike and slab distribution

tau <- 1
c <- 3

set.seed(481)
slab <- rnorm(100000, 0, tau^2)
spike <- rnorm(100000, 0, c^2 * tau^2)

data.frame(slab, spike) %>% 
  pivot_longer(everything()) %>% 
  mutate(name = factor(name, levels = c("spike", "slab"))) %>% 
  ggplot(aes(x = value, fill = name)) +
  geom_density(alpha = 0.5) +
  theme_bw() +
  labs(x = TeX("$\\beta$"), y = "Density", fill = "Distribution",
       title = "Plot of SSVS Spike-and-Slab Distribution",
       subtitle = TeX("$\\tau = 1, c = 3$")) +
  theme(plot.title = element_text(hjust = 0.5),
        plot.subtitle = element_text(hjust = 0.5),
        panel.grid = element_blank(),
        legend.position = "bottom",
        text = element_text(size = 16)) +
  scale_fill_viridis(discrete = TRUE, direction = -1) +
  coord_cartesian(xlim = c(-30, 30))
```


## Shrinkage Priors

Unlike spike-and-slab priors, shrinkage priors are single-point, continuous distributions. Their goal is to pull some of the $\beta$s towards zero, while minimally penalizing the remaining coefficients (allowing them to reach their true values).

The Bayesian analogue to LASSO regression places an exponential prior on the variance, which results in a Laplace prior for the $\beta$s. 

The Normal-Gamma prior expands on the Bayesian LASSO, placing a gamma prior on the variance which allows the distribution to have a lot of mass close to zero while maintaining heavy tails for the non-zero coefficients.

## LASSO Prior

```{r}
#| label: LASSO distribution

lambda <- c(0.5, 1, 2, 4)

N_samples <- 100000

lasso_values <- matrix(NA, nrow = N_samples, ncol = length(lambda))

for(l in 1:length(lambda)){
  lasso_values[,l] <- rlaplace(N_samples, 0, 1 / lambda[l])
}


data.frame(lasso_values) %>%
  pivot_longer(everything(), names_to = "lambda_number") %>% 
  mutate(lambda_number = as.integer(substring(lambda_number, 2))) %>% 
  mutate(lambda = lambda[lambda_number]) %>% 
  mutate(lambda = factor(lambda)) %>% 
  mutate(lambda = fct_rev(lambda)) %>% 
  ggplot(aes(x = value, fill = lambda)) +
  geom_density(alpha = 0.5) +
  coord_cartesian(xlim = c(-10, 10)) +
  theme_bw() +
  labs(x = TeX("$\\beta$"), y = "Density", fill = TeX("$\\lambda$"),
       title = "Plot of Laplace (LASSO Prior) Distribution") +
  theme(plot.title = element_text(hjust = 0.5),
        panel.grid = element_blank(),
        text = element_text(size = 16)) +
  scale_fill_viridis(discrete = TRUE, option = "B") +
  guides(fill = guide_legend(reverse = TRUE))
```

## Horseshoe Prior

Horseshoe priors use a global hyperparameter to shrink all coefficients towards zero, while maintaining a local hyperparameter to adjust the scale of shrinkage locally for some coefficients. This is known as a **global-local** shrinkage prior. 

The horseshoe prior setting results in a shrinkage coefficient that places a lot of mass near zero (indicating signals) and one (indicating noise), giving the horseshoe prior its name.


## Horseshoe Prior

```{r}
#| label: horseshoe distribution
#| fig-align: center

set.seed(523)
ggplot(mapping = aes(x = rbeta(1000000, 1/2, 1/2))) +
  geom_density(fill = "darkgrey", alpha = 0.5) +
  theme_bw() +
  labs(x = TeX("$\\kappa_j$"), y = "Density",
       title = "Plot of Horseshoe Prior Distribution") +
  theme(plot.title = element_text(hjust = 0.5),
        panel.grid = element_blank(),
        text = element_text(size = 16))
```

# Expectation-Maximization Variable Selection

## Expectation-Maximization Variable Selection

As an alternative to MCMC sampling, the EMVS algorithm performs Bayesian regression with a spike-and-slab prior, repeating two steps until convergence:

 - **Expectation (E) Step:** Calculate the posterior inclusion probabilities for each $\beta_j$ based on current parameter estimates.
 - **Maximization (M) Step:** Update parameter estimates by maximizing the expectation of the data likelihood.
 
Researchers have suggested that the EMVS algorithm may perform much better than MCMC when $p >> n$. It is also much more computationally efficient.

# Experiment

## Setup

 - Focused on synthetic data for comparison
 - Low-observation setting for the sake of runtime
 - All runtimes are listed by orders of magnitude (seconds, minutes, hours, etc.)
 - Focused on four settings:

$$
\begin{equation*}
  n >> p \\
  n > p \\
  p > n \\
  p >> n
\end{equation*}
$$

 - Focused only on sparse signal settings: All algorithms performed poorly in dense signal setting
 
 
## Model 1: $n >> p$

$n = 100$, $p = 10$, $p\_sig = 3$

```{r}
#| label: model 1 table

load("model_0_rates.RData")
model_0_rates %>% 
  as.data.frame() %>% 
  mutate(Size = as.numeric(Size),
         TPR = as.numeric(TPR),
         FNR = as.numeric(FNR),
         FPR = as.numeric(FPR),
         TNR = as.numeric(TNR),
         MSE_Nonzero = as.numeric(MSE_Nonzero),
         MSE_All = as.numeric(MSE_All),
         MSE_Resp = as.numeric(MSE_Resp),
         MSE_Resp_Test = as.numeric(MSE_Resp_Test)) %>% 
  kable(digits = 2, format = "html",
        col.names = c("Name", "Size", "TPR", "FNR", "FPR", "TNR", "Time",
                      "MSE\nCoef\nSignal", "MSE\nCoef\nAll", "MSE\nTrain\nY",
                      "MSE\nTest\nY")) %>%
  kable_styling(full_width = TRUE, position = "center", font_size = 25)
```

## Model 2: $n > p$

$n = 100$, $p = 95$, $p\_sig = 8$

```{r}
#| label: model 2 table

load("model_3_rates.RData")
model_3_rates %>% 
  as.data.frame() %>% 
  mutate(Size = as.numeric(Size),
         TPR = as.numeric(TPR),
         FNR = as.numeric(FNR),
         FPR = as.numeric(FPR),
         TNR = as.numeric(TNR),
         MSE_Nonzero = as.numeric(MSE_Nonzero),
         MSE_All = as.numeric(MSE_All),
         MSE_Resp = as.numeric(MSE_Resp),
         MSE_Resp_Test = as.numeric(MSE_Resp_Test)) %>% 
  kable(digits = 2, format = "html",
        col.names = c("Name", "Size", "TPR", "FNR", "FPR", "TNR", "Time",
                      "MSE\nCoef\nSignal", "MSE\nCoef\nAll", "MSE\nTrain\nY",
                      "MSE\nTest\nY")) %>%
  kable_styling(full_width = TRUE, position = "center", font_size = 25)
```

## Model 3: $p > n$

$n = 100$, $p = 105$, $p\_sig = 8$

```{r}
#| label: model 3 table

load("model_4_rates.RData")
model_4_rates %>% 
  as.data.frame() %>% 
  mutate(Size = as.numeric(Size),
         TPR = as.numeric(TPR),
         FNR = as.numeric(FNR),
         FPR = as.numeric(FPR),
         TNR = as.numeric(TNR),
         MSE_Nonzero = as.numeric(MSE_Nonzero),
         MSE_All = as.numeric(MSE_All),
         MSE_Resp = as.numeric(MSE_Resp),
         MSE_Resp_Test = as.numeric(MSE_Resp_Test)) %>% 
  kable(digits = 2, format = "html",
        col.names = c("Name", "Size", "TPR", "FNR", "FPR", "TNR", "Time",
                      "MSE\nCoef\nSignal", "MSE\nCoef\nAll", "MSE\nTrain\nY",
                      "MSE\nTest\nY")) %>%
  kable_styling(full_width = TRUE, position = "center", font_size = 25)
```

## Model 4: $p >> n$

$n = 100$, $p = 1000$, $p\_sig = 8$

```{r}
#| label: model 4 table

load("model_5_rates.RData")
model_5_rates %>% 
  as.data.frame() %>% 
  mutate(Size = as.numeric(Size),
         TPR = as.numeric(TPR),
         FNR = as.numeric(FNR),
         FPR = as.numeric(FPR),
         TNR = as.numeric(TNR),
         MSE_Nonzero = as.numeric(MSE_Nonzero),
         MSE_All = as.numeric(MSE_All),
         MSE_Resp = as.numeric(MSE_Resp),
         MSE_Resp_Test = as.numeric(MSE_Resp_Test)) %>% 
  kable(digits = 2, format = "html",
        col.names = c("Name", "Size", "TPR", "FNR", "FPR", "TNR", "Time",
                      "MSE\nCoef\nSignal", "MSE\nCoef\nAll", "MSE\nTrain\nY",
                      "MSE\nTest\nY")) %>%
  kable_styling(full_width = TRUE, position = "center", font_size = 25)
```

# Takeaways

## Takeaways

 - In low-dimensional settings, all algorithms are very fast and effective.
 - As dimensionality increases, BMA and BMS become less useful; runtime increases dramatically for prior-based methods.
 - Generally, EMVS performs pretty well, and it is so efficient that it leaves much more room for tuning.
 - EMVS and BMA have a tendency to overfit (might be reduced with more data).
 - No model is unilaterally the best! They may have different performance in another setting - there are many more variables at play here.
 
## Winners

 - **Signal Coefficient Accuracy:** EMVS
 - **All Coefficient Accuracy:** Horseshoe
 - **Training Set Accuracy:** BMA
 - **Testing Set Accuracy:** All Shrinkage Priors
 - **Model Accuracy:** Varies, usually EMVS
 - **Runtime:** EMVS
 
# Questions?

## References

All source code for this project and citations can be found at github.com/davmking/bayesian-variable-selection. References, which will be formalized in the final report, can be found in the file references.bib; Chat GPT citations can be found in the file chat-gpt-prompts.txt.




