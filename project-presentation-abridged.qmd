---
title: "Bayesian Methods of Variable Selection"
author: "Dav King"
format: 
  revealjs:
    css: styles.css
    slideNumber: true
---

```{r}
#| label: libraries

library(VGAM)
library(tidyverse)
library(viridis)
library(patchwork)
library(latex2exp)
library(knitr)
library(kableExtra)
```


## Variable Selection: Motivation

 - In modeling, we frequently have many possible predictors, but anticipate only a few are associated with the outcome
 - Solution: Control the number of predictors
   - Reduce computational complexity
   - Minimize model variance
   - Improve interpretability
 - Useful in fields like genomics \& social sciences
 - **Problem:** With $p$ parameters, there are $2^p$ possible model formulations, known as the **model space** $\mathcal{M}$
 
# Bayesian Statistics
 
## Bayesian Statistics: Background

**Bayesian statistics** is a field of statistics that uses Bayes' theorem to update beliefs about a probability based on new information:

$$
  p(B | A) = \frac{p(A | B)p(B)}{p(A)}
$$

In **Bayesian modeling**, we use Bayes' theorem to update our beliefs about model parameters:

$$
  p(\beta | \mathbf{Y}) = \frac{p(\mathbf{Y} | \beta)p(\beta)}{\int_\beta p(\mathbf{Y} | \beta)p(\beta)d\beta}
$$


## Bayesian Statistics: Background

$$
  p(\beta | \mathbf{Y}) = \frac{p(\mathbf{Y} | \beta)p(\beta)}{\int_\beta p(\mathbf{Y} | \beta)p(\beta)d\beta}
$$

In this equation,

 - $p(\mathbf{Y} | \beta)$ is the **Data Generative Model**.
 - $p(\beta)$ is the **Prior Distribution**(**).
 - $\int_\beta p(\mathbf{Y} | \beta) p(\beta) d\beta$ is a **constant**, which we can usually ignore.
 - $p(\mathbf{\beta} | \mathbf{Y})$ is the **Posterior Distribution**, which is our target.
 
 
## Bayesian Statistics: Markov Chain Monte Carlo

Since many posterior distributions are hard or impossible to compute by hand, we run a **Markov Chain Monte Carlo (MCMC)** algorithm to sample from the posterior:
 
  1. Initialize $\beta$ and $\mathbf{Y}$.
  2. Sample $\beta^{(s + 1)}$ based on $\mathbf{Y}^{(s)}$.
  3. Sample $\mathbf{Y}^{(s + 1)}$ based on (updated) $\mathbf{\beta}^{(s + 1)}$.
  4. Sample hyperparameters if needed.

This allows us to explore parameter space and identify regions of high posterior density. With the right initialization, it often converges very quickly.


## Why Bayesian?

 - Compared to the frequentist setting, Bayesian approaches allow us to incorporate prior beliefs on the structure of the model.
 - This helps stabilize calculations and produce an outcome of the desired form.
 - We can sample hyperparameters in the MCMC process, instead of doing cross-validation.
 - We can also identify regions of high probability in the model space (i.e., highly probable models) much more quickly than in the frequentist setting.
 
 
## Research Questions

 1. What methods of Bayesian variable selection exist, and how do they relate to one another?
 2. Which methods perform best in each of the following settings?
 
$$
  \begin{equation*}
    n >> p \\
    n > p \\
    p > n \\
    p >> n
  \end{equation*}
$$
 
 3. What are the computation time/accuracy tradeoffs?
 
 
# Non Prior-Based Methods

## Bayesian Model Selection (BMS)

 - Define $\gamma = \gamma_1, \dots, \gamma_p$ so that $\gamma_i$ is a binary indicator variable denoting whether $\beta_i$ is included in model $\mathcal{M}_\gamma$. Different instances of $\gamma$ represent the whole model space $\mathcal{M}$.
 - In the MCMC sampler, propose new values of $\gamma$ along with $\beta$ and $\mathbf{Y}$, and accept/reject the proposal based on model likelihood.
 - More probable models will appear more frequently in the resulting posterior distribution of $\gamma$.
 - Often this procedure converges quickly to one (or a few) strong candidate models, despite only exploring a small portion of the model space.

## Bayesian Model Averaging (BMA)

 - Instead of identifying a single model with high posterior density, estimate model coefficients by averaging over all generated models.
   - If $\gamma_i = 0$, then $\beta_i = 0$ for this model.
 - This results in posterior estimates for $\beta$ that are weighted by the probability of each model specification $\gamma$.
 - While unintuitive, this procedure frequently generates highly accurate estimates of the true coefficients, with (relatively) minimal computational demand.
 
# Prior-Based Methods
 
## Prior-Based Methods

 - Suppose we have some prior belief about the structure of our $\beta$s: specifically, we believe that not all of them are included in the true data-generative model.
 - We place a prior distribution on the $\beta$s that reflects this belief.
 - This produces an outcome of the desired form.
 
 
## Spike-And-Slab Priors

A spike-and-slab prior is a mixture distribution: A combination of two distributions, drawing from one or the other with some Bernoulli probability (according to $\gamma$). Spike-and-slab priors have the general form

$$
  \beta_j | \gamma_j \sim (1 - \gamma_j) \phi_0(\beta_j) + \gamma_j \phi_1(\beta_j),
$$

where

 - $\phi_0(\beta_j)$ is a concentrated "spike" distribution that pulls some of the $\beta$s to (near) zero.
 - $\phi_1(\beta_j)$ is a diffuse "slab" distribution that allows the remaining $\beta$s to attain their true coefficients.
 - A common technique, Stochastic Search Variable Selection (SSVS), represents the two distributions as normals, and finds highly probable instances of $\gamma$.
 
 
## SSVS Prior Distribution


```{r}
#| label: spike and slab distribution

tau <- 1
c <- 3

set.seed(481)
spike <- rnorm(100000, 0, tau^2)
slab <- rnorm(100000, 0, c^2 * tau^2)

data.frame(slab, spike) %>% 
  pivot_longer(everything()) %>% 
  mutate(name = factor(name, levels = c("slab", "spike"))) %>% 
  ggplot(aes(x = value, fill = name)) +
  geom_density(alpha = 0.5) +
  theme_bw() +
  labs(x = TeX("$\\beta$"), y = "Density", fill = "Distribution",
       title = "Plot of SSVS Spike-and-Slab Distribution",
       subtitle = TeX("$\\tau = 1, c = 3$")) +
  theme(plot.title = element_text(hjust = 0.5),
        plot.subtitle = element_text(hjust = 0.5),
        panel.grid = element_blank(),
        legend.position = "bottom",
        text = element_text(size = 16)) +
  scale_fill_viridis(discrete = TRUE, direction = -1) +
  coord_cartesian(xlim = c(-30, 30))
```


## Shrinkage Priors

Unlike spike-and-slab priors, shrinkage priors are single-point, continuous distributions. Their goal is to pull some of the $\beta$s towards zero, while minimally penalizing the remaining coefficients (allowing them to reach their true values).

The Bayesian analogue to LASSO is hierarchical: $\beta$s are drawn from a normal distribution, with random variance drawn from an exponential prior, resulting in a conditional Laplace prior for the $\beta$s. 

The Normal-Gamma prior modifies the Bayesian LASSO, placing a gamma prior on the variance, which allows the distribution to have a lot of mass close to zero while maintaining heavy tails for the non-zero coefficients.

## LASSO Prior

```{r}
#| label: LASSO distribution

lambda <- c(0.5, 1, 2, 4)

N_samples <- 100000

lasso_values <- matrix(NA, nrow = N_samples, ncol = length(lambda))

for(l in 1:length(lambda)){
  lasso_values[,l] <- rlaplace(N_samples, 0, 1 / lambda[l])
}


data.frame(lasso_values) %>%
  pivot_longer(everything(), names_to = "lambda_number") %>% 
  mutate(lambda_number = as.integer(substring(lambda_number, 2))) %>% 
  mutate(lambda = lambda[lambda_number]) %>% 
  mutate(lambda = factor(lambda)) %>% 
  mutate(lambda = fct_rev(lambda)) %>% 
  ggplot(aes(x = value, fill = lambda)) +
  geom_density(alpha = 0.5) +
  coord_cartesian(xlim = c(-10, 10)) +
  theme_bw() +
  labs(x = TeX("$\\beta$"), y = "Density", fill = TeX("$\\lambda$"),
       title = "Plot of Laplace (LASSO Prior) Distribution") +
  theme(plot.title = element_text(hjust = 0.5),
        panel.grid = element_blank(),
        text = element_text(size = 16)) +
  scale_fill_viridis(discrete = TRUE, option = "B") +
  guides(fill = guide_legend(reverse = TRUE))
```

## Horseshoe Prior

Horseshoe priors use a global hyperparameter to shrink all coefficients towards zero, while maintaining a local hyperparameter to adjust the scale of shrinkage for some coefficients. This is known as a **global-local** shrinkage prior. 

The horseshoe prior setting results in a shrinkage coefficient that places a lot of mass near 0 (indicating signals) and 1 (indicating noise), giving the horseshoe prior its name.


## Horseshoe Prior

```{r}
#| label: horseshoe distribution
#| fig-align: center

set.seed(523)
ggplot(mapping = aes(x = rbeta(1000000, 1/2, 1/2))) +
  geom_density(fill = "darkgrey", alpha = 0.5) +
  theme_bw() +
  labs(x = TeX("$\\kappa_j$"), y = "Density",
       title = "Plot of Horseshoe Prior Distribution") +
  theme(plot.title = element_text(hjust = 0.5),
        panel.grid = element_blank(),
        text = element_text(size = 16))
```

# Expectation-Maximization Variable Selection

## Expectation-Maximization Variable Selection

As an alternative to MCMC sampling, the EMVS algorithm performs Bayesian regression with a spike-and-slab prior, repeating two steps until convergence:

 - **Expectation (E) Step:** Calculate the posterior inclusion probabilities (expectations) for each $\beta_j$ based on current parameter estimates.
 - **Maximization (M) Step:** Update parameter estimates by maximizing the objective function of all model parameters.
 
Researchers have suggested that the EMVS algorithm may perform much better than MCMC when $p >> n$. It is also much more computationally efficient.

Smoothing can be added with **deterministic annealing**, which can improve the model's tendency to overfit (discussed in report).

# Experiment

## Setup

 - Focused on synthetic data for comparison
 - Low-observation setting for the sake of runtime
 - All runtimes are listed by orders of magnitude (seconds, minutes, hours, etc.)
 - Focused on four settings:

$$
\begin{equation*}
  n >> p \\
  n > p \\
  p > n \\
  p >> n
\end{equation*}
$$

 - Focused only on sparse signal settings: All algorithms performed poorly in dense signal setting
 
 
## Model 1: $n >> p$

$n = 100$, $p = 10$, $p\_sig = 3$

```{r}
#| label: model 1 table

load("model_0_rates.RData")
model_0_rates %>% 
  as.data.frame() %>% 
  mutate(Size = as.numeric(Size),
         TPR = as.numeric(TPR),
         FNR = as.numeric(FNR),
         FPR = as.numeric(FPR),
         TNR = as.numeric(TNR),
         MSE_Nonzero = as.numeric(MSE_Nonzero),
         MSE_All = as.numeric(MSE_All),
         MSE_Resp = as.numeric(MSE_Resp),
         MSE_Resp_Test = as.numeric(MSE_Resp_Test)) %>% 
  kable(digits = 2, format = "html",
        col.names = c("Name", "Size", "TPR", "FNR", "FPR", "TNR", "Time",
                      "MSE\nCoef\nSignal", "MSE\nCoef\nAll", "MSE\nTrain\nY",
                      "MSE\nTest\nY")) %>%
  kable_styling(full_width = TRUE, position = "center", font_size = 25)
```

## Model 2: $n > p$

$n = 100$, $p = 95$, $p\_sig = 8$

```{r}
#| label: model 2 table

load("model_3_rates.RData")
model_3_rates %>% 
  as.data.frame() %>% 
  mutate(Size = as.numeric(Size),
         TPR = as.numeric(TPR),
         FNR = as.numeric(FNR),
         FPR = as.numeric(FPR),
         TNR = as.numeric(TNR),
         MSE_Nonzero = as.numeric(MSE_Nonzero),
         MSE_All = as.numeric(MSE_All),
         MSE_Resp = as.numeric(MSE_Resp),
         MSE_Resp_Test = as.numeric(MSE_Resp_Test)) %>% 
  kable(digits = 2, format = "html",
        col.names = c("Name", "Size", "TPR", "FNR", "FPR", "TNR", "Time",
                      "MSE\nCoef\nSignal", "MSE\nCoef\nAll", "MSE\nTrain\nY",
                      "MSE\nTest\nY")) %>%
  kable_styling(full_width = TRUE, position = "center", font_size = 25)
```

## Model 3: $p > n$

$n = 100$, $p = 105$, $p\_sig = 8$

```{r}
#| label: model 3 table

load("model_4_rates.RData")
model_4_rates %>% 
  as.data.frame() %>% 
  mutate(Size = as.numeric(Size),
         TPR = as.numeric(TPR),
         FNR = as.numeric(FNR),
         FPR = as.numeric(FPR),
         TNR = as.numeric(TNR),
         MSE_Nonzero = as.numeric(MSE_Nonzero),
         MSE_All = as.numeric(MSE_All),
         MSE_Resp = as.numeric(MSE_Resp),
         MSE_Resp_Test = as.numeric(MSE_Resp_Test)) %>% 
  kable(digits = 2, format = "html",
        col.names = c("Name", "Size", "TPR", "FNR", "FPR", "TNR", "Time",
                      "MSE\nCoef\nSignal", "MSE\nCoef\nAll", "MSE\nTrain\nY",
                      "MSE\nTest\nY")) %>%
  kable_styling(full_width = TRUE, position = "center", font_size = 25)
```

## Model 4: $p >> n$

$n = 100$, $p = 1000$, $p\_sig = 8$

```{r}
#| label: model 4 table

load("model_5_rates.RData")
model_5_rates %>% 
  as.data.frame() %>% 
  mutate(Size = as.numeric(Size),
         TPR = as.numeric(TPR),
         FNR = as.numeric(FNR),
         FPR = as.numeric(FPR),
         TNR = as.numeric(TNR),
         MSE_Nonzero = as.numeric(MSE_Nonzero),
         MSE_All = as.numeric(MSE_All),
         MSE_Resp = as.numeric(MSE_Resp),
         MSE_Resp_Test = as.numeric(MSE_Resp_Test)) %>% 
  kable(digits = 2, format = "html",
        col.names = c("Name", "Size", "TPR", "FNR", "FPR", "TNR", "Time",
                      "MSE\nCoef\nSignal", "MSE\nCoef\nAll", "MSE\nTrain\nY",
                      "MSE\nTest\nY")) %>%
  kable_styling(full_width = TRUE, position = "center", font_size = 25)
```

# Takeaways

## Takeaways

 - In low-dimensional settings, all algorithms are very fast and effective.
 - As dimensionality increases, BMA and BMS become less useful; runtime increases dramatically for prior-based methods.
 - Generally, EMVS performs pretty well, and it is so efficient that it leaves much more room for tuning.
 - EMVS and BMA have a tendency to overfit (might be reduced with more training data).
 - No model is unilaterally the best! They may have different performance in another setting - there are many more variables at play here.
 
## Winners

 - **Signal Coefficient Accuracy:** EMVS
 - **All Coefficient Accuracy:** Horseshoe
 - **Training Set Accuracy:** BMA
 - **Testing Set Accuracy:** All Shrinkage Priors
 - **Model Accuracy:** Varies, usually EMVS
 - **Runtime:** EMVS
 
# Questions?

# Appendix

## References

All source code for this project and citations can be found at github.com/davmking/bayesian-variable-selection. References, which will be formalized in the final report, can be found in the file references.bib; Chat GPT citations can be found in the file chat-gpt-prompts.txt.

## Bayesian Definitions

 - **Prior Distribution:** Our beliefs about the distribution of a paramter *a priori*, i.e. before seeing the data.
 - **Data Generative Model:** The true mechanism by which the data are generated (including some zero-mean random noise).
 - **Normalizing Constant:** The constant required to make a probability distribution integrate to 1. Generally speaking, this is absorbed into proportionality.
 - **(Joint) Posterior Distribution:** The distribution of (all) variables, incorporating both our prior beliefs and our data generative model's likelihood function.
 - **Full Conditional Distribution:** The distribution of a parameter given all other parameters. This is always proportional to the joint posterior. We usually sample from the full conditional in MCMC sampling.

## Gibbs and Metropolis-Hastings Algorithms

Two of the most common MCMC algorithms are Gibbs samplers and Metropolis-Hastings samplers. 

In **Gibbs sampling**, the full conditional has a closed-form expression, and you simply sample from the full conditional for each parameter. This is much more computationally efficient, but requires a specific setting that is frequently untrue.

In **Metropolis-Hastings sampling**, the full conditional does not have a closed-form expression. Instead, you generate a proposal value (usually very close to the current parameter value), and calculate the likelihood ratio $\alpha = \ell(\text{proposal}) / \ell(\text{current})$. You then accept the proposal with probability $\text{min}\{\alpha, 1\}$ (i.e., if the proposal has a higher likelihood, it is always accepted; if not, it is still sometimes accepted so that we can continue to explore parameter space).


## Stochastic Search Variable Selection

 - Just like in Bayesian model selection, SSVS embeds the entire modeling processing in a hierarchical Bayesian setup, including the model selection $\gamma$.
 - The conditional distribution is represented as a scale mixture of normal:
 
$$
  \beta_j | \gamma_j \sim (1 - \gamma_j) N(0, \tau_j^2) + \gamma_j N(0, c_j^2 \tau_j^2)
$$

 - You can place prior distributions on $\tau_j^2$ and $c_j$, or hold them constant.
 - Other priors, such as the normal mixture of inverse gammas (NMIG), extend this process.
 
 
## LASSO and NG Specifications

The hierarchical representation of the full conditional model under the LASSO prior is given below:

$$
\begin{align*}
  \beta_j | \tau_j &\sim N(0, \sigma^2 \tau_j^2) \\
  \tau_j^2 | \lambda &\sim \text{exp}(\lambda^2 / 2)
\end{align*}
$$

The hierarchical representation of the full conditional model under the normal-gamma prior is given below:

$$
\begin{align*}
  \beta_j | \tau_j &\sim N(0, \tau_j^2) \\
  \tau_j^2 &\sim \text{gamma}(\lambda, 1/(2\xi^2))
\end{align*}
$$

## Horseshoe Prior

The hierarchical representation for the full conditional model under the horseshoe prior is given below:

$$
\begin{align*}
  \beta_j | \eta_j &\sim N(0, \eta_j^2) \\
  \eta_j | \tau &\sim C^+(0, \tau) \\
  \tau | \sigma &\sim C^+(0, \sigma)
\end{align*}
$$

In this case, $C^+$ is a half-Cauchy distribution. Here, $\eta_j$ is the local shrinkage parameter, while $\tau$ is the global shrinkage parameter. With some integration, this yields the shrinkage coefficient $\kappa_j = 1 / (1 + \eta_j^2)$. The half-cauchy prior on $\eta_j$ implies a Beta(1/2, 1/2) prior distribution for $\kappa_j$, which yields the horseshoe.

## Expectation-Maximization Variable Selection

Equations for the objective function and calculations to be taken at the E and M steps can be found in Rockova (2013), pp. 44-47.




