---
title: "experiment"
format: html
---

# Libraries and Data

## Libraries

```{r}
#| label: libraries

library(coda)
library(MCMCpack)
library(BayesVarSel)
library(BMS)
library(BoomSpikeSlab)
library(spikeSlabGAM)
library(bayeslm)
library(BayesSubsets)
#library(EMVS)  # Man
library(tidyverse)
```


## Data

Here is a table that describes each model and its characteristics:

| Name | n | p | p_sig |  n vs. p | Description |
|------|---|---|-------|----------|-------------|
| Model 1 | 100 | 10 | 3 | n >> p | Few predictors, sparse |
| Model 2 | 100 | 95 | 75 | n > p | n slightly > p, dense |
| Model 3 | 100 | 95 | 5 | n > p | n slightly > p, sparse |
| Model 4 | 100 | 105 | 95 | n < p | n slightly < p, dense |
| Model 5 | 100 | 105 | 5 | n < p | n slightly < p, sparse |
| Model 6 | 100 | 1000 | 950 | n << p | Many predictors, dense |
| Model 7 | 100 | 1000 | 5 | n << p | Many predictors, sparse |

```{r}
#| label: generate data

set.seed(465)

model_1 <- simulate_lm(100, 10, p_sig = 3)
model_2 <- simulate_lm(100, 95, p_sig = 75)
model_3 <- simulate_lm(100, 95, p_sig = 5)
model_4 <- simulate_lm(100, 105, p_sig = 95)
model_5 <- simulate_lm(100, 105, p_sig = 5)
model_6 <- simulate_lm(100, 1000, p_sig = 950)
model_7 <- simulate_lm(100, 1000, p_sig = 5)
```

## Hyperparameters

```{r}
#| label: defining hyperparameters

N_SAMPLES <- 11000
BURN_IN <- 1000

N_SAMPLES_2 <- 31000
```



# Non-Prior Methods

## Subset Selection

Note: For now, I am skipping the models with dense signals. I don't think it makes much sense to run them, and I think the computation time is too difficult.

Note: This code needs to be run overnight. It is not currently running fast enough to be useful.

### Model 1

```{r}
#| label: run bayeslm and evaluate

set.seed(465)

X <- model_1$X
y <- model_1$y

model_1_fit <- bayeslm(y ~ X[,-1],
                       N = N_SAMPLES,
                       burnin = BURN_IN,
                       prior = "ridge"
                       )

temp <- post_predict(post_y_hat = tcrossprod(model_1_fit$beta, X),
                    post_sigma = model_1_fit$sigma,
                    yy = y)
post_y_pred = temp$post_y_pred
post_lpd = temp$post_lpd

indicators_1 <- branch_and_bound(yy = fitted(model_1_fit),
                              XX = X)

accept_info <- accept_family(post_y_pred = post_y_pred,
                            post_lpd = post_lpd,
                            XX = X,
                            indicators = indicators_1,
                            yy = y,
                            post_y_hat = tcrossprod(model_1_fit$beta, X))



# How many subsets are in the acceptable family?
length(accept_info$all_accept) # 127

# Simplest acceptable subset:
beta_hat_small <- accept_info$beta_hat_small
beta_hat_small

# Which coefficients are nonzero:
S_small <- which(beta_hat_small != 0)
S_small

# How many coefficients are nonzero:
length(S_small) # 1

# Acceptable subset that minimizes CV error:
beta_hat_min <- accept_info$beta_hat_min

# Typically much larger (and often too large...)
sum(beta_hat_min != 0) # 6

# Variable importance metrics
vi_e <- var_imp(indicators = indicators_1,
               all_accept = accept_info$all_accept)$vi_inc

# Variables appearing in all acceptable subsets
all_acceptable <- which(vi_e == 1)
all_acceptable

# Size
length(all_acceptable) # 1
```

With $n = 100$, we aren't picking up the right variables - the smallest subsets are just the intercept. Might need to increase the SNR or n?


### Model 3

```{r}
#| label: run bayeslm and evaluate 3

set.seed(465)

X <- model_3$X
y <- model_3$y

model_3_fit <- bayeslm(y ~ X[,-1],
                       N = N_SAMPLES,
                       burnin = BURN_IN,
                       prior = "ridge"
                       )

temp <- post_predict(post_y_hat = tcrossprod(model_3_fit$beta, X),
                    post_sigma = model_3_fit$sigma,
                    yy = y)
post_y_pred = temp$post_y_pred
post_lpd = temp$post_lpd

start <- Sys.time()
indicators_3 <- branch_and_bound(yy = fitted(model_3_fit),
                              XX = X)
print(Sys.time() - start)

accept_info <- accept_family(post_y_pred = post_y_pred,
                            post_lpd = post_lpd,
                            XX = X,
                            indicators = indicators_1,
                            yy = y,
                            post_y_hat = tcrossprod(model_1_fit$beta, X))



# How many subsets are in the acceptable family?
length(accept_info$all_accept) # 76

# These are the rows of `indicators` that belong to the acceptable family:
head(accept_info$all_accept)

# An example acceptable subset:
ex_accept <- accept_info$all_accept[1]
which(indicators[ex_accept,])

# Simplest acceptable subset:
beta_hat_small <- accept_info$beta_hat_small

# Which coefficients are nonzero:
S_small <- which(beta_hat_small != 0)

# How many coefficients are nonzero:
length(S_small) # 4

# Acceptable subset that minimizes CV error:
beta_hat_min <- accept_info$beta_hat_min

# Typically much larger (and often too large...)
sum(beta_hat_min != 0) # 5

# Variable importance metrics
vi_e <- var_imp(indicators = indicators_1,
               all_accept = accept_info$all_accept)$vi_inc

# Variables appearing in all acceptable subsets
all_acceptable <- which(vi_e == 1) # Correct variables

# Size
length(all_acceptable) # 4
```


## Bayesian Model Selection

### Model 1

```{r}
#| label: bms 1

y <- model_1$y
X <- model_1$X
data <- data.frame(y = y, X[,-1])

bvs_1 <- GibbsBvs(
  y ~ .,
  data = data,
  prior.betas = "gZellner",
  prior.models = "ScottBerger",
  n.iter = N_SAMPLES - BURN_IN,
  n.burnin = BURN_IN,
  time.test = TRUE,
  seed = 465
)

# Most probable model
bvs_1$HPMbin 

# Inclusion probabilities
bvs_1$inclprob

# Runtime
bvs_1$time
```

Recovers the correct variables. Not super slow in terms of efficiency.

### Model 2

```{r}
#| label: bms 2

y <- model_2$y
X <- model_2$X
data <- data.frame(y = y, X[,-1])

bvs_2 <- GibbsBvs(
  y ~ .,
  data = data,
  prior.betas = "gZellner",
  prior.models = "ScottBerger",
  n.iter = N_SAMPLES - BURN_IN,
  n.burnin = BURN_IN,
  time.test = TRUE,
  seed = 465
)

# Most probable model
bvs_2$HPMbin 

# Inclusion probabilities
bvs_2$inclprob

# Runtime
bvs_2$time
```

Much worse runtime than Model 3 has. I think this algorithm should not be used on dense signals. It also does not correctly converge to the right subset - it kinda just says everything should be in the model.


### Model 3

```{r}
#| label: bms 3

y <- model_3$y
X <- model_3$X
data <- data.frame(y = y, X[,-1])

bvs_3 <- GibbsBvs(
  y ~ .,
  data = data,
  prior.betas = "gZellner",
  prior.models = "ScottBerger",
  n.iter = N_SAMPLES - BURN_IN,
  n.burnin = BURN_IN,
  time.test = TRUE,
  seed = 465
)

# Most probable model
bvs_3$HPMbin 

# Inclusion probabilities
bvs_3$inclprob

# Runtime
bvs_3$time
```

Perfect performance. Runs incredibly quickly, even though it says it shouldn't.

### Model 5

```{r}
#| label: bms 5

y <- model_5$y
X <- model_5$X
data <- data.frame(y = y, X[,-1])

bvs_5 <- GibbsBvs(
  y ~ .,
  data = data,
  prior.betas = "gZellner",
  prior.models = "ScottBerger",
  n.iter = N_SAMPLES - BURN_IN,
  n.burnin = BURN_IN,
  time.test = TRUE,
  seed = 465,
  init.model = "Null"
)

# Most probable model
bvs_5$HPMbin 

# Inclusion probabilities
bvs_5$inclprob

# Runtime
bvs_5$time
```

Did not converge to the correct variables under random initialization (though the only ones it did include are part of the signal). Does not converge under null initialization either, though at least they have the same posterior.

### Model 7

```{r}
#| label: bms 7

y <- model_7$y
X <- model_7$X
data <- data.frame(y = y, X[,-1])

bvs_7 <- GibbsBvs(
  y ~ .,
  data = data,
  prior.betas = "gZellner",
  prior.models = "ScottBerger",
  n.iter = N_SAMPLES - BURN_IN,
  n.burnin = BURN_IN,
  time.test = TRUE,
  seed = 465,
  init.model = "Null"
)

# Most probable model
bvs_7$HPMbin 

# Inclusion probabilities
bvs_7$inclprob

# Runtime
bvs_7$time
```

Did not converge to all of the correct variables under null initialization, but it did converge to two betas that are in the true five (and one that is not), which is reasonable-ish performance. We can do better, though.


## Bayesian Model Averaging

### Model 1

```{r}
#| label: BMA 1

y <- model_1$y
X <- model_1$X
data <- data.frame(y = y, X[,-1])

set.seed(465)
bma_1 <- bms(data,
             burn = BURN_IN,
             iter = N_SAMPLES_2,
             mcmc = "rev.jump")

estimates.bma(bma_1, order.by.pip = FALSE) # Correct
which(estimates.bma(bma_1, order.by.pip = FALSE)[,1] > 0.5)

summary(bma_1)
```

Correct and very fast.


### Model 2

```{r}
#| label: BMA 2

y <- model_2$y
X <- model_2$X
data <- data.frame(y = y, X[,-1])

set.seed(465)
bma_2 <- bms(data,
             burn = BURN_IN,
             iter = N_SAMPLES_2,
             mcmc = "rev.jump")

estimates.bma(bma_2, order.by.pip = FALSE)
which(estimates.bma(bma_2, order.by.pip = FALSE)[,1] > 0.5) # Includes all predictors

summary(bma_2)
```

Once again, does not do well when we run it on a dense signal. Might want to stop doing that.


### Model 3

```{r}
#| label: BMA 3

y <- model_3$y
X <- model_3$X
data <- data.frame(y = y, X[,-1])

set.seed(465)
bma_3 <- bms(data,
             burn = BURN_IN,
             iter = N_SAMPLES_2,
             mcmc = "rev.jump")

estimates.bma(bma_3, order.by.pip = FALSE)
which(estimates.bma(bma_3, order.by.pip = FALSE)[,1] > 0.5) # Includes all predictors

summary(bma_3)
```

Technically, this got all of our predictors, but it also included several false positives, which is not great for us.


### Model 5

```{r}
#| label: BMA 5

y <- model_5$y
X <- model_5$X
data <- data.frame(y = y, X[,-1])

set.seed(465)
bma_5 <- bms(data,
             burn = BURN_IN,
             iter = N_SAMPLES_2,
             mcmc = "rev.jump")

estimates.bma(bma_5, order.by.pip = FALSE)
which(estimates.bma(bma_5, order.by.pip = FALSE)[,1] > 0.5) # Includes all predictors

summary(bma_5)
```

This performs incredibly poorly when $n > p$, even by a little bit. That's very important to note here.


### Model 7

```{r}
#| label: BMA 7

y <- model_7$y
X <- model_7$X
data <- data.frame(y = y, X[,-1])

set.seed(465)
bma_7 <- bms(data,
             burn = BURN_IN,
             iter = N_SAMPLES_2,
             mcmc = "rev.jump")

estimates.bma(bma_7, order.by.pip = FALSE)
which(estimates.bma(bma_7, order.by.pip = FALSE)[,1] > 0.5) # Includes several predictors, but doesn't get a single one right

summary(bma_7)
```

Again, this really doesn't do too great. It wants to flag a lot of regressors, and none of them are actually correct.


# Prior Methods

## SSVS

### Model 1

```{r}
#| label: SSVS 1

y <- model_1$y
X <- model_1$X
data <- data.frame(y = y, X[,-1])

set.seed(465)
start <- Sys.time()
ssvs_1_g <- lm.spike(y ~ .,
                   data = data,
                   niter = N_SAMPLES_2,
                   error.distribution = "gaussian")
print(Sys.time() - start)

ssvs_1_g$beta[,-1] %>% 
  as.data.frame() %>% 
  mutate(iter = row_number()) %>% 
  filter(iter > BURN_IN) %>% 
  pivot_longer(-iter, names_to = "beta", values_to = "value") %>% 
  mutate(beta = substring(beta, 2)) %>% 
  mutate(beta = as.integer(beta) - 1) %>% 
  ggplot(aes(x = iter, y = value)) +
  geom_line() +
  facet_wrap(~beta, scales = "free_y", nrow = 2) +
  theme_bw() +
  labs(x = "Iteration", y = "Beta Value", title = "SSVS Progression") +
  theme(plot.title = element_text(hjust = 0.5))

# Probability of inclusion
colMeans(ssvs_1_g$beta[BURN_IN:N_SAMPLES_2,-1] != 0)

# Coefficient estimates
colMeans(ssvs_1_g$beta[BURN_IN:N_SAMPLES_2,-1])


# Student
set.seed(465)
start <- Sys.time()
ssvs_1_s <- lm.spike(y ~ .,
                   data = data,
                   niter = N_SAMPLES_2,
                   error.distribution = "student")
print(Sys.time() - start)

ssvs_1_s$beta[,-1] %>% 
  as.data.frame() %>% 
  mutate(iter = row_number()) %>% 
  filter(iter > BURN_IN) %>% 
  pivot_longer(-iter, names_to = "beta", values_to = "value") %>% 
  mutate(beta = substring(beta, 2)) %>% 
  mutate(beta = as.integer(beta) - 1) %>% 
  ggplot(aes(x = iter, y = value)) +
  geom_line() +
  facet_wrap(~beta, scales = "free_y", nrow = 2) +
  theme_bw() +
  labs(x = "Iteration", y = "Beta Value", title = "SSVS Progression") +
  theme(plot.title = element_text(hjust = 0.5))

# Probability of inclusion
colMeans(ssvs_1_s$beta[BURN_IN:N_SAMPLES_2,-1] != 0)

# Coefficient estimates
colMeans(ssvs_1_s$beta[BURN_IN:N_SAMPLES_2,-1])
```

The student's distribution definitely does a better job at getting to the true coefficient values, and it keeps out the variables we don't want just as effectively.


### Model 2

```{r}
#| label: SSVS 2

y <- model_2$y
X <- model_2$X
data <- data.frame(y = y, X[,-1])

set.seed(465)
start <- Sys.time()
ssvs_2_g <- lm.spike(y ~ .,
                   data = data,
                   niter = N_SAMPLES_2,
                   error.distribution = "gaussian")
print(Sys.time() - start)

ssvs_2_g$beta[,-1] %>% 
  as.data.frame() %>% 
  mutate(iter = row_number()) %>% 
  filter(iter > BURN_IN) %>% 
  pivot_longer(-iter, names_to = "beta", values_to = "value") %>% 
  mutate(beta = substring(beta, 2)) %>% 
  mutate(beta = as.integer(beta) - 1) %>% 
  ggplot(aes(x = iter, y = value)) +
  geom_line() +
  facet_wrap(~beta, scales = "free_y") +
  theme_bw() +
  labs(x = "Iteration", y = "Beta Value", title = "SSVS Progression") +
  theme(plot.title = element_text(hjust = 0.5))

# Probability of inclusion
colMeans(ssvs_2_g$beta[BURN_IN:N_SAMPLES_2,-1] != 0)
which(colMeans(ssvs_2_g$beta[BURN_IN:N_SAMPLES_2,-1] != 0) > 0.5) # Awful job

# Coefficient estimates
colMeans(ssvs_2_g$beta[BURN_IN:N_SAMPLES_2,-1])


# Student
set.seed(465)
start <- Sys.time()
ssvs_2_s <- lm.spike(y ~ .,
                   data = data,
                   niter = N_SAMPLES_2,
                   error.distribution = "student")
print(Sys.time() - start)

ssvs_2_s$beta[,-1] %>% 
  as.data.frame() %>% 
  mutate(iter = row_number()) %>% 
  filter(iter > BURN_IN) %>% 
  pivot_longer(-iter, names_to = "beta", values_to = "value") %>% 
  mutate(beta = substring(beta, 2)) %>% 
  mutate(beta = as.integer(beta) - 1) %>% 
  ggplot(aes(x = iter, y = value)) +
  geom_line() +
  facet_wrap(~beta, scales = "free_y") +
  theme_bw() +
  labs(x = "Iteration", y = "Beta Value", title = "SSVS Progression") +
  theme(plot.title = element_text(hjust = 0.5))

# Probability of inclusion
colMeans(ssvs_2_s$beta[BURN_IN:N_SAMPLES_2,-1] != 0)
which(colMeans(ssvs_2_s$beta[BURN_IN:N_SAMPLES_2,-1] != 0) > 0.5) # Still job

# Coefficient estimates
colMeans(ssvs_2_s$beta[BURN_IN:N_SAMPLES_2,-1])
```

This does a truly terrible job with the dense signal - it recovers almost nothing as being greater than 0. Could we get it there by tweaking the prior? Probably.


### Model 3

```{r}
#| label: SSVS 3

y <- model_3$y
X <- model_3$X
data <- data.frame(y = y, X[,-1])

set.seed(465)
start <- Sys.time()
ssvs_3_g <- lm.spike(y ~ .,
                   data = data,
                   niter = N_SAMPLES_2,
                   error.distribution = "gaussian")
print(Sys.time() - start)

ssvs_3_g$beta[,-1] %>% 
  as.data.frame() %>% 
  mutate(iter = row_number()) %>% 
  filter(iter > BURN_IN) %>% 
  pivot_longer(-iter, names_to = "beta", values_to = "value") %>% 
  mutate(beta = substring(beta, 2)) %>% 
  mutate(beta = as.integer(beta) - 1) %>% 
  ggplot(aes(x = iter, y = value)) +
  geom_line() +
  facet_wrap(~beta, scales = "free_y") +
  theme_bw() +
  labs(x = "Iteration", y = "Beta Value", title = "SSVS Progression") +
  theme(plot.title = element_text(hjust = 0.5))

# Probability of inclusion
colMeans(ssvs_3_g$beta[BURN_IN:N_SAMPLES_2,-1] != 0)
which(colMeans(ssvs_3_g$beta[BURN_IN:N_SAMPLES_2,-1] != 0) > 0.01) # It doesn't give us many whp, but it definitely IDs the strongest candidates first
# Probably would perform better with a smarter prior specification

# Coefficient estimates
colMeans(ssvs_3_g$beta[BURN_IN:N_SAMPLES_2,-1])


# Student
set.seed(465)
start <- Sys.time()
ssvs_3_s <- lm.spike(y ~ .,
                   data = data,
                   niter = N_SAMPLES_2,
                   error.distribution = "student")
print(Sys.time() - start)

ssvs_3_s$beta[,-1] %>% 
  as.data.frame() %>% 
  mutate(iter = row_number()) %>% 
  filter(iter > BURN_IN) %>% 
  pivot_longer(-iter, names_to = "beta", values_to = "value") %>% 
  mutate(beta = substring(beta, 2)) %>% 
  mutate(beta = as.integer(beta) - 1) %>% 
  ggplot(aes(x = iter, y = value)) +
  geom_line() +
  facet_wrap(~beta, scales = "free_y") +
  theme_bw() +
  labs(x = "Iteration", y = "Beta Value", title = "SSVS Progression") +
  theme(plot.title = element_text(hjust = 0.5))

# Probability of inclusion
colMeans(ssvs_3_s$beta[BURN_IN:N_SAMPLES_2,-1] != 0)
which(colMeans(ssvs_3_s$beta[BURN_IN:N_SAMPLES_2,-1] != 0) > 0.01) # Same performance as gaussian

# Coefficient estimates
colMeans(ssvs_3_s$beta[BURN_IN:N_SAMPLES_2,-1])
```

Not too bad with the sparse model - we would probably want to adjust the prior to have higher inclusion probabilities, because as of right now it's pushing everything pretty close to 0. However, this is otherwise a pretty strong performance, and it does ID the correct variables quickly.


### Model 5

```{r}
#| label: SSVS 5

y <- model_5$y
X <- model_5$X
data <- data.frame(y = y, X[,-1])

set.seed(465)
start <- Sys.time()
ssvs_5_g <- lm.spike(y ~ .,
                   data = data,
                   niter = N_SAMPLES_2,
                   error.distribution = "gaussian")
print(Sys.time() - start)

ssvs_5_g$beta[,-1] %>% 
  as.data.frame() %>% 
  mutate(iter = row_number()) %>% 
  filter(iter > BURN_IN) %>% 
  pivot_longer(-iter, names_to = "beta", values_to = "value") %>% 
  mutate(beta = substring(beta, 2)) %>% 
  mutate(beta = as.integer(beta) - 1) %>% 
  ggplot(aes(x = iter, y = value)) +
  geom_line() +
  facet_wrap(~beta, scales = "free_y") +
  theme_bw() +
  labs(x = "Iteration", y = "Beta Value", title = "SSVS Progression") +
  theme(plot.title = element_text(hjust = 0.5))

# Probability of inclusion
colMeans(ssvs_5_g$beta[BURN_IN:N_SAMPLES_2,-1] != 0)
which(colMeans(ssvs_5_g$beta[BURN_IN:N_SAMPLES_2,-1] != 0) > 0.01) # Does a little worse - introduces false negatives and false positives here.

# Coefficient estimates
colMeans(ssvs_5_g$beta[BURN_IN:N_SAMPLES_2,-1])


# Student
set.seed(465)
start <- Sys.time()
ssvs_5_s <- lm.spike(y ~ .,
                   data = data,
                   niter = N_SAMPLES_2,
                   error.distribution = "student")
print(Sys.time() - start)

ssvs_5_s$beta[,-1] %>% 
  as.data.frame() %>% 
  mutate(iter = row_number()) %>% 
  filter(iter > BURN_IN) %>% 
  pivot_longer(-iter, names_to = "beta", values_to = "value") %>% 
  mutate(beta = substring(beta, 2)) %>% 
  mutate(beta = as.integer(beta) - 1) %>% 
  ggplot(aes(x = iter, y = value)) +
  geom_line() +
  facet_wrap(~beta, scales = "free_y") +
  theme_bw() +
  labs(x = "Iteration", y = "Beta Value", title = "SSVS Progression") +
  theme(plot.title = element_text(hjust = 0.5))

# Probability of inclusion
colMeans(ssvs_5_s$beta[BURN_IN:N_SAMPLES_2,-1] != 0)
which(colMeans(ssvs_5_s$beta[BURN_IN:N_SAMPLES_2,-1] != 0) > 0.01) # Same performance as gaussian

# Coefficient estimates
colMeans(ssvs_5_s$beta[BURN_IN:N_SAMPLES_2,-1])
```

Definitely doing a little worse here with $n > p$. We have the introduction of false negatives as well as false positives.


### Model 7

```{r}
#| label: SSVS 7

y <- model_7$y
X <- model_7$X
data <- data.frame(y = y, X[,-1])

set.seed(465)
start <- Sys.time()
ssvs_7_g <- lm.spike(y ~ .,
                   data = data,
                   niter = N_SAMPLES_2,
                   error.distribution = "gaussian")
print(Sys.time() - start)

# Probability of inclusion
colMeans(ssvs_7_g$beta[BURN_IN:N_SAMPLES_2,-1] != 0)
which(colMeans(ssvs_7_g$beta[BURN_IN:N_SAMPLES_2,-1] != 0) > 0.001) # Mixed bag - about 50/50 on true vs false signal. We do ID 4 of our 5 correct signals, which is nice.

# Coefficient estimates
colMeans(ssvs_7_g$beta[BURN_IN:N_SAMPLES_2,-1])
```

Does a pretty okay job, although I feel like better performance is certainly possible. Had to remove the call for Student's because it was taking too long (had not completed 3k iterations in 3 minutes before giving up).


