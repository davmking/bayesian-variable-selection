---
title: "experiment"
format: html
---

# Libraries and Data

## Libraries

```{r}
#| label: libraries

library(coda)
library(MCMCpack)
library(BayesVarSel)
library(BMS)
library(BoomSpikeSlab)
library(spikeSlabGAM)
library(bayeslm)
library(BayesSubsets)
library(monomvn)
#library(EMVS)  # Man
library(tidyverse)
```


## Data

Here is a table that describes each model and its characteristics:

| Name | n | p | p_sig |  n vs. p | Description |
|------|---|---|-------|----------|-------------|
| Model 0 | 100 | 10 | 3 | n >> p | Few predictors, sparse |
| Model 1 | 100 | 10 | 8 | n >> p | Few predictors, dense |
| Model 2 | 100 | 95 | 75 | n > p | n slightly > p, dense |
| Model 3 | 100 | 95 | 8 | n > p | n slightly > p, sparse |
| Model 4 | 100 | 105 | 8 | n < p | n slightly < p, sparse |
| Model 5 | 100 | 1000 | 8 | n << p | Many predictors, sparse |

```{r}
#| label: generate data

set.seed(465)

model_0 <- simulate_lm(100, 10, p_sig = 3)
model_1 <- simulate_lm(100, 10, p_sig = 8)
model_2 <- simulate_lm(100, 95, p_sig = 75)
model_3 <- simulate_lm(100, 95, p_sig = 8)
model_4 <- simulate_lm(100, 105, p_sig = 8)
model_5 <- simulate_lm(100, 1000, p_sig = 8)


model_0$beta_true[2:4] <- model_0$beta_true[2:4] * c(3, 2, 2)
model_0$X[,2:4] <- model_0$X[,2:4] * c(3, 2, 2)

X <- model_0$X
y <- model_0$y
```

## Hyperparameters

```{r}
#| label: defining hyperparameters

N_SAMPLES <- 11000
BURN_IN <- 1000

N_SAMPLES_2 <- 31000
```



# Non-Prior Methods

## Subset Selection

Note: For now, I am skipping the models with dense signals. I don't think it makes much sense to run them, and I think the computation time is too difficult.

Note: This code needs to be run overnight. It is not currently running fast enough to be useful.

### Model 1

```{r}
#| label: run bayeslm and evaluate

set.seed(465)

X <- model_1$X
y <- model_1$y

model_1_fit <- bayeslm(y ~ X[,-1],
                       N = N_SAMPLES,
                       burnin = BURN_IN,
                       prior = "ridge"
                       )

temp <- post_predict(post_y_hat = tcrossprod(model_1_fit$beta, X),
                    post_sigma = model_1_fit$sigma,
                    yy = y)
post_y_pred = temp$post_y_pred
post_lpd = temp$post_lpd

indicators_1 <- branch_and_bound(yy = fitted(model_1_fit),
                              XX = X)

accept_info <- accept_family(post_y_pred = post_y_pred,
                            post_lpd = post_lpd,
                            XX = X,
                            indicators = indicators_1,
                            yy = y,
                            post_y_hat = tcrossprod(model_1_fit$beta, X))



# How many subsets are in the acceptable family?
length(accept_info$all_accept) # 127

# Simplest acceptable subset:
beta_hat_small <- accept_info$beta_hat_small
beta_hat_small

# Which coefficients are nonzero:
S_small <- which(beta_hat_small != 0)
S_small

# How many coefficients are nonzero:
length(S_small) # 1

# Acceptable subset that minimizes CV error:
beta_hat_min <- accept_info$beta_hat_min

# Typically much larger (and often too large...)
sum(beta_hat_min != 0) # 6

# Variable importance metrics
vi_e <- var_imp(indicators = indicators_1,
               all_accept = accept_info$all_accept)$vi_inc

# Variables appearing in all acceptable subsets
all_acceptable <- which(vi_e == 1)
all_acceptable

# Size
length(all_acceptable) # 1
```

With $n = 100$, we aren't picking up the right variables - the smallest subsets are just the intercept. Might need to increase the SNR or n?


### Model 3

```{r}
#| label: run bayeslm and evaluate 3

set.seed(465)

X <- model_3$X
y <- model_3$y

model_3_fit <- bayeslm(y ~ X[,-1],
                       N = N_SAMPLES,
                       burnin = BURN_IN,
                       prior = "ridge"
                       )

temp <- post_predict(post_y_hat = tcrossprod(model_3_fit$beta, X),
                    post_sigma = model_3_fit$sigma,
                    yy = y)
post_y_pred = temp$post_y_pred
post_lpd = temp$post_lpd

start <- Sys.time()
indicators_3 <- branch_and_bound(yy = fitted(model_3_fit),
                              XX = X)
print(Sys.time() - start)

accept_info <- accept_family(post_y_pred = post_y_pred,
                            post_lpd = post_lpd,
                            XX = X,
                            indicators = indicators_1,
                            yy = y,
                            post_y_hat = tcrossprod(model_1_fit$beta, X))



# How many subsets are in the acceptable family?
length(accept_info$all_accept) # 76

# These are the rows of `indicators` that belong to the acceptable family:
head(accept_info$all_accept)

# An example acceptable subset:
ex_accept <- accept_info$all_accept[1]
which(indicators[ex_accept,])

# Simplest acceptable subset:
beta_hat_small <- accept_info$beta_hat_small

# Which coefficients are nonzero:
S_small <- which(beta_hat_small != 0)

# How many coefficients are nonzero:
length(S_small) # 4

# Acceptable subset that minimizes CV error:
beta_hat_min <- accept_info$beta_hat_min

# Typically much larger (and often too large...)
sum(beta_hat_min != 0) # 5

# Variable importance metrics
vi_e <- var_imp(indicators = indicators_1,
               all_accept = accept_info$all_accept)$vi_inc

# Variables appearing in all acceptable subsets
all_acceptable <- which(vi_e == 1) # Correct variables

# Size
length(all_acceptable) # 4
```


## Bayesian Model Selection

### Model 1

```{r}
#| label: bms 1

y <- model_1$y
X <- model_1$X
data <- data.frame(y = y, X[,-1])

bvs_1 <- GibbsBvs(
  y ~ .,
  data = data,
  prior.betas = "gZellner",
  prior.models = "ScottBerger",
  n.iter = N_SAMPLES - BURN_IN,
  n.burnin = BURN_IN,
  time.test = TRUE,
  seed = 465
)

# Most probable model
bvs_1$HPMbin 

# Inclusion probabilities
bvs_1$inclprob

# Runtime
bvs_1$time
```

Recovers the correct variables. Not super slow in terms of efficiency.

### Model 2

```{r}
#| label: bms 2

y <- model_2$y
X <- model_2$X
data <- data.frame(y = y, X[,-1])

bvs_2 <- GibbsBvs(
  y ~ .,
  data = data,
  prior.betas = "gZellner",
  prior.models = "ScottBerger",
  n.iter = N_SAMPLES - BURN_IN,
  n.burnin = BURN_IN,
  time.test = TRUE,
  seed = 465
)

# Most probable model
bvs_2$HPMbin 

# Inclusion probabilities
bvs_2$inclprob

# Runtime
bvs_2$time
```

Much worse runtime than Model 3 has. I think this algorithm should not be used on dense signals. It also does not correctly converge to the right subset - it kinda just says everything should be in the model.


### Model 3

```{r}
#| label: bms 3

y <- model_3$y
X <- model_3$X
data <- data.frame(y = y, X[,-1])

bvs_3 <- GibbsBvs(
  y ~ .,
  data = data,
  prior.betas = "gZellner",
  prior.models = "ScottBerger",
  n.iter = N_SAMPLES - BURN_IN,
  n.burnin = BURN_IN,
  time.test = TRUE,
  seed = 465
)

# Most probable model
bvs_3$HPMbin 

# Inclusion probabilities
bvs_3$inclprob

# Runtime
bvs_3$time
```

Perfect performance. Runs incredibly quickly, even though it says it shouldn't.

### Model 5

```{r}
#| label: bms 5

y <- model_5$y
X <- model_5$X
data <- data.frame(y = y, X[,-1])

bvs_5 <- GibbsBvs(
  y ~ .,
  data = data,
  prior.betas = "gZellner",
  prior.models = "ScottBerger",
  n.iter = N_SAMPLES - BURN_IN,
  n.burnin = BURN_IN,
  time.test = TRUE,
  seed = 465,
  init.model = "Null"
)

# Most probable model
bvs_5$HPMbin 

# Inclusion probabilities
bvs_5$inclprob

# Runtime
bvs_5$time
```

Did not converge to the correct variables under random initialization (though the only ones it did include are part of the signal). Does not converge under null initialization either, though at least they have the same posterior.

### Model 7

```{r}
#| label: bms 7

y <- model_7$y
X <- model_7$X
data <- data.frame(y = y, X[,-1])

bvs_7 <- GibbsBvs(
  y ~ .,
  data = data,
  prior.betas = "gZellner",
  prior.models = "ScottBerger",
  n.iter = N_SAMPLES - BURN_IN,
  n.burnin = BURN_IN,
  time.test = TRUE,
  seed = 465,
  init.model = "Null"
)

# Most probable model
bvs_7$HPMbin 

# Inclusion probabilities
bvs_7$inclprob

# Runtime
bvs_7$time
```

Did not converge to all of the correct variables under null initialization, but it did converge to two betas that are in the true five (and one that is not), which is reasonable-ish performance. We can do better, though.


## Bayesian Model Averaging

### Model 1

```{r}
#| label: BMA 1

y <- model_1$y
X <- model_1$X
data <- data.frame(y = y, X[,-1])

set.seed(465)
bma_1 <- bms(data,
             burn = BURN_IN,
             iter = N_SAMPLES_2,
             mcmc = "rev.jump")

estimates.bma(bma_1, order.by.pip = FALSE) # Correct
which(estimates.bma(bma_1, order.by.pip = FALSE)[,1] > 0.5)

summary(bma_1)
```

Correct and very fast.


### Model 2

```{r}
#| label: BMA 2

y <- model_2$y
X <- model_2$X
data <- data.frame(y = y, X[,-1])

set.seed(465)
bma_2 <- bms(data,
             burn = BURN_IN,
             iter = N_SAMPLES_2,
             mcmc = "rev.jump")

estimates.bma(bma_2, order.by.pip = FALSE)
which(estimates.bma(bma_2, order.by.pip = FALSE)[,1] > 0.5) # Includes all predictors

summary(bma_2)
```

Once again, does not do well when we run it on a dense signal. Might want to stop doing that.


### Model 3

```{r}
#| label: BMA 3

y <- model_3$y
X <- model_3$X
data <- data.frame(y = y, X[,-1])

set.seed(465)
bma_3 <- bms(data,
             burn = BURN_IN,
             iter = N_SAMPLES_2,
             mcmc = "rev.jump")

estimates.bma(bma_3, order.by.pip = FALSE)
which(estimates.bma(bma_3, order.by.pip = FALSE)[,1] > 0.5) # Includes all predictors

summary(bma_3)
```

Technically, this got all of our predictors, but it also included several false positives, which is not great for us.


### Model 5

```{r}
#| label: BMA 5

y <- model_5$y
X <- model_5$X
data <- data.frame(y = y, X[,-1])

set.seed(465)
bma_5 <- bms(data,
             burn = BURN_IN,
             iter = N_SAMPLES_2,
             mcmc = "rev.jump")

estimates.bma(bma_5, order.by.pip = FALSE)
which(estimates.bma(bma_5, order.by.pip = FALSE)[,1] > 0.5) # Includes all predictors

summary(bma_5)
```

This performs incredibly poorly when $n > p$, even by a little bit. That's very important to note here.


### Model 7

```{r}
#| label: BMA 7

y <- model_7$y
X <- model_7$X
data <- data.frame(y = y, X[,-1])

set.seed(465)
bma_7 <- bms(data,
             burn = BURN_IN,
             iter = N_SAMPLES_2,
             mcmc = "rev.jump")

estimates.bma(bma_7, order.by.pip = FALSE)
which(estimates.bma(bma_7, order.by.pip = FALSE)[,1] > 0.5) # Includes several predictors, but doesn't get a single one right

summary(bma_7)
```

Again, this really doesn't do too great. It wants to flag a lot of regressors, and none of them are actually correct.


# Prior Methods

## SSVS

### Model 1

```{r}
#| label: SSVS 1

y <- model_1$y
X <- model_1$X
data <- data.frame(y = y, X[,-1])

set.seed(465)
start <- Sys.time()
ssvs_1_g <- lm.spike(y ~ .,
                   data = data,
                   niter = N_SAMPLES_2,
                   error.distribution = "gaussian")
print(Sys.time() - start)

ssvs_1_g$beta[,-1] %>% 
  as.data.frame() %>% 
  mutate(iter = row_number()) %>% 
  filter(iter > BURN_IN) %>% 
  pivot_longer(-iter, names_to = "beta", values_to = "value") %>% 
  mutate(beta = substring(beta, 2)) %>% 
  mutate(beta = as.integer(beta) - 1) %>% 
  ggplot(aes(x = iter, y = value)) +
  geom_line() +
  facet_wrap(~beta, scales = "free_y", nrow = 2) +
  theme_bw() +
  labs(x = "Iteration", y = "Beta Value", title = "SSVS Progression") +
  theme(plot.title = element_text(hjust = 0.5))

# Probability of inclusion
colMeans(ssvs_1_g$beta[BURN_IN:N_SAMPLES_2,-1] != 0)

# Coefficient estimates
colMeans(ssvs_1_g$beta[BURN_IN:N_SAMPLES_2,-1])


# Student
set.seed(465)
start <- Sys.time()
ssvs_1_s <- lm.spike(y ~ .,
                   data = data,
                   niter = N_SAMPLES_2,
                   error.distribution = "student")
print(Sys.time() - start)

ssvs_1_s$beta[,-1] %>% 
  as.data.frame() %>% 
  mutate(iter = row_number()) %>% 
  filter(iter > BURN_IN) %>% 
  pivot_longer(-iter, names_to = "beta", values_to = "value") %>% 
  mutate(beta = substring(beta, 2)) %>% 
  mutate(beta = as.integer(beta) - 1) %>% 
  ggplot(aes(x = iter, y = value)) +
  geom_line() +
  facet_wrap(~beta, scales = "free_y", nrow = 2) +
  theme_bw() +
  labs(x = "Iteration", y = "Beta Value", title = "SSVS Progression") +
  theme(plot.title = element_text(hjust = 0.5))

# Probability of inclusion
colMeans(ssvs_1_s$beta[BURN_IN:N_SAMPLES_2,-1] != 0)

# Coefficient estimates
colMeans(ssvs_1_s$beta[BURN_IN:N_SAMPLES_2,-1])
```

The student's distribution definitely does a better job at getting to the true coefficient values, and it keeps out the variables we don't want just as effectively.


### Model 2

```{r}
#| label: SSVS 2

y <- model_2$y
X <- model_2$X
data <- data.frame(y = y, X[,-1])

set.seed(465)
start <- Sys.time()
ssvs_2_g <- lm.spike(y ~ .,
                   data = data,
                   niter = N_SAMPLES_2,
                   error.distribution = "gaussian")
print(Sys.time() - start)

ssvs_2_g$beta[,-1] %>% 
  as.data.frame() %>% 
  mutate(iter = row_number()) %>% 
  filter(iter > BURN_IN) %>% 
  pivot_longer(-iter, names_to = "beta", values_to = "value") %>% 
  mutate(beta = substring(beta, 2)) %>% 
  mutate(beta = as.integer(beta) - 1) %>% 
  ggplot(aes(x = iter, y = value)) +
  geom_line() +
  facet_wrap(~beta, scales = "free_y") +
  theme_bw() +
  labs(x = "Iteration", y = "Beta Value", title = "SSVS Progression") +
  theme(plot.title = element_text(hjust = 0.5))

# Probability of inclusion
colMeans(ssvs_2_g$beta[BURN_IN:N_SAMPLES_2,-1] != 0)
which(colMeans(ssvs_2_g$beta[BURN_IN:N_SAMPLES_2,-1] != 0) > 0.5) # Awful job

# Coefficient estimates
colMeans(ssvs_2_g$beta[BURN_IN:N_SAMPLES_2,-1])


# Student
set.seed(465)
start <- Sys.time()
ssvs_2_s <- lm.spike(y ~ .,
                   data = data,
                   niter = N_SAMPLES_2,
                   error.distribution = "student")
print(Sys.time() - start)

ssvs_2_s$beta[,-1] %>% 
  as.data.frame() %>% 
  mutate(iter = row_number()) %>% 
  filter(iter > BURN_IN) %>% 
  pivot_longer(-iter, names_to = "beta", values_to = "value") %>% 
  mutate(beta = substring(beta, 2)) %>% 
  mutate(beta = as.integer(beta) - 1) %>% 
  ggplot(aes(x = iter, y = value)) +
  geom_line() +
  facet_wrap(~beta, scales = "free_y") +
  theme_bw() +
  labs(x = "Iteration", y = "Beta Value", title = "SSVS Progression") +
  theme(plot.title = element_text(hjust = 0.5))

# Probability of inclusion
colMeans(ssvs_2_s$beta[BURN_IN:N_SAMPLES_2,-1] != 0)
which(colMeans(ssvs_2_s$beta[BURN_IN:N_SAMPLES_2,-1] != 0) > 0.5) # Still job

# Coefficient estimates
colMeans(ssvs_2_s$beta[BURN_IN:N_SAMPLES_2,-1])
```

This does a truly terrible job with the dense signal - it recovers almost nothing as being greater than 0. Could we get it there by tweaking the prior? Probably.


### Model 3

```{r}
#| label: SSVS 3

y <- model_3$y
X <- model_3$X
data <- data.frame(y = y, X[,-1])

set.seed(465)
start <- Sys.time()
ssvs_3_g <- lm.spike(y ~ .,
                   data = data,
                   niter = N_SAMPLES_2,
                   error.distribution = "gaussian")
print(Sys.time() - start)

ssvs_3_g$beta[,-1] %>% 
  as.data.frame() %>% 
  mutate(iter = row_number()) %>% 
  filter(iter > BURN_IN) %>% 
  pivot_longer(-iter, names_to = "beta", values_to = "value") %>% 
  mutate(beta = substring(beta, 2)) %>% 
  mutate(beta = as.integer(beta) - 1) %>% 
  ggplot(aes(x = iter, y = value)) +
  geom_line() +
  facet_wrap(~beta, scales = "free_y") +
  theme_bw() +
  labs(x = "Iteration", y = "Beta Value", title = "SSVS Progression") +
  theme(plot.title = element_text(hjust = 0.5))

# Probability of inclusion
colMeans(ssvs_3_g$beta[BURN_IN:N_SAMPLES_2,-1] != 0)
which(colMeans(ssvs_3_g$beta[BURN_IN:N_SAMPLES_2,-1] != 0) > 0.01) # It doesn't give us many whp, but it definitely IDs the strongest candidates first
# Probably would perform better with a smarter prior specification

# Coefficient estimates
colMeans(ssvs_3_g$beta[BURN_IN:N_SAMPLES_2,-1])


# Student
set.seed(465)
start <- Sys.time()
ssvs_3_s <- lm.spike(y ~ .,
                   data = data,
                   niter = N_SAMPLES_2,
                   error.distribution = "student")
print(Sys.time() - start)

ssvs_3_s$beta[,-1] %>% 
  as.data.frame() %>% 
  mutate(iter = row_number()) %>% 
  filter(iter > BURN_IN) %>% 
  pivot_longer(-iter, names_to = "beta", values_to = "value") %>% 
  mutate(beta = substring(beta, 2)) %>% 
  mutate(beta = as.integer(beta) - 1) %>% 
  ggplot(aes(x = iter, y = value)) +
  geom_line() +
  facet_wrap(~beta, scales = "free_y") +
  theme_bw() +
  labs(x = "Iteration", y = "Beta Value", title = "SSVS Progression") +
  theme(plot.title = element_text(hjust = 0.5))

# Probability of inclusion
colMeans(ssvs_3_s$beta[BURN_IN:N_SAMPLES_2,-1] != 0)
which(colMeans(ssvs_3_s$beta[BURN_IN:N_SAMPLES_2,-1] != 0) > 0.01) # Same performance as gaussian

# Coefficient estimates
colMeans(ssvs_3_s$beta[BURN_IN:N_SAMPLES_2,-1])
```

Not too bad with the sparse model - we would probably want to adjust the prior to have higher inclusion probabilities, because as of right now it's pushing everything pretty close to 0. However, this is otherwise a pretty strong performance, and it does ID the correct variables quickly.


### Model 5

```{r}
#| label: SSVS 5

y <- model_5$y
X <- model_5$X
data <- data.frame(y = y, X[,-1])

set.seed(465)
start <- Sys.time()
ssvs_5_g <- lm.spike(y ~ .,
                   data = data,
                   niter = N_SAMPLES_2,
                   error.distribution = "gaussian")
print(Sys.time() - start)

ssvs_5_g$beta[,-1] %>% 
  as.data.frame() %>% 
  mutate(iter = row_number()) %>% 
  filter(iter > BURN_IN) %>% 
  pivot_longer(-iter, names_to = "beta", values_to = "value") %>% 
  mutate(beta = substring(beta, 2)) %>% 
  mutate(beta = as.integer(beta) - 1) %>% 
  ggplot(aes(x = iter, y = value)) +
  geom_line() +
  facet_wrap(~beta, scales = "free_y") +
  theme_bw() +
  labs(x = "Iteration", y = "Beta Value", title = "SSVS Progression") +
  theme(plot.title = element_text(hjust = 0.5))

# Probability of inclusion
colMeans(ssvs_5_g$beta[BURN_IN:N_SAMPLES_2,-1] != 0)
which(colMeans(ssvs_5_g$beta[BURN_IN:N_SAMPLES_2,-1] != 0) > 0.01) # Does a little worse - introduces false negatives and false positives here.

# Coefficient estimates
colMeans(ssvs_5_g$beta[BURN_IN:N_SAMPLES_2,-1])


# Student
set.seed(465)
start <- Sys.time()
ssvs_5_s <- lm.spike(y ~ .,
                   data = data,
                   niter = N_SAMPLES_2,
                   error.distribution = "student")
print(Sys.time() - start)

ssvs_5_s$beta[,-1] %>% 
  as.data.frame() %>% 
  mutate(iter = row_number()) %>% 
  filter(iter > BURN_IN) %>% 
  pivot_longer(-iter, names_to = "beta", values_to = "value") %>% 
  mutate(beta = substring(beta, 2)) %>% 
  mutate(beta = as.integer(beta) - 1) %>% 
  ggplot(aes(x = iter, y = value)) +
  geom_line() +
  facet_wrap(~beta, scales = "free_y") +
  theme_bw() +
  labs(x = "Iteration", y = "Beta Value", title = "SSVS Progression") +
  theme(plot.title = element_text(hjust = 0.5))

# Probability of inclusion
colMeans(ssvs_5_s$beta[BURN_IN:N_SAMPLES_2,-1] != 0)
which(colMeans(ssvs_5_s$beta[BURN_IN:N_SAMPLES_2,-1] != 0) > 0.01) # Same performance as gaussian

# Coefficient estimates
colMeans(ssvs_5_s$beta[BURN_IN:N_SAMPLES_2,-1])
```

Definitely doing a little worse here with $n > p$. We have the introduction of false negatives as well as false positives.


### Model 7

```{r}
#| label: SSVS 7

y <- model_7$y
X <- model_7$X
data <- data.frame(y = y, X[,-1])

set.seed(465)
start <- Sys.time()
ssvs_7_g <- lm.spike(y ~ .,
                   data = data,
                   niter = N_SAMPLES_2,
                   error.distribution = "gaussian")
print(Sys.time() - start)

# Probability of inclusion
colMeans(ssvs_7_g$beta[BURN_IN:N_SAMPLES_2,-1] != 0)
which(colMeans(ssvs_7_g$beta[BURN_IN:N_SAMPLES_2,-1] != 0) > 0.001) # Mixed bag - about 50/50 on true vs false signal. We do ID 4 of our 5 correct signals, which is nice.

# Coefficient estimates
colMeans(ssvs_7_g$beta[BURN_IN:N_SAMPLES_2,-1])
```

Does a pretty okay job, although I feel like better performance is certainly possible. Had to remove the call for Student's because it was taking too long (had not completed 3k iterations in 3 minutes before giving up).


## LASSO

### Model 1

```{r}
#| label: lasso 1

y <- model_1$y
X <- model_1$X[,-1]

set.seed(465)
lasso_1 <- blasso(X, y, T = N_SAMPLES_2, case = "default")

lasso_1$beta[,-1] %>% 
  as.data.frame() %>% 
  mutate(iter = row_number()) %>% 
  filter(iter > BURN_IN) %>% 
  pivot_longer(-iter, names_to = "beta", values_to = "value") %>% 
  mutate(beta = substring(beta, 3)) %>% 
  mutate(beta = as.integer(beta) - 1) %>% 
  ggplot(aes(x = iter, y = value)) +
  geom_line() +
  facet_wrap(~beta, scales = "free_y") +
  theme_bw() +
  labs(x = "Iteration", y = "Beta Value", title = "LASSO Progression") +
  theme(plot.title = element_text(hjust = 0.5))

# Probabilities of inclusion
colMeans(lasso_1$beta[,-1] != 0)
which(colMeans(lasso_1$beta[,-1] != 0) > 0.5)

# Est. coefficients
colMeans(lasso_1$beta[,-1])

# Lambda mostly stays low
plot(seq(N_SAMPLES - BURN_IN), lasso_1$lambda2[(BURN_IN + 1):N_SAMPLES])
```

This almost recovers the correct values - however, it also has a false positive included.


### Model 2

```{r}
#| label: lasso 2

y <- model_2$y
X <- model_2$X[,-1]

set.seed(465)
start <- Sys.time()
print(start)
lasso_2 <- blasso(X, y, T = N_SAMPLES, case = "default")
print(Sys.time() - start)

lasso_2$beta[,-1] %>% 
  as.data.frame() %>% 
  mutate(iter = row_number()) %>% 
  filter(iter > BURN_IN) %>% 
  pivot_longer(-iter, names_to = "beta", values_to = "value") %>% 
  mutate(beta = substring(beta, 3)) %>% 
  mutate(beta = as.integer(beta) - 1) %>% 
  ggplot(aes(x = iter, y = value)) +
  geom_line() +
  facet_wrap(~beta, scales = "free_y") +
  theme_bw() +
  labs(x = "Iteration", y = "Beta Value", title = "LASSO Progression") +
  theme(plot.title = element_text(hjust = 0.5))

# Probabilities of inclusion
colMeans(lasso_2$beta[,-1] != 0)
which(colMeans(lasso_2$beta[,-1] != 0) > 0.5) # So we're only capturing some of them

# Est. coefficients
colMeans(lasso_2$beta[,-1]) # Most of these are incredibly close to 0


# Lambda mostly stays low
plot(seq(N_SAMPLES - BURN_IN), lasso_2$lambda2[(BURN_IN + 1):N_SAMPLES])

# Model size stays pretty consistent in that 45-50 range
plot(seq(N_SAMPLES - BURN_IN), lasso_2$m[(BURN_IN + 1):N_SAMPLES])
```

This definitely doesn't do a great job - it gives us a lot of false negatives, and only captures about half of the true negatives. It also takes far, far longer to converge than some of the other algorithms.


### Model 3

```{r}
#| label: lasso 3

y <- model_3$y
X <- model_3$X[,-1]

set.seed(465)
start <- Sys.time()
print(start)
lasso_3 <- blasso(X, y, T = N_SAMPLES, case = "default")
print(Sys.time() - start)

lasso_3$beta[,-1] %>% 
  as.data.frame() %>% 
  mutate(iter = row_number()) %>% 
  filter(iter > BURN_IN) %>% 
  pivot_longer(-iter, names_to = "beta", values_to = "value") %>% 
  mutate(beta = substring(beta, 3)) %>% 
  mutate(beta = as.integer(beta) - 1) %>% 
  ggplot(aes(x = iter, y = value)) +
  geom_line() +
  facet_wrap(~beta, scales = "free_y") +
  theme_bw() +
  labs(x = "Iteration", y = "Beta Value", title = "LASSO Progression") +
  theme(plot.title = element_text(hjust = 0.5))

# Probabilities of inclusion
colMeans(lasso_3$beta[,-1] != 0)
which(colMeans(lasso_3$beta[,-1] != 0) > 0.75) # Not terrible performance, does a perfect job if you threshold it high

# Est. coefficients
colMeans(lasso_3$beta[,-1]) # Most of these are incredibly close to 0


# Lambda mostly stays low
plot(seq(N_SAMPLES - BURN_IN), lasso_3$lambda2[(BURN_IN + 1):N_SAMPLES])

# Model size stays pretty consistent in that 45-50 range
plot(seq(N_SAMPLES - BURN_IN), lasso_3$m[(BURN_IN + 1):N_SAMPLES])
```

LASSO performs well here - you have to threshold the probability of inclusion higher, but once you do that, the algorithm has perfect performance.


### Model 5

```{r}
#| label: lasso 5

y <- model_5$y
X <- model_5$X[,-1]

set.seed(465)
start <- Sys.time()
print(start)
lasso_5 <- blasso(X, y, T = N_SAMPLES, case = "default")
print(Sys.time() - start)

lasso_5$beta[,-1] %>% 
  as.data.frame() %>% 
  mutate(iter = row_number()) %>% 
  filter(iter > BURN_IN) %>% 
  pivot_longer(-iter, names_to = "beta", values_to = "value") %>% 
  mutate(beta = substring(beta, 3)) %>% 
  mutate(beta = as.integer(beta) - 1) %>% 
  ggplot(aes(x = iter, y = value)) +
  geom_line() +
  facet_wrap(~beta, scales = "free_y") +
  theme_bw() +
  labs(x = "Iteration", y = "Beta Value", title = "LASSO Progression") +
  theme(plot.title = element_text(hjust = 0.5))

# Probabilities of inclusion
colMeans(lasso_5$beta[,-1] != 0)
which(colMeans(lasso_5$beta[,-1] != 0) > 0.7) # Thresholding it high gets it pretty close, but not perfect

# Est. coefficients
colMeans(lasso_5$beta[,-1]) # Most of these are incredibly close to 0


# Lambda mostly stays low
plot(seq(N_SAMPLES - BURN_IN), lasso_5$lambda2[(BURN_IN + 1):N_SAMPLES])

# Model size stays pretty consistent in that 50-55 range
plot(seq(N_SAMPLES - BURN_IN), lasso_5$m[(BURN_IN + 1):N_SAMPLES])
```

Slightly worse performance, but it's still pretty good with high thresholding.


### Model 7

```{r}
#| label: lasso 7

y <- model_7$y
X <- model_7$X[,-1]

set.seed(465)
start <- Sys.time()
print(start)
lasso_7 <- blasso(X, y, T = N_SAMPLES, case = "default")
print(Sys.time() - start)

lasso_7$beta[,-1] %>% 
  as.data.frame() %>% 
  mutate(iter = row_number()) %>% 
  filter(iter > BURN_IN) %>% 
  pivot_longer(-iter, names_to = "beta", values_to = "value") %>% 
  mutate(beta = substring(beta, 3)) %>% 
  mutate(beta = as.integer(beta) - 1) %>% 
  ggplot(aes(x = iter, y = value)) +
  geom_line() +
  facet_wrap(~beta, scales = "free_y") +
  theme_bw() +
  labs(x = "Iteration", y = "Beta Value", title = "LASSO Progression") +
  theme(plot.title = element_text(hjust = 0.5))

# Probabilities of inclusion
colMeans(lasso_7$beta[,-1] != 0)
which(colMeans(lasso_7$beta[,-1] != 0) > 0.7) # Thresholding it high gets it pretty close, but not perfect

# Est. coefficients
colMeans(lasso_7$beta[,-1]) # Most of these are incredibly close to 0


# Lambda mostly stays low
plot(seq(N_SAMPLES - BURN_IN), lasso_7$lambda2[(BURN_IN + 1):N_SAMPLES])

# Model size stays pretty consistent in that 50-55 range
plot(seq(N_SAMPLES - BURN_IN), lasso_7$m[(BURN_IN + 1):N_SAMPLES])
```


## NG

### Model 1

```{r}
#| label: ng 1

y <- model_1$y
X <- model_1$X[,-1]

set.seed(465)
ng_1 <- blasso(X, y, T = N_SAMPLES, case = "ng")

ng_1$beta[,-1] %>% 
  as.data.frame() %>% 
  mutate(iter = row_number()) %>% 
  filter(iter > BURN_IN) %>% 
  pivot_longer(-iter, names_to = "beta", values_to = "value") %>% 
  mutate(beta = substring(beta, 3)) %>% 
  mutate(beta = as.integer(beta) - 1) %>% 
  ggplot(aes(x = iter, y = value)) +
  geom_line() +
  facet_wrap(~beta, scales = "free_y") +
  theme_bw() +
  labs(x = "Iteration", y = "Beta Value", title = "LASSO Progression") +
  theme(plot.title = element_text(hjust = 0.5))

# Probabilities of inclusion
colMeans(ng_1$beta[,-1] != 0)
which(colMeans(ng_1$beta[,-1] != 0) > 0.55) # Needs a little thresholding

# Est. coefficients
colMeans(ng_1$beta[,-1])

# Lambda mostly stays low
plot(seq(N_SAMPLES - BURN_IN), ng_1$lambda2[(BURN_IN + 1):N_SAMPLES])
```

This almost recovers the correct values - however, it also has a false positive included.


### Model 2

```{r}
#| label: ng 2

y <- model_2$y
X <- model_2$X[,-1]

set.seed(465)
start <- Sys.time()
print(start)
ng_2 <- blasso(X, y, T = N_SAMPLES, case = "ng")
print(Sys.time() - start)

ng_2$beta[,-1] %>% 
  as.data.frame() %>% 
  mutate(iter = row_number()) %>% 
  filter(iter > BURN_IN) %>% 
  pivot_longer(-iter, names_to = "beta", values_to = "value") %>% 
  mutate(beta = substring(beta, 3)) %>% 
  mutate(beta = as.integer(beta) - 1) %>% 
  ggplot(aes(x = iter, y = value)) +
  geom_line() +
  facet_wrap(~beta, scales = "free_y") +
  theme_bw() +
  labs(x = "Iteration", y = "Beta Value", title = "LASSO Progression") +
  theme(plot.title = element_text(hjust = 0.5))

# Probabilities of inclusion
colMeans(ng_2$beta[,-1] != 0)
which(colMeans(ng_2$beta[,-1] != 0) > 0.5) # So we're only capturing some of them

# Est. coefficients
colMeans(ng_2$beta[,-1])


# Lambda mostly stays low
plot(seq(N_SAMPLES - BURN_IN), ng_2$lambda2[(BURN_IN + 1):N_SAMPLES])

# Model size stays pretty consistent in that 45-50 range
plot(seq(N_SAMPLES - BURN_IN), ng_2$m[(BURN_IN + 1):N_SAMPLES])
```

This definitely doesn't do a great job - it gives us a lot of false negatives, and only captures about half of the true negatives. It also takes far, far longer to converge than some of the other algorithms.


### Model 3

```{r}
#| label: ng 3

y <- model_3$y
X <- model_3$X[,-1]

set.seed(465)
start <- Sys.time()
print(start)
ng_3 <- blasso(X, y, T = N_SAMPLES, case = "ng")
print(Sys.time() - start)

ng_3$beta[,-1] %>% 
  as.data.frame() %>% 
  mutate(iter = row_number()) %>% 
  filter(iter > BURN_IN) %>% 
  pivot_longer(-iter, names_to = "beta", values_to = "value") %>% 
  mutate(beta = substring(beta, 3)) %>% 
  mutate(beta = as.integer(beta) - 1) %>% 
  ggplot(aes(x = iter, y = value)) +
  geom_line() +
  facet_wrap(~beta, scales = "free_y") +
  theme_bw() +
  labs(x = "Iteration", y = "Beta Value", title = "LASSO Progression") +
  theme(plot.title = element_text(hjust = 0.5))

# Probabilities of inclusion
colMeans(ng_3$beta[,-1] != 0)
which(colMeans(ng_3$beta[,-1] != 0) > 0.75) # Not terrible performance, does a perfect job if you threshold it high
# We could threshold this as high as 94% (in this instance) and still get the correct output
# Marginally better performance than LASSO, but might not hold up across multiple runs

# Est. coefficients
colMeans(ng_3$beta[,-1]) # Most of these are incredibly close to 0
# Almost true coefficients for some, farther from true coefficients for others (b.5, e.g.)


# Lambda mostly stays low
plot(seq(N_SAMPLES - BURN_IN), ng_3$lambda2[(BURN_IN + 1):N_SAMPLES])

# Model size stays pretty consistent in that 45-50 range
plot(seq(N_SAMPLES - BURN_IN), ng_3$m[(BURN_IN + 1):N_SAMPLES])
```

NG performs well here - you have to threshold the probability of inclusion higher, but once you do that, the algorithm has perfect performance. Slightly better than LASSO, but extremely marginal.


### Model 5

```{r}
#| label: ng 5

y <- model_5$y
X <- model_5$X[,-1]

set.seed(465)
start <- Sys.time()
print(start)
ng_5 <- blasso(X, y, T = N_SAMPLES, case = "ng")
print(Sys.time() - start)

ng_5$beta[,-1] %>% 
  as.data.frame() %>% 
  mutate(iter = row_number()) %>% 
  filter(iter > BURN_IN) %>% 
  pivot_longer(-iter, names_to = "beta", values_to = "value") %>% 
  mutate(beta = substring(beta, 3)) %>% 
  mutate(beta = as.integer(beta) - 1) %>% 
  ggplot(aes(x = iter, y = value)) +
  geom_line() +
  facet_wrap(~beta, scales = "free_y") +
  theme_bw() +
  labs(x = "Iteration", y = "Beta Value", title = "LASSO Progression") +
  theme(plot.title = element_text(hjust = 0.5))

# Probabilities of inclusion
colMeans(ng_5$beta[,-1] != 0)
which(colMeans(ng_5$beta[,-1] != 0) > 0.7) # Have to threshold pretty high to remove random noise

# Est. coefficients
colMeans(ng_5$beta[,-1]) # Most of these are incredibly close to 0


# Lambda mostly stays low
plot(seq(N_SAMPLES - BURN_IN), ng_5$lambda2[(BURN_IN + 1):N_SAMPLES])

# Model size stays pretty consistent in that 50-55 range
plot(seq(N_SAMPLES - BURN_IN), ng_5$m[(BURN_IN + 1):N_SAMPLES])
```

Slightly worse performance, but it's still maybe acceptable with high thresholding.


### Model 7

```{r}
#| label: ng 7

y <- model_7$y
X <- model_7$X[,-1]

set.seed(465)
start <- Sys.time()
print(start)
ng_7 <- blasso(X, y, T = N_SAMPLES, case = "ng")
print(Sys.time() - start)

ng_7$beta[,-1] %>% 
  as.data.frame() %>% 
  mutate(iter = row_number()) %>% 
  filter(iter > BURN_IN) %>% 
  pivot_longer(-iter, names_to = "beta", values_to = "value") %>% 
  mutate(beta = substring(beta, 3)) %>% 
  mutate(beta = as.integer(beta) - 1) %>% 
  ggplot(aes(x = iter, y = value)) +
  geom_line() +
  facet_wrap(~beta, scales = "free_y") +
  theme_bw() +
  labs(x = "Iteration", y = "Beta Value", title = "LASSO Progression") +
  theme(plot.title = element_text(hjust = 0.5))

# Probabilities of inclusion
colMeans(ng_7$beta[,-1] != 0)
which(colMeans(ng_7$beta[,-1] != 0) > 0.7) # Thresholding it high gets it pretty close, but not perfect

# Est. coefficients
colMeans(ng_7$beta[,-1]) # Most of these are incredibly close to 0


# Lambda mostly stays low
plot(seq(N_SAMPLES - BURN_IN), ng_7$lambda2[(BURN_IN + 1):N_SAMPLES])

# Model size stays pretty consistent in that 50-55 range
plot(seq(N_SAMPLES - BURN_IN), ng_7$m[(BURN_IN + 1):N_SAMPLES])
```






# Performance Summary

## Runtime (Order of Magnitude)

### Non-Prior Methods

#### Subset Selection (BBA)

Order of hours.

#### Bayesian Model Selection

Order of seconds-minutes.

#### Bayesian Model Averaging

Order of seconds.

### Prior Methods

#### SSVS

Order of seconds-minutes for normally distributed errors, add in another order of magnitude for student-distributed errors.

#### LASSO

Order of minutes-10s of minutes.

#### NG

Order of minutes-10s of minutes.

#### Horseshoe



## True/False Positive Rates

### Non-Prior Methods

#### Subset Selection (BBA)

#### Bayesian Model Selection

#### Bayesian Model Averaging


### Prior Methods

#### SSVS

#### LASSO

#### NG

#### Horseshoe


## Recovery of True Coefficients (MSE)


