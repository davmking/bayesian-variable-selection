---
title: "experiment"
format: html
---

# Libraries and Data

## Libraries

```{r}
#| label: libraries

library(coda)
library(MCMCpack)
library(BayesVarSel)
library(BMS)
library(BoomSpikeSlab)
library(spikeSlabGAM)
library(bayeslm)
library(BayesSubsets)
library(monomvn)
#library(EMVS)  # Man
library(tidyverse)
```


## Data

Here is a table that describes each model and its characteristics:

| Name | n | p | p_sig |  n vs. p | Description |
|------|---|---|-------|----------|-------------|
| Model 0 | 100 | 10 | 3 | n >> p | Few predictors, sparse |
| Model 1 | 100 | 10 | 8 | n >> p | Few predictors, dense |
| Model 2 | 100 | 95 | 75 | n > p | n slightly > p, dense |
| Model 3 | 100 | 95 | 8 | n > p | n slightly > p, sparse |
| Model 4 | 100 | 105 | 8 | n < p | n slightly < p, sparse |
| Model 5 | 100 | 1000 | 8 | n << p | Many predictors, sparse |

```{r}
#| label: generate data

set.seed(465)

model_0 <- simulate_lm(100, 10, p_sig = 3)
model_1 <- simulate_lm(100, 10, p_sig = 8)
model_2 <- simulate_lm(100, 95, p_sig = 75)
model_3 <- simulate_lm(100, 95, p_sig = 8)
model_4 <- simulate_lm(100, 105, p_sig = 8)
model_5 <- simulate_lm(100, 1000, p_sig = 8)

# Set the first 3 coefficients to be (3, 2, -2), and rescale the response
# Retain the original residuals in doing so
# Note: This should be justified. Residuals are just the error term, and this should be independent of the betas
# Particularly since it is zero-mean
BETA_REPLACE <- c(3, 2, -2)

resids_0 <- model_0$Ey_true - model_0$y
model_0$beta_true[2:4] <- BETA_REPLACE
model_0$Ey_true <- model_0$X %*% model_0$beta_true
model_0$y <- model_0$Ey_true + resids_0

resids_1 <- model_1$Ey_true - model_1$y
model_1$beta_true[2:4] <- BETA_REPLACE
model_1$Ey_true <- model_1$X %*% model_1$beta_true
model_1$y <- model_1$Ey_true + resids_1


resids_2 <- model_2$Ey_true - model_2$y
model_2$beta_true[2:4] <- BETA_REPLACE
model_2$Ey_true <- model_2$X %*% model_2$beta_true
model_2$y <- model_2$Ey_true + resids_2

resids_3 <- model_3$Ey_true - model_3$y
model_3$beta_true[2:4] <- BETA_REPLACE
model_3$Ey_true <- model_3$X %*% model_3$beta_true
model_3$y <- model_3$Ey_true + resids_3

resids_4 <- model_4$Ey_true - model_4$y
model_4$beta_true[2:4] <- BETA_REPLACE
model_4$Ey_true <- model_4$X %*% model_4$beta_true
model_4$y <- model_4$Ey_true + resids_4

resids_5 <- model_5$Ey_true - model_5$y
model_5$beta_true[2:4] <- BETA_REPLACE
model_5$Ey_true <- model_5$X %*% model_5$beta_true
model_5$y <- model_5$Ey_true + resids_5
```

## Hyperparameters

```{r}
#| label: defining hyperparameters

N_SAMPLES <- 11000
BURN_IN <- 1000

N_SAMPLES_2 <- 31000
```



# Non-Prior Methods

## Subset Selection

Note: For now, I am skipping the models with dense signals. I don't think it makes much sense to run them, and I think the computation time is too difficult.

Note: This code needs to be run overnight. It is not currently running fast enough to be useful.

### Model 1

```{r}
#| label: run bayeslm and evaluate

set.seed(465)

X <- model_1$X
y <- model_1$y

model_1_fit <- bayeslm(y ~ X[,-1],
                       N = N_SAMPLES,
                       burnin = BURN_IN,
                       prior = "ridge"
                       )

temp <- post_predict(post_y_hat = tcrossprod(model_1_fit$beta, X),
                    post_sigma = model_1_fit$sigma,
                    yy = y)
post_y_pred = temp$post_y_pred
post_lpd = temp$post_lpd

indicators_1 <- branch_and_bound(yy = fitted(model_1_fit),
                              XX = X)

accept_info <- accept_family(post_y_pred = post_y_pred,
                            post_lpd = post_lpd,
                            XX = X,
                            indicators = indicators_1,
                            yy = y,
                            post_y_hat = tcrossprod(model_1_fit$beta, X))



# How many subsets are in the acceptable family?
length(accept_info$all_accept) # 127

# Simplest acceptable subset:
beta_hat_small <- accept_info$beta_hat_small
beta_hat_small

# Which coefficients are nonzero:
S_small <- which(beta_hat_small != 0)
S_small

# How many coefficients are nonzero:
length(S_small) # 1

# Acceptable subset that minimizes CV error:
beta_hat_min <- accept_info$beta_hat_min

# Typically much larger (and often too large...)
sum(beta_hat_min != 0) # 6

# Variable importance metrics
vi_e <- var_imp(indicators = indicators_1,
               all_accept = accept_info$all_accept)$vi_inc

# Variables appearing in all acceptable subsets
all_acceptable <- which(vi_e == 1)
all_acceptable

# Size
length(all_acceptable) # 1
```

With $n = 100$, we aren't picking up the right variables - the smallest subsets are just the intercept. Might need to increase the SNR or n?


### Model 3

```{r}
#| label: run bayeslm and evaluate 3

set.seed(465)

X <- model_3$X
y <- model_3$y

model_3_fit <- bayeslm(y ~ X[,-1],
                       N = N_SAMPLES,
                       burnin = BURN_IN,
                       prior = "ridge"
                       )

temp <- post_predict(post_y_hat = tcrossprod(model_3_fit$beta, X),
                    post_sigma = model_3_fit$sigma,
                    yy = y)
post_y_pred = temp$post_y_pred
post_lpd = temp$post_lpd

start <- Sys.time()
indicators_3 <- branch_and_bound(yy = fitted(model_3_fit),
                              XX = X)
print(Sys.time() - start)

accept_info <- accept_family(post_y_pred = post_y_pred,
                            post_lpd = post_lpd,
                            XX = X,
                            indicators = indicators_1,
                            yy = y,
                            post_y_hat = tcrossprod(model_1_fit$beta, X))



# How many subsets are in the acceptable family?
length(accept_info$all_accept) # 76

# These are the rows of `indicators` that belong to the acceptable family:
head(accept_info$all_accept)

# An example acceptable subset:
ex_accept <- accept_info$all_accept[1]
which(indicators[ex_accept,])

# Simplest acceptable subset:
beta_hat_small <- accept_info$beta_hat_small

# Which coefficients are nonzero:
S_small <- which(beta_hat_small != 0)

# How many coefficients are nonzero:
length(S_small) # 4

# Acceptable subset that minimizes CV error:
beta_hat_min <- accept_info$beta_hat_min

# Typically much larger (and often too large...)
sum(beta_hat_min != 0) # 5

# Variable importance metrics
vi_e <- var_imp(indicators = indicators_1,
               all_accept = accept_info$all_accept)$vi_inc

# Variables appearing in all acceptable subsets
all_acceptable <- which(vi_e == 1) # Correct variables

# Size
length(all_acceptable) # 4
```


## Bayesian Model Selection

### Model 0

```{r}
#| label: bms 0

y <- model_0$y
X <- model_0$X
data <- data.frame(y = y, X[,-1])

bvs_0 <- GibbsBvs(
  y ~ .,
  data = data,
  prior.betas = "gZellner",
  prior.models = "ScottBerger",
  n.iter = N_SAMPLES - BURN_IN,
  n.burnin = BURN_IN,
  time.test = TRUE,
  seed = 465
)

# Most probable model
bvs_0$HPMbin 

# Inclusion probabilities
bvs_0$inclprob

# Runtime
bvs_0$time
```

Correct variables - need to figure out how to actually extract the posterior probabilities, though.

### Model 1

```{r}
#| label: bms 1

y <- model_1$y
X <- model_1$X
data <- data.frame(y = y, X[,-1])

bvs_1 <- GibbsBvs(
  y ~ .,
  data = data,
  prior.betas = "gZellner",
  prior.models = "ScottBerger",
  n.iter = N_SAMPLES - BURN_IN,
  n.burnin = BURN_IN,
  time.test = TRUE,
  seed = 465
)

# Most probable model
bvs_1$HPMbin 

# Inclusion probabilities
bvs_1$inclprob

# Runtime
bvs_1$time
```

Recovers the correct variables. Not super slow in terms of efficiency.

### Model 2

```{r}
#| label: bms 2

y <- model_2$y
X <- model_2$X
data <- data.frame(y = y, X[,-1])

bvs_2 <- GibbsBvs(
  y ~ .,
  data = data,
  prior.betas = "gZellner",
  prior.models = "ScottBerger",
  n.iter = N_SAMPLES - BURN_IN,
  n.burnin = BURN_IN,
  time.test = TRUE,
  seed = 465
)

# Most probable model
bvs_2$HPMbin 

# Inclusion probabilities
bvs_2$inclprob

# Runtime
bvs_2$time
```

Much worse runtime than Model 3 has. I think this algorithm should not be used on dense signals. It also does not correctly converge to the right subset - it suggests we should have a sparse model, which is inaccurate for this.


### Model 3

```{r}
#| label: bms 3

y <- model_3$y
X <- model_3$X
data <- data.frame(y = y, X[,-1])

bvs_3 <- GibbsBvs(
  y ~ .,
  data = data,
  prior.betas = "gZellner",
  prior.models = "ScottBerger",
  n.iter = N_SAMPLES - BURN_IN,
  n.burnin = BURN_IN,
  time.test = TRUE,
  seed = 465
)

# Most probable model
bvs_3$HPMbin 

# Inclusion probabilities
bvs_3$inclprob

# Runtime
bvs_3$time
```

Runs quickly, but the HPM is only some of the correct coefficients.

### Model 4

```{r}
#| label: bms 4

y <- model_4$y
X <- model_4$X
data <- data.frame(y = y, X[,-1])

bvs_4 <- GibbsBvs(
  y ~ .,
  data = data,
  prior.betas = "gZellner",
  prior.models = "ScottBerger",
  n.iter = N_SAMPLES - BURN_IN,
  n.burnin = BURN_IN,
  time.test = TRUE,
  seed = 465,
  init.model = "Null"
)

# Most probable model
bvs_4$HPMbin 

# Inclusion probabilities
bvs_4$inclprob

# Runtime
bvs_4$time
```

It's actually a little closer than model 3, but not by much. Probably dependent on randomness in the convergence.

### Model 5

```{r}
#| label: bms 5

y <- model_5$y
X <- model_5$X
data <- data.frame(y = y, X[,-1])

bvs_5 <- GibbsBvs(
  y ~ .,
  data = data,
  prior.betas = "gZellner",
  prior.models = "ScottBerger",
  n.iter = N_SAMPLES - BURN_IN,
  n.burnin = BURN_IN,
  time.test = TRUE,
  seed = 465,
  init.model = "Null"
)

# Most probable model
bvs_5$HPMbin 

# Inclusion probabilities
bvs_5$inclprob

# Runtime
bvs_5$time
```

Good performance (albeit slow), but only grabbed the variables with bigger effects.


## Bayesian Model Averaging

### Model 0

```{r}
#| label: BMA 0

y <- model_0$y
X <- model_0$X
data <- data.frame(y = y, X[,-1])

set.seed(465)
bma_0 <- bms(data,
             burn = BURN_IN,
             iter = N_SAMPLES_2,
             mcmc = "rev.jump")

estimates.bma(bma_0, order.by.pip = FALSE) # Correct
which(estimates.bma(bma_0, order.by.pip = FALSE)[,1] > 0.5)

summary(bma_0)
```

Correct, and approximately correct posterior means.

### Model 1

```{r}
#| label: BMA 1

y <- model_1$y
X <- model_1$X
data <- data.frame(y = y, X[,-1])

set.seed(465)
bma_1 <- bms(data,
             burn = BURN_IN,
             iter = N_SAMPLES_2,
             mcmc = "rev.jump")

estimates.bma(bma_1, order.by.pip = FALSE) # Correct
which(estimates.bma(bma_1, order.by.pip = FALSE)[,1] > 0.5)

summary(bma_1)
```

Correct (a little fishy) and very fast.


### Model 2

```{r}
#| label: BMA 2

y <- model_2$y
X <- model_2$X
data <- data.frame(y = y, X[,-1])

set.seed(465)
bma_2 <- bms(data,
             burn = BURN_IN,
             iter = N_SAMPLES_2,
             mcmc = "rev.jump")

estimates.bma(bma_2, order.by.pip = FALSE)
which(estimates.bma(bma_2, order.by.pip = FALSE)[,1] > 0.5) # Includes all predictors

summary(bma_2)
```

Once again, does not do well when we run it on a dense signal. Might want to stop doing that.


### Model 3

```{r}
#| label: BMA 3

y <- model_3$y
X <- model_3$X
data <- data.frame(y = y, X[,-1])

set.seed(465)
bma_3 <- bms(data,
             burn = BURN_IN,
             iter = N_SAMPLES_2,
             mcmc = "rev.jump")

estimates.bma(bma_3, order.by.pip = FALSE)
which(estimates.bma(bma_3, order.by.pip = FALSE)[,1] > 0.5) # Includes all predictors

summary(bma_3)
```

This performed pretty well - it got the most of the true signal correct, with only one false positive. Coefficients are alright.


### Model 4

```{r}
#| label: BMA 4

y <- model_4$y
X <- model_4$X
data <- data.frame(y = y, X[,-1])

set.seed(465)
bma_4 <- bms(data,
             burn = BURN_IN,
             iter = N_SAMPLES_2,
             mcmc = "rev.jump")

estimates.bma(bma_4, order.by.pip = FALSE)
which(estimates.bma(bma_4, order.by.pip = FALSE)[,1] > 0.5) # Includes all predictors

summary(bma_4)
```

Doesn't do great - misses some variables, and has a lot of false positives. Mean regressors is 40. Does have bimodal posterior in terms of model size (maybe trimodal), with the larger mode at the correct size, so there's that. It might converge in more iterations.


### Model 5

```{r}
#| label: BMA 5

y <- model_5$y
X <- model_5$X
data <- data.frame(y = y, X[,-1])

set.seed(465)
bma_5 <- bms(data,
             burn = BURN_IN,
             iter = N_SAMPLES_2,
             mcmc = "rev.jump")

estimates.bma(bma_5, order.by.pip = FALSE)
which(estimates.bma(bma_5, order.by.pip = FALSE)[,1] > 0.5) # Includes several predictors, but doesn't get a single one right

summary(bma_5)
```

Again, this really doesn't do too great. Doesn't get any of the true regressors correct, but thinks we should have a model with ~90 regressors. Posterior for model size is extremely concentrated at this value.

# Prior Methods

## SSVS

### Model 0

```{r}
#| label: SSVS 0

y <- model_0$y
X <- model_0$X
data <- data.frame(y = y, X[,-1])

set.seed(465)
start <- Sys.time()
ssvs_0_g <- lm.spike(y ~ .,
                   data = data,
                   niter = N_SAMPLES_2,
                   error.distribution = "gaussian")
print(Sys.time() - start)

ssvs_0_g$beta[,-1] %>% 
  as.data.frame() %>% 
  mutate(iter = row_number()) %>% 
  filter(iter > BURN_IN) %>% 
  pivot_longer(-iter, names_to = "beta", values_to = "value") %>% 
  mutate(beta = substring(beta, 2)) %>% 
  mutate(beta = as.integer(beta) - 1) %>% 
  ggplot(aes(x = iter, y = value)) +
  geom_line() +
  facet_wrap(~beta, scales = "free_y", nrow = 2) +
  theme_bw() +
  labs(x = "Iteration", y = "Beta Value", title = "SSVS Progression") +
  theme(plot.title = element_text(hjust = 0.5))

# Probability of inclusion
colMeans(ssvs_0_g$beta[BURN_IN:N_SAMPLES_2,-1] != 0)

# Coefficient estimates
colMeans(ssvs_0_g$beta[BURN_IN:N_SAMPLES_2,-1])
```

Correct and reasonably fast. Gets back to the proper coefficients, which is good.


### Model 1

```{r}
#| label: SSVS 1

y <- model_1$y
X <- model_1$X
data <- data.frame(y = y, X[,-1])

set.seed(465)
start <- Sys.time()
ssvs_1_g <- lm.spike(y ~ .,
                   data = data,
                   niter = N_SAMPLES_2,
                   error.distribution = "gaussian")
print(Sys.time() - start)

ssvs_1_g$beta[,-1] %>% 
  as.data.frame() %>% 
  mutate(iter = row_number()) %>% 
  filter(iter > BURN_IN) %>% 
  pivot_longer(-iter, names_to = "beta", values_to = "value") %>% 
  mutate(beta = substring(beta, 2)) %>% 
  mutate(beta = as.integer(beta) - 1) %>% 
  ggplot(aes(x = iter, y = value)) +
  geom_line() +
  facet_wrap(~beta, scales = "free_y", nrow = 2) +
  theme_bw() +
  labs(x = "Iteration", y = "Beta Value", title = "SSVS Progression") +
  theme(plot.title = element_text(hjust = 0.5))

# Probability of inclusion
colMeans(ssvs_1_g$beta[BURN_IN:N_SAMPLES_2,-1] != 0)

# Coefficient estimates
colMeans(ssvs_1_g$beta[BURN_IN:N_SAMPLES_2,-1])
```

Both seem to miss on some of the variables in our signal, and we aren't super close to the correct coefficients in our estimates.


### Model 2

```{r}
#| label: SSVS 2

y <- model_2$y
X <- model_2$X
data <- data.frame(y = y, X[,-1])

set.seed(465)
start <- Sys.time()
ssvs_2_g <- lm.spike(y ~ .,
                   data = data,
                   niter = N_SAMPLES_2,
                   error.distribution = "gaussian")
print(Sys.time() - start)

ssvs_2_g$beta[,-1] %>% 
  as.data.frame() %>% 
  mutate(iter = row_number()) %>% 
  filter(iter > BURN_IN) %>% 
  pivot_longer(-iter, names_to = "beta", values_to = "value") %>% 
  mutate(beta = substring(beta, 2)) %>% 
  mutate(beta = as.integer(beta) - 1) %>% 
  ggplot(aes(x = iter, y = value)) +
  geom_line() +
  facet_wrap(~beta, scales = "free_y") +
  theme_bw() +
  labs(x = "Iteration", y = "Beta Value", title = "SSVS Progression") +
  theme(plot.title = element_text(hjust = 0.5))

# Probability of inclusion
colMeans(ssvs_2_g$beta[BURN_IN:N_SAMPLES_2,-1] != 0)
which(colMeans(ssvs_2_g$beta[BURN_IN:N_SAMPLES_2,-1] != 0) > 0.5) # Awful job

# Coefficient estimates
colMeans(ssvs_2_g$beta[BURN_IN:N_SAMPLES_2,-1])
```

This does a truly terrible job with the dense signal - it recovers almost nothing as being greater than 0. Could we get it there by tweaking the prior? Probably.


### Model 3

```{r}
#| label: SSVS 3

y <- model_3$y
X <- model_3$X
data <- data.frame(y = y, X[,-1])

set.seed(465)
start <- Sys.time()
ssvs_3_g <- lm.spike(y ~ .,
                   data = data,
                   niter = N_SAMPLES_2,
                   error.distribution = "gaussian")
print(Sys.time() - start)

ssvs_3_g$beta[,-1] %>% 
  as.data.frame() %>% 
  mutate(iter = row_number()) %>% 
  filter(iter > BURN_IN) %>% 
  pivot_longer(-iter, names_to = "beta", values_to = "value") %>% 
  mutate(beta = substring(beta, 2)) %>% 
  mutate(beta = as.integer(beta) - 1) %>% 
  ggplot(aes(x = iter, y = value)) +
  geom_line() +
  facet_wrap(~beta, scales = "free_y") +
  theme_bw() +
  labs(x = "Iteration", y = "Beta Value", title = "SSVS Progression") +
  theme(plot.title = element_text(hjust = 0.5))

# Probability of inclusion
colMeans(ssvs_3_g$beta[BURN_IN:N_SAMPLES_2,-1] != 0)
which(colMeans(ssvs_3_g$beta[BURN_IN:N_SAMPLES_2,-1] != 0) > 0.01) # It doesn't give us many whp, but it definitely IDs the strongest candidates first
# Probably would perform better with a smarter prior specification

# Coefficient estimates
colMeans(ssvs_3_g$beta[BURN_IN:N_SAMPLES_2,-1])
```

Not too bad with the sparse model - we would probably want to adjust the prior to have higher inclusion probabilities, because as of right now it's pushing everything pretty close to 0. It does ID some of the correct variables quickly, and it gets a reasonable interpretation of their coefficients. However, it also gives us some false positives.


### Model 4

```{r}
#| label: SSVS 4

y <- model_4$y
X <- model_4$X
data <- data.frame(y = y, X[,-1])

set.seed(465)
start <- Sys.time()
ssvs_4_g <- lm.spike(y ~ .,
                   data = data,
                   niter = N_SAMPLES_2,
                   error.distribution = "gaussian")
print(Sys.time() - start)

# Probability of inclusion
colMeans(ssvs_4_g$beta[BURN_IN:N_SAMPLES_2,-1] != 0)
which(colMeans(ssvs_4_g$beta[BURN_IN:N_SAMPLES_2,-1] != 0) > 0.01) # Mixed bag - gets some of the correct variables, still mostly the ones with stronger signal.

# Coefficient estimates
colMeans(ssvs_4_g$beta[BURN_IN:N_SAMPLES_2,-1])
```

Does a pretty okay job, although I feel like better performance is certainly possible. We are getting somewhat close to our true coefficients in the true positives, anyway.


### Model 5

```{r}
#| label: SSVS 5

y <- model_5$y
X <- model_5$X
data <- data.frame(y = y, X[,-1])

set.seed(465)
start <- Sys.time()
ssvs_5_g <- lm.spike(y ~ .,
                   data = data,
                   niter = N_SAMPLES_2,
                   error.distribution = "gaussian")
print(Sys.time() - start)

# Probability of inclusion
colMeans(ssvs_5_g$beta[BURN_IN:N_SAMPLES_2,-1] != 0)
which(colMeans(ssvs_5_g$beta[BURN_IN:N_SAMPLES_2,-1] != 0) > 0.01) # About 50/50 with true and false positives, also have some false negatives.

# Coefficient estimates
colMeans(ssvs_5_g$beta[BURN_IN:N_SAMPLES_2,-1])
```

Performance is a little worse, but still tolerable. Note that the coefficients are certainly getting worse now.


## LASSO

### Model 0

```{r}
#| label: lasso 0

y <- model_0$y
X <- model_0$X[,-1]

set.seed(465)
lasso_0 <- blasso(X, y, T = N_SAMPLES, case = "default")

lasso_0$beta %>% 
  as.data.frame() %>% 
  mutate(iter = row_number()) %>% 
  filter(iter > BURN_IN) %>% 
  pivot_longer(-iter, names_to = "beta", values_to = "value") %>% 
  mutate(beta = substring(beta, 3)) %>% 
  mutate(beta = as.integer(beta) - 1) %>% 
  ggplot(aes(x = iter, y = value)) +
  geom_line() +
  facet_wrap(~beta, scales = "free_y") +
  theme_bw() +
  labs(x = "Iteration", y = "Beta Value", title = "LASSO Progression") +
  theme(plot.title = element_text(hjust = 0.5))

# Probabilities of inclusion
colMeans(lasso_0$beta != 0)
which(colMeans(lasso_0$beta != 0) > 0.5)

# Est. coefficients
colMeans(lasso_0$beta)

# Lambda stays incredibly low
plot(seq(N_SAMPLES - BURN_IN), lasso_0$lambda2[(BURN_IN + 1):N_SAMPLES])
```

Correct, fast, and restores the correct coefficient values.

### Model 1

```{r}
#| label: lasso 1

y <- model_1$y
X <- model_1$X[,-1]

set.seed(465)
lasso_1 <- blasso(X, y, T = N_SAMPLES, case = "default")

lasso_1$beta %>% 
  as.data.frame() %>% 
  mutate(iter = row_number()) %>% 
  filter(iter > BURN_IN) %>% 
  pivot_longer(-iter, names_to = "beta", values_to = "value") %>% 
  mutate(beta = substring(beta, 3)) %>% 
  mutate(beta = as.integer(beta) - 1) %>% 
  ggplot(aes(x = iter, y = value)) +
  geom_line() +
  facet_wrap(~beta, scales = "free_y") +
  theme_bw() +
  labs(x = "Iteration", y = "Beta Value", title = "LASSO Progression") +
  theme(plot.title = element_text(hjust = 0.5))

# Probabilities of inclusion
colMeans(lasso_1$beta != 0)
which(colMeans(lasso_1$beta != 0) > 0.5)

# Est. coefficients
colMeans(lasso_1$beta)

# Lambda stays very low
plot(seq(N_SAMPLES - BURN_IN), lasso_1$lambda2[(BURN_IN + 1):N_SAMPLES])
```

This almost recovers the correct values - all of the correct variables, but the coefficients are a little off (because they are shrunk towards zero).

### Model 2

```{r}
#| label: lasso 2

y <- model_2$y
X <- model_2$X[,-1]

set.seed(465)
start <- Sys.time()
print(start)
lasso_2 <- blasso(X, y, T = N_SAMPLES, case = "default")
print(Sys.time() - start)

lasso_2$beta %>% 
  as.data.frame() %>% 
  mutate(iter = row_number()) %>% 
  filter(iter > BURN_IN) %>% 
  pivot_longer(-iter, names_to = "beta", values_to = "value") %>% 
  mutate(beta = substring(beta, 3)) %>% 
  mutate(beta = as.integer(beta) - 1) %>% 
  ggplot(aes(x = iter, y = value)) +
  geom_line() +
  facet_wrap(~beta, scales = "free_y") +
  theme_bw() +
  labs(x = "Iteration", y = "Beta Value", title = "LASSO Progression") +
  theme(plot.title = element_text(hjust = 0.5))

# Probabilities of inclusion
colMeans(lasso_2$beta != 0)
which(colMeans(lasso_2$beta != 0) > 0.5) # Should be the first 75, so we aren't capturing a bunch of them

# Est. coefficients
colMeans(lasso_2$beta) # Most of these are incredibly close to 0


# Lambda mostly stays low
plot(seq(N_SAMPLES - BURN_IN), lasso_2$lambda2[(BURN_IN + 1):N_SAMPLES])

# Model size stays pretty consistent in that 45-50 range
plot(seq(N_SAMPLES - BURN_IN), lasso_2$m[(BURN_IN + 1):N_SAMPLES])
```

This definitely doesn't do a great job - it gives us a lot of false negatives, but also has some false positives.


### Model 3

```{r}
#| label: lasso 3

y <- model_3$y
X <- model_3$X[,-1]

set.seed(465)
start <- Sys.time()
print(start)
lasso_3 <- blasso(X, y, T = N_SAMPLES, case = "default")
print(Sys.time() - start)

lasso_3$beta %>% 
  as.data.frame() %>% 
  mutate(iter = row_number()) %>% 
  filter(iter > BURN_IN) %>% 
  pivot_longer(-iter, names_to = "beta", values_to = "value") %>% 
  mutate(beta = substring(beta, 3)) %>% 
  mutate(beta = as.integer(beta) - 1) %>% 
  ggplot(aes(x = iter, y = value)) +
  geom_line() +
  facet_wrap(~beta, scales = "free_y") +
  theme_bw() +
  labs(x = "Iteration", y = "Beta Value", title = "LASSO Progression") +
  theme(plot.title = element_text(hjust = 0.5))

# Probabilities of inclusion
colMeans(lasso_3$beta != 0)
which(colMeans(lasso_3$beta != 0) > 0.65) # Not terrible performance, does well with thresholding. However, there are some very persisitent false positives.

# Est. coefficients
colMeans(lasso_3$beta) # Most of these are incredibly close to 0


# Lambda mostly stays low
plot(seq(N_SAMPLES - BURN_IN), lasso_3$lambda2[(BURN_IN + 1):N_SAMPLES])

# Model size stays pretty consistent in that 45-50 range
plot(seq(N_SAMPLES - BURN_IN), lasso_3$m[(BURN_IN + 1):N_SAMPLES])
```

LASSO performs well here - you have to threshold the probability of inclusion higher, but once you do that, the algorithm has strong performance. Still picking up on some false positives, and we are missing our coefficient estimates.

### Model 4

```{r}
#| label: lasso 4

y <- model_4$y
X <- model_4$X[,-1]

set.seed(465)
start <- Sys.time()
print(start)
lasso_4 <- blasso(X, y, T = N_SAMPLES, case = "default")
print(Sys.time() - start)

lasso_4$beta %>% 
  as.data.frame() %>% 
  mutate(iter = row_number()) %>% 
  filter(iter > BURN_IN) %>% 
  pivot_longer(-iter, names_to = "beta", values_to = "value") %>% 
  mutate(beta = substring(beta, 3)) %>% 
  mutate(beta = as.integer(beta) - 1) %>% 
  ggplot(aes(x = iter, y = value)) +
  geom_line() +
  facet_wrap(~beta, scales = "free_y") +
  theme_bw() +
  labs(x = "Iteration", y = "Beta Value", title = "LASSO Progression") +
  theme(plot.title = element_text(hjust = 0.5))

# Probabilities of inclusion
colMeans(lasso_4$beta != 0)
which(colMeans(lasso_4$beta != 0) > 0.65) # Thresholding it high gets it pretty close, but not perfect

# Est. coefficients
colMeans(lasso_4$beta) # Most of these are incredibly close to 0


# Lambda mostly stays low
plot(seq(N_SAMPLES - BURN_IN), lasso_4$lambda2[(BURN_IN + 1):N_SAMPLES])

# Model size stays pretty consistent in that 50-55 range
plot(seq(N_SAMPLES - BURN_IN), lasso_4$m[(BURN_IN + 1):N_SAMPLES])
```

Actually slightly better performance here than the previous - I think this must just be a factor of the random runs.


### Model 5

```{r}
#| label: lasso 5

y <- model_5$y
X <- model_5$X[,-1]

set.seed(465)
start <- Sys.time()
print(start)
lasso_5 <- blasso(X, y, T = N_SAMPLES, case = "default")
print(Sys.time() - start)

lasso_5$beta %>% 
  as.data.frame() %>% 
  mutate(iter = row_number()) %>% 
  filter(iter > BURN_IN) %>% 
  pivot_longer(-iter, names_to = "beta", values_to = "value") %>% 
  mutate(beta = substring(beta, 3)) %>% 
  mutate(beta = as.integer(beta) - 1) %>% 
  ggplot(aes(x = iter, y = value)) +
  geom_line() +
  facet_wrap(~beta, scales = "free_y") +
  theme_bw() +
  labs(x = "Iteration", y = "Beta Value", title = "LASSO Progression") +
  theme(plot.title = element_text(hjust = 0.5))

# Probabilities of inclusion
colMeans(lasso_5$beta != 0)
which(colMeans(lasso_5$beta != 0) > 0.7) # Thresholding it high gets it pretty close, but not perfect

# Est. coefficients
colMeans(lasso_5$beta) # Most of these are incredibly close to 0


# Lambda mostly stays low
plot(seq(N_SAMPLES - BURN_IN), lasso_5$lambda2[(BURN_IN + 1):N_SAMPLES])

# Model size stays pretty consistent in that 50-55 range
plot(seq(N_SAMPLES - BURN_IN), lasso_5$m[(BURN_IN + 1):N_SAMPLES])
```

Slightly worse performance, but it's still pretty good with high thresholding.


## NG

### Model 0

```{r}
#| label: ng 1

y <- model_0$y
X <- model_0$X[,-1]

set.seed(465)
ng_0 <- blasso(X, y, T = N_SAMPLES, case = "ng")

ng_0$beta %>% 
  as.data.frame() %>% 
  mutate(iter = row_number()) %>% 
  filter(iter > BURN_IN) %>% 
  pivot_longer(-iter, names_to = "beta", values_to = "value") %>% 
  mutate(beta = substring(beta, 3)) %>% 
  mutate(beta = as.integer(beta) - 1) %>% 
  ggplot(aes(x = iter, y = value)) +
  geom_line() +
  facet_wrap(~beta, scales = "free_y") +
  theme_bw() +
  labs(x = "Iteration", y = "Beta Value", title = "LASSO Progression") +
  theme(plot.title = element_text(hjust = 0.5))

# Probabilities of inclusion
colMeans(ng_0$beta != 0)
which(colMeans(ng_0$beta != 0) > 0.5)

# Est. coefficients
colMeans(ng_0$beta)

# Lambda stays very low
plot(seq(N_SAMPLES - BURN_IN), ng_0$lambda2[(BURN_IN + 1):N_SAMPLES])
```

Very well done, gets the correct variables and super close to correct coefficients.


### Model 1

```{r}
#| label: ng 1

y <- model_1$y
X <- model_1$X[,-1]

set.seed(465)
ng_1 <- blasso(X, y, T = N_SAMPLES, case = "ng")

ng_1$beta %>% 
  as.data.frame() %>% 
  mutate(iter = row_number()) %>% 
  filter(iter > BURN_IN) %>% 
  pivot_longer(-iter, names_to = "beta", values_to = "value") %>% 
  mutate(beta = substring(beta, 3)) %>% 
  mutate(beta = as.integer(beta) - 1) %>% 
  ggplot(aes(x = iter, y = value)) +
  geom_line() +
  facet_wrap(~beta, scales = "free_y") +
  theme_bw() +
  labs(x = "Iteration", y = "Beta Value", title = "LASSO Progression") +
  theme(plot.title = element_text(hjust = 0.5))

# Probabilities of inclusion
colMeans(ng_1$beta != 0)
which(colMeans(ng_1$beta != 0) > 0.55) # Needs a little thresholding

# Est. coefficients
colMeans(ng_1$beta)

# Lambda mostly stays low
plot(seq(N_SAMPLES - BURN_IN), ng_1$lambda2[(BURN_IN + 1):N_SAMPLES])
```

Gets the correct variables, but it does miss on the coefficients a good bit.


### Model 2

```{r}
#| label: ng 2

y <- model_2$y
X <- model_2$X[,-1]

set.seed(465)
start <- Sys.time()
print(start)
ng_2 <- blasso(X, y, T = N_SAMPLES, case = "ng")
print(Sys.time() - start)

ng_2$beta %>% 
  as.data.frame() %>% 
  mutate(iter = row_number()) %>% 
  filter(iter > BURN_IN) %>% 
  pivot_longer(-iter, names_to = "beta", values_to = "value") %>% 
  mutate(beta = substring(beta, 3)) %>% 
  mutate(beta = as.integer(beta) - 1) %>% 
  ggplot(aes(x = iter, y = value)) +
  geom_line() +
  facet_wrap(~beta, scales = "free_y") +
  theme_bw() +
  labs(x = "Iteration", y = "Beta Value", title = "LASSO Progression") +
  theme(plot.title = element_text(hjust = 0.5))

# Probabilities of inclusion
colMeans(ng_2$beta != 0)
which(colMeans(ng_2$beta != 0) > 0.5) # So we're only capturing some of them

# Est. coefficients
colMeans(ng_2$beta)


# Lambda mostly stays low
plot(seq(N_SAMPLES - BURN_IN), ng_2$lambda2[(BURN_IN + 1):N_SAMPLES])

# Model size stays pretty consistent in that 45-50 range
plot(seq(N_SAMPLES - BURN_IN), ng_2$m[(BURN_IN + 1):N_SAMPLES])
```

This definitely doesn't do a great job - it gives us a lot of false negatives, and only captures about half of the true negatives.


### Model 3

```{r}
#| label: ng 3

y <- model_3$y
X <- model_3$X[,-1]

set.seed(465)
start <- Sys.time()
print(start)
ng_3 <- blasso(X, y, T = N_SAMPLES, case = "ng")
print(Sys.time() - start)

ng_3$beta %>% 
  as.data.frame() %>% 
  mutate(iter = row_number()) %>% 
  filter(iter > BURN_IN) %>% 
  pivot_longer(-iter, names_to = "beta", values_to = "value") %>% 
  mutate(beta = substring(beta, 3)) %>% 
  mutate(beta = as.integer(beta) - 1) %>% 
  ggplot(aes(x = iter, y = value)) +
  geom_line() +
  facet_wrap(~beta, scales = "free_y") +
  theme_bw() +
  labs(x = "Iteration", y = "Beta Value", title = "LASSO Progression") +
  theme(plot.title = element_text(hjust = 0.5))

# Probabilities of inclusion
colMeans(ng_3$beta != 0)
which(colMeans(ng_3$beta != 0) > 0.7) # Not terrible performance, does a better job if you threshold it high

# Est. coefficients
colMeans(ng_3$beta) # Most of these are incredibly close to 0
# Shrinkage still affecting our coefficients


# Lambda mostly stays low
plot(seq(N_SAMPLES - BURN_IN), ng_3$lambda2[(BURN_IN + 1):N_SAMPLES])

# Model size stays pretty consistent in that 45-50 range
plot(seq(N_SAMPLES - BURN_IN), ng_3$m[(BURN_IN + 1):N_SAMPLES])
```

NG performs well here - you have to threshold the probability of inclusion higher, but once you do that, the algorithm has perfect performance. Slightly better than LASSO, but extremely marginal.

### Model 4

```{r}
#| label: ng 4

y <- model_4$y
X <- model_4$X[,-1]

set.seed(465)
start <- Sys.time()
print(start)
ng_4 <- blasso(X, y, T = N_SAMPLES, case = "ng")
print(Sys.time() - start)

ng_4$beta %>% 
  as.data.frame() %>% 
  mutate(iter = row_number()) %>% 
  filter(iter > BURN_IN) %>% 
  pivot_longer(-iter, names_to = "beta", values_to = "value") %>% 
  mutate(beta = substring(beta, 3)) %>% 
  mutate(beta = as.integer(beta) - 1) %>% 
  ggplot(aes(x = iter, y = value)) +
  geom_line() +
  facet_wrap(~beta, scales = "free_y") +
  theme_bw() +
  labs(x = "Iteration", y = "Beta Value", title = "LASSO Progression") +
  theme(plot.title = element_text(hjust = 0.5))

# Probabilities of inclusion
colMeans(ng_4$beta != 0)
which(colMeans(ng_4$beta != 0) > 0.65) # Thresholding it high gets it pretty close, but not perfect

# Est. coefficients
colMeans(ng_4$beta) # Most of these are incredibly close to 0


# Lambda mostly stays low
plot(seq(N_SAMPLES - BURN_IN), ng_4$lambda2[(BURN_IN + 1):N_SAMPLES])

# Model size stays pretty consistent in that 50-55 range
plot(seq(N_SAMPLES - BURN_IN), ng_4$m[(BURN_IN + 1):N_SAMPLES])
```

Does pretty well - maybe slightly better than with $n > p$ in the previous example, though again these are close. Still having some issues with the true pos/neg, but at least the coefficients are pretty close here.


### Model 5

```{r}
#| label: ng 5

y <- model_5$y
X <- model_5$X[,-1]

set.seed(465)
start <- Sys.time()
print(start)
ng_5 <- blasso(X, y, T = N_SAMPLES, case = "ng")
print(Sys.time() - start)

ng_5$beta %>% 
  as.data.frame() %>% 
  mutate(iter = row_number()) %>% 
  filter(iter > BURN_IN) %>% 
  pivot_longer(-iter, names_to = "beta", values_to = "value") %>% 
  mutate(beta = substring(beta, 3)) %>% 
  mutate(beta = as.integer(beta) - 1) %>% 
  ggplot(aes(x = iter, y = value)) +
  geom_line() +
  facet_wrap(~beta, scales = "free_y") +
  theme_bw() +
  labs(x = "Iteration", y = "Beta Value", title = "LASSO Progression") +
  theme(plot.title = element_text(hjust = 0.5))

# Probabilities of inclusion
colMeans(ng_5$beta != 0)
which(colMeans(ng_5$beta != 0) > 0.7) # Have to threshold pretty high to remove random noise

# Est. coefficients
colMeans(ng_5$beta) # Most of these are incredibly close to 0


# Lambda mostly stays low
plot(seq(N_SAMPLES - BURN_IN), ng_5$lambda2[(BURN_IN + 1):N_SAMPLES])

# Model size stays pretty consistent in that 50-55 range
plot(seq(N_SAMPLES - BURN_IN), ng_5$m[(BURN_IN + 1):N_SAMPLES])
```

Slightly worse performance, but it's still maybe acceptable with high thresholding.









# Performance Summary

## Runtime (Order of Magnitude)

### Non-Prior Methods

#### Subset Selection (BBA)

Order of hours.

#### Bayesian Model Selection

Order of seconds-minutes.

#### Bayesian Model Averaging

Order of seconds.

### Prior Methods

#### SSVS

Order of seconds-minutes for normally distributed errors, add in another order of magnitude for student-distributed errors.

#### LASSO

Order of minutes-10s of minutes.

#### NG

Order of minutes-10s of minutes.

#### Horseshoe



## True/False Positive Rates

### Non-Prior Methods

#### Subset Selection (BBA)

#### Bayesian Model Selection

#### Bayesian Model Averaging


### Prior Methods

#### SSVS

#### LASSO

#### NG

#### Horseshoe


## Recovery of True Coefficients (MSE)


