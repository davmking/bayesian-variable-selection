---
title: "experiment"
format: html
---

# Libraries and Data

## Libraries

```{r}
#| label: libraries

library(coda)
library(MCMCpack)
library(BayesVarSel)
library(BMS)
library(BoomSpikeSlab)
library(spikeSlabGAM)
library(bayeslm)
library(BayesSubsets)
library(monomvn)
#library(EMVS)  # Man
library(tidyverse)
library(knitr)
library(pROC)
library(EMVS) # installed from CRAN archive version 1.2.1
library(viridis)
```


## Data

Here is a table that describes each model and its characteristics:

| Name | n | p | p_sig |  n vs. p | Description |
|------|---|---|-------|----------|-------------|
| Model 0 | 100 | 10 | 3 | n >> p | Few predictors, sparse |
| Model 1 | 100 | 10 | 8 | n >> p | Few predictors, dense |
| Model 2 | 100 | 95 | 75 | n > p | n slightly > p, dense |
| Model 3 | 100 | 95 | 8 | n > p | n slightly > p, sparse |
| Model 4 | 100 | 105 | 8 | n < p | n slightly < p, sparse |
| Model 5 | 100 | 1000 | 8 | n << p | Many predictors, sparse |

```{r}
#| label: generate data

set.seed(465)

model_0 <- simulate_lm(100, 10, p_sig = 3)
model_1 <- simulate_lm(100, 10, p_sig = 8)
model_2 <- simulate_lm(100, 95, p_sig = 75)
model_3 <- simulate_lm(100, 95, p_sig = 8)
model_4 <- simulate_lm(100, 105, p_sig = 8)
model_5 <- simulate_lm(100, 1000, p_sig = 8)

# Set the first 3 coefficients to be (3, 2, -2), and rescale the response
# Retain the original residuals in doing so
# Note: This should be justified. Residuals are just the error term, and this should be independent of the betas
# Particularly since it is zero-mean
BETA_REPLACE <- c(3, 2, -2)

resids_0 <- model_0$Ey_true - model_0$y
model_0$beta_true[2:4] <- BETA_REPLACE
model_0$Ey_true <- model_0$X %*% model_0$beta_true
model_0$y <- model_0$Ey_true + resids_0

resids_1 <- model_1$Ey_true - model_1$y
model_1$beta_true[2:4] <- BETA_REPLACE
model_1$Ey_true <- model_1$X %*% model_1$beta_true
model_1$y <- model_1$Ey_true + resids_1


resids_2 <- model_2$Ey_true - model_2$y
model_2$beta_true[2:4] <- BETA_REPLACE
model_2$Ey_true <- model_2$X %*% model_2$beta_true
model_2$y <- model_2$Ey_true + resids_2

resids_3 <- model_3$Ey_true - model_3$y
model_3$beta_true[2:4] <- BETA_REPLACE
model_3$Ey_true <- model_3$X %*% model_3$beta_true
model_3$y <- model_3$Ey_true + resids_3

resids_4 <- model_4$Ey_true - model_4$y
model_4$beta_true[2:4] <- BETA_REPLACE
model_4$Ey_true <- model_4$X %*% model_4$beta_true
model_4$y <- model_4$Ey_true + resids_4

resids_5 <- model_5$Ey_true - model_5$y
model_5$beta_true[2:4] <- BETA_REPLACE
model_5$Ey_true <- model_5$X %*% model_5$beta_true
model_5$y <- model_5$Ey_true + resids_5
```

### Test Data

```{r}
#| label: generate test data

set.seed(16384)

model_0_test <- simulate_lm(100, 10, p_sig = 3)
model_1_test <- simulate_lm(100, 10, p_sig = 8)
model_2_test <- simulate_lm(100, 95, p_sig = 75)
model_3_test <- simulate_lm(100, 95, p_sig = 8)
model_4_test <- simulate_lm(100, 105, p_sig = 8)
model_5_test <- simulate_lm(100, 1000, p_sig = 8)

# Set the first 3 coefficients to be (3, 2, -2), and rescale the response
# Retain the original residuals in doing so
# Note: This should be justified. Residuals are just the error term, and this should be independent of the betas
# Particularly since it is zero-mean
BETA_REPLACE <- c(3, 2, -2)

resids_0_test <- model_0_test$Ey_true - model_0_test$y
model_0_test$beta_true[2:4] <- BETA_REPLACE
model_0_test$Ey_true <- model_0_test$X %*% model_0_test$beta_true
model_0_test$y <- model_0_test$Ey_true + resids_0_test

resids_1_test <- model_1_test$Ey_true - model_1_test$y
model_1_test$beta_true[2:4] <- BETA_REPLACE
model_1_test$Ey_true <- model_1_test$X %*% model_1_test$beta_true
model_1_test$y <- model_1_test$Ey_true + resids_1_test


resids_2_test <- model_2_test$Ey_true - model_2_test$y
model_2_test$beta_true[2:4] <- BETA_REPLACE
model_2_test$Ey_true <- model_2_test$X %*% model_2_test$beta_true
model_2_test$y <- model_2_test$Ey_true + resids_2_test

resids_3_test <- model_3_test$Ey_true - model_3_test$y
model_3_test$beta_true[2:4] <- BETA_REPLACE
model_3_test$Ey_true <- model_3_test$X %*% model_3_test$beta_true
model_3_test$y <- model_3_test$Ey_true + resids_3_test

resids_4_test <- model_4_test$Ey_true - model_4_test$y
model_4_test$beta_true[2:4] <- BETA_REPLACE
model_4_test$Ey_true <- model_4_test$X %*% model_4_test$beta_true
model_4_test$y <- model_4_test$Ey_true + resids_4_test

resids_5_test <- model_5_test$Ey_true - model_5_test$y
model_5_test$beta_true[2:4] <- BETA_REPLACE
model_5_test$Ey_true <- model_5_test$X %*% model_5_test$beta_true
model_5_test$y <- model_5_test$Ey_true + resids_5_test
```

## Hyperparameters

```{r}
#| label: defining hyperparameters

N_SAMPLES <- 11000
BURN_IN <- 1000

N_SAMPLES_2 <- 31000
```

## Helpers

```{r}
#| label: plot betas function

# Note: Pass it only the variables you want plotted, i.e. exclude intercept
plot_betas <- function(betas){
  betas %>% 
    as.data.frame() %>% 
    mutate(iter = row_number()) %>% 
    filter(iter > BURN_IN) %>% 
    pivot_longer(-iter, names_to = "beta", values_to = "value") %>% 
    mutate(beta = substring(beta, 2)) %>% 
    mutate(beta = as.integer(beta) - 1) %>% 
    ggplot(aes(x = iter, y = value)) +
    geom_line() +
    facet_wrap(~beta, scales = "free_y", nrow = 2) +
    theme_bw() +
    labs(x = "Iteration", y = "Beta Value", title = "SSVS Progression") +
    theme(plot.title = element_text(hjust = 0.5))
}
```




# Non-Prior Methods

## Subset Selection

Note: For now, I am skipping the models with dense signals. I don't think it makes much sense to run them, and I think the computation time is too difficult.

Note: This code needs to be run overnight. It is not currently running fast enough to be useful.

### Model 0

```{r}
#| label: run bayeslm and evaluate

set.seed(465)

X <- model_0$X
y <- model_0$y

model_0_fit <- bayeslm(y ~ X[,-1],
                       N = N_SAMPLES,
                       burnin = BURN_IN,
                       prior = "ridge"
                       )

temp <- post_predict(post_y_hat = tcrossprod(model_0_fit$beta, X),
                    post_sigma = model_0_fit$sigma,
                    yy = y)
post_y_pred = temp$post_y_pred
post_lpd = temp$post_lpd

indicators_0 <- branch_and_bound(yy = fitted(model_0_fit),
                              XX = X)

accept_info_0 <- accept_family(post_y_pred = post_y_pred,
                            post_lpd = post_lpd,
                            XX = X,
                            indicators = indicators_0,
                            yy = y,
                            post_y_hat = tcrossprod(model_0_fit$beta, X))

# Smallest acceptable subset
bss_0_small <- as.integer(accept_info_0$beta_hat_small[-1] != 0)
bss_0_small


# Acceptable subset that minimizes CV error:
bss_0_min <- as.integer(accept_info_0$beta_hat_min[-1] != 0)
bss_0_min

# Present in all acceptable subsets
bss_0_all <- as.integer(var_imp(indicators = indicators_0,
               all_accept = accept_info_0$all_accept)$vi_inc[-1] == 1)
bss_0_all
```

### Model 1

```{r}
#| label: run bayeslm and evaluate 1

set.seed(465)

X <- model_1$X
y <- model_1$y

model_1_fit <- bayeslm(y ~ X[,-1],
                       N = N_SAMPLES,
                       burnin = BURN_IN,
                       prior = "ridge"
                       )

temp <- post_predict(post_y_hat = tcrossprod(model_1_fit$beta, X),
                    post_sigma = model_1_fit$sigma,
                    yy = y)
post_y_pred = temp$post_y_pred
post_lpd = temp$post_lpd

indicators_1 <- branch_and_bound(yy = fitted(model_1_fit),
                              XX = X)

accept_info_1 <- accept_family(post_y_pred = post_y_pred,
                            post_lpd = post_lpd,
                            XX = X,
                            indicators = indicators_1,
                            yy = y,
                            post_y_hat = tcrossprod(model_1_fit$beta, X))

# Smallest acceptable subset
bss_1_small <- as.integer(accept_info_1$beta_hat_small[-1] != 0)
bss_1_small


# Acceptable subset that minimizes CV error:
bss_1_min <- as.integer(accept_info_1$beta_hat_min[-1] != 0)
bss_1_min

# Present in all acceptable subsets
bss_1_all <- as.integer(var_imp(indicators = indicators_1,
               all_accept = accept_info_1$all_accept)$vi_inc[-1] == 1)
bss_1_all
```

With $n = 100$, we aren't picking up the right variables - the smallest subsets are just the intercept. Might need to increase the SNR or n?


## Bayesian Model Selection

### Model 0

```{r}
#| label: bms 0

y <- model_0$y
X <- model_0$X
data <- data.frame(y = y, X[,-1])

bvs_0 <- GibbsBvs(
  y ~ .,
  data = data,
  prior.betas = "gZellner",
  prior.models = "ScottBerger",
  n.iter = N_SAMPLES - BURN_IN,
  n.burnin = BURN_IN,
  time.test = FALSE,
  seed = 465
)

# Most probable model
bvs_0$HPMbin 

# Inclusion probabilities
bvs_0$inclprob

# Runtime
bvs_0$time
```

Correct variables - need to figure out how to actually extract the posterior probabilities, though.

### Model 1

```{r}
#| label: bms 1

y <- model_1$y
X <- model_1$X
data <- data.frame(y = y, X[,-1])

bvs_1 <- GibbsBvs(
  y ~ .,
  data = data,
  prior.betas = "gZellner",
  prior.models = "ScottBerger",
  n.iter = N_SAMPLES - BURN_IN,
  n.burnin = BURN_IN,
  time.test = FALSE,
  seed = 465
)

# Most probable model
bvs_1$HPMbin 

# Inclusion probabilities
bvs_1$inclprob

# Runtime
bvs_1$time
```

Recovers the correct variables. Not super slow in terms of efficiency.

### Model 2

```{r}
#| label: bms 2

y <- model_2$y
X <- model_2$X
data <- data.frame(y = y, X[,-1])

bvs_2 <- GibbsBvs(
  y ~ .,
  data = data,
  prior.betas = "gZellner",
  prior.models = "ScottBerger",
  n.iter = N_SAMPLES - BURN_IN,
  n.burnin = BURN_IN,
  time.test = FALSE,
  seed = 465
)

# Most probable model
bvs_2$HPMbin 

# Inclusion probabilities
bvs_2$inclprob

# Runtime
bvs_2$time
```

Much worse runtime than Model 3 has. I think this algorithm should not be used on dense signals. It also does not correctly converge to the right subset - it suggests we should have a sparse model, which is inaccurate for this.


### Model 3

```{r}
#| label: bms 3

y <- model_3$y
X <- model_3$X
data <- data.frame(y = y, X[,-1])

bvs_3 <- GibbsBvs(
  y ~ .,
  data = data,
  prior.betas = "gZellner",
  prior.models = "ScottBerger",
  n.iter = N_SAMPLES - BURN_IN,
  n.burnin = BURN_IN,
  time.test = FALSE,
  seed = 465
)

# Most probable model
bvs_3$HPMbin 

# Inclusion probabilities
bvs_3$inclprob

# Runtime
bvs_3$time
```

Runs quickly, but the HPM is only some of the correct coefficients.

### Model 4

```{r}
#| label: bms 4

y <- model_4$y
X <- model_4$X
data <- data.frame(y = y, X[,-1])

bvs_4 <- GibbsBvs(
  y ~ .,
  data = data,
  prior.betas = "gZellner",
  prior.models = "ScottBerger",
  n.iter = N_SAMPLES - BURN_IN,
  n.burnin = BURN_IN,
  time.test = FALSE,
  seed = 465,
  init.model = "Null"
)

# Most probable model
bvs_4$HPMbin 

# Inclusion probabilities
bvs_4$inclprob

# Runtime
bvs_4$time
```

It's actually a little closer than model 3, but not by much. Probably dependent on randomness in the convergence.

### Model 5

```{r}
#| label: bms 5

y <- model_5$y
X <- model_5$X
data <- data.frame(y = y, X[,-1])

bvs_5 <- GibbsBvs(
  y ~ .,
  data = data,
  prior.betas = "gZellner",
  prior.models = "ScottBerger",
  n.iter = N_SAMPLES - BURN_IN,
  n.burnin = BURN_IN,
  time.test = FALSE,
  seed = 465,
  init.model = "Null"
)

# Most probable model
bvs_5$HPMbin 

# Inclusion probabilities
bvs_5$inclprob

# Runtime
bvs_5$time
```

Good performance (albeit slow), but only grabbed the variables with bigger effects.


## Bayesian Model Averaging

### Model 0

```{r}
#| label: BMA 0

y <- model_0$y
X <- model_0$X
data <- data.frame(y = y, X[,-1])

set.seed(465)
bma_0 <- bms(data,
             burn = BURN_IN,
             iter = N_SAMPLES_2,
             mcmc = "rev.jump")

estimates.bma(bma_0, order.by.pip = FALSE) # Correct
which(estimates.bma(bma_0, order.by.pip = FALSE)[,1] > 0.5)

summary(bma_0)
```

Correct, and approximately correct posterior means.

### Model 1

```{r}
#| label: BMA 1

y <- model_1$y
X <- model_1$X
data <- data.frame(y = y, X[,-1])

set.seed(465)
bma_1 <- bms(data,
             burn = BURN_IN,
             iter = N_SAMPLES_2,
             mcmc = "rev.jump")

estimates.bma(bma_1, order.by.pip = FALSE) # Correct
which(estimates.bma(bma_1, order.by.pip = FALSE)[,1] > 0.5)

summary(bma_1)
```

Correct (a little fishy) and very fast.


### Model 2

```{r}
#| label: BMA 2

y <- model_2$y
X <- model_2$X
data <- data.frame(y = y, X[,-1])

set.seed(465)
bma_2 <- bms(data,
             burn = BURN_IN,
             iter = N_SAMPLES_2,
             mcmc = "rev.jump")

estimates.bma(bma_2, order.by.pip = FALSE)
which(estimates.bma(bma_2, order.by.pip = FALSE)[,1] > 0.5) # Includes all predictors

summary(bma_2)
```

Once again, does not do well when we run it on a dense signal. Might want to stop doing that.


### Model 3

```{r}
#| label: BMA 3

y <- model_3$y
X <- model_3$X
data <- data.frame(y = y, X[,-1])

set.seed(465)
bma_3 <- bms(data,
             burn = BURN_IN,
             iter = N_SAMPLES_2,
             mcmc = "rev.jump")

estimates.bma(bma_3, order.by.pip = FALSE)
which(estimates.bma(bma_3, order.by.pip = FALSE)[,1] > 0.5) # Includes all predictors

summary(bma_3)
```

This performed pretty well - it got the most of the true signal correct, with only one false positive. Coefficients are alright.


### Model 4

```{r}
#| label: BMA 4

y <- model_4$y
X <- model_4$X
data <- data.frame(y = y, X[,-1])

set.seed(465)
bma_4 <- bms(data,
             burn = BURN_IN,
             iter = N_SAMPLES_2,
             mcmc = "rev.jump")

estimates.bma(bma_4, order.by.pip = FALSE)
which(estimates.bma(bma_4, order.by.pip = FALSE)[,1] > 0.5) # Includes all predictors

summary(bma_4)
```

Doesn't do great - misses some variables, and has a lot of false positives. Mean regressors is 40. Does have bimodal posterior in terms of model size (maybe trimodal), with the larger mode at the correct size, so there's that. It might converge in more iterations.


### Model 5

```{r}
#| label: BMA 5

y <- model_5$y
X <- model_5$X
data <- data.frame(y = y, X[,-1])

set.seed(465)
bma_5 <- bms(data,
             burn = BURN_IN,
             iter = N_SAMPLES_2,
             mcmc = "rev.jump")

estimates.bma(bma_5, order.by.pip = FALSE)
which(estimates.bma(bma_5, order.by.pip = FALSE)[,1] > 0.5) # Includes several predictors, but doesn't get a single one right

summary(bma_5)
```

Again, this really doesn't do too great. Doesn't get any of the true regressors correct, but thinks we should have a model with ~90 regressors. Posterior for model size is extremely concentrated at this value.


## EMVS

```{r}
#| label: vars to try

v0_try <- c(1e-6, 1e-5, 1e-4, 1e-3, 1e-2, 1e-1)
```


### Model 0

```{r}
#| label: emvs model 0

X <- model_0$X[,-1]
y <- model_0$y

set.seed(465)
emvs_0 <- EMVS(
  X = X,
  Y = y,
  #v0 = 0.01,
  v0 = v0_try,
  v1 = 1,
  type = "betabinomial",
  beta_init = rep(0, ncol(X)),
  sigma_init = 1,
  epsilon = 1e-6,
  a = 1,
  b = ncol(X)
)

# Estimates: very close to expectation
emvs_0$betas

# Inclusion probabilites: accurate
emvs_0$prob_inclusion
emvs_0$prob_inclusion > 0.5
rowSums(emvs_0$prob_inclusion > 0.5)

# Iterations to converge
emvs_0$niters
```


### Model 1

```{r}
#| label: emvs model 1

X <- model_1$X[,-1]
y <- model_1$y

set.seed(465)
emvs_1 <- EMVS(
  X = X,
  Y = y,
  #v0 = 0.01,
  v0 = v0_try,
  v1 = 1,
  type = "betabinomial",
  beta_init = rep(0, ncol(X)),
  sigma_init = 1,
  epsilon = 1e-6,
  a = 1,
  b = ncol(X)
)

# Estimates: very close to expectation
emvs_1$betas

# Inclusion probabilites: accurate
emvs_1$prob_inclusion
emvs_1$prob_inclusion > 0.5
rowSums(emvs_1$prob_inclusion > 0.5)

# Iterations to converge
emvs_1$niters
```

Somewhat worse performance here.


### Model 2

```{r}
#| label: emvs model 2

X <- model_2$X[,-1]
y <- model_2$y

set.seed(465)
emvs_2 <- EMVS(
  X = X,
  Y = y,
  #v0 = 0.01,
  v0 = v0_try,
  v1 = 1,
  type = "betabinomial",
  beta_init = rep(0, ncol(X)),
  sigma_init = 1,
  epsilon = 1e-6,
  a = 1,
  b = ncol(X)
)

# Estimates: very close to expectation
emvs_2$betas

# Inclusion probabilites: accurate
emvs_2$prob_inclusion
emvs_2$prob_inclusion > 0.5
rowSums(emvs_2$prob_inclusion > 0.5)

# Iterations to converge
emvs_2$niters
```

### Model 3

```{r}
#| label: emvs model 3

X <- model_3$X[,-1]
y <- model_3$y

set.seed(465)
emvs_3 <- EMVS(
  X = X,
  Y = y,
  #v0 = 0.01,
  v0 = v0_try,
  v1 = 1,
  type = "betabinomial",
  beta_init = rep(0, ncol(X)),
  sigma_init = 1,
  epsilon = 1e-6,
  a = 1,
  b = ncol(X)
)

# Estimates: very close to expectation
emvs_3$betas

# Inclusion probabilites: accurate
emvs_3$prob_inclusion
emvs_3$prob_inclusion > 0.5
rowSums(emvs_3$prob_inclusion > 0.5)

# Iterations to converge
emvs_3$niters
```

### Model 4

```{r}
#| label: emvs model 4

X <- model_4$X[,-1]
y <- model_4$y

set.seed(465)
emvs_4 <- EMVS(
  X = X,
  Y = y,
  #v0 = 0.01,
  v0 = v0_try,
  v1 = 1,
  type = "betabinomial",
  beta_init = rep(0, ncol(X)),
  sigma_init = 1,
  epsilon = 1e-6,
  a = 1,
  b = ncol(X)
)

# Estimates: very close to expectation
emvs_4$betas

# Inclusion probabilites: accurate
emvs_4$prob_inclusion
emvs_4$prob_inclusion > 0.5
rowSums(emvs_4$prob_inclusion > 0.5)

# Iterations to converge
emvs_4$niters
```

### Model 5

```{r}
#| label: emvs model 5

X <- model_5$X[,-1]
y <- model_5$y

set.seed(465)
emvs_5 <- EMVS(
  X = X,
  Y = y,
  #v0 = 0.01,
  v0 = v0_try,
  v1 = 1,
  type = "betabinomial",
  beta_init = rep(0, ncol(X)),
  sigma_init = 1,
  epsilon = 1e-6,
  a = 1,
  b = ncol(X)
)

# Estimates: very close to expectation
emvs_5$betas

# Inclusion probabilites: accurate
emvs_5$prob_inclusion
emvs_5$prob_inclusion > 0.5
rowSums(emvs_5$prob_inclusion > 0.5)

# Iterations to converge
emvs_5$niters
```

```{r}
#| label: model 5 temperature test

X <- model_5$X[,-1]
y <- model_5$y

set.seed(465)
emvs_5_t0 <- EMVS(
  X = X,
  Y = y,
  #v0 = 0.01,
  v0 = v0_try,
  v1 = 1,
  type = "betabinomial",
  beta_init = rep(0, ncol(X)),
  sigma_init = 1,
  epsilon = 1e-6,
  a = 1,
  b = ncol(X),
  temperature = 0.1
)

set.seed(465)
emvs_5_t1 <- EMVS(
  X = X,
  Y = y,
  #v0 = 0.01,
  v0 = v0_try,
  v1 = 1,
  type = "betabinomial",
  beta_init = rep(0, ncol(X)),
  sigma_init = 1,
  epsilon = 1e-6,
  a = 1,
  b = ncol(X),
  temperature = 1
)

set.seed(465)
emvs_5_t2 <- EMVS(
  X = X,
  Y = y,
  #v0 = 0.01,
  v0 = v0_try,
  v1 = 1,
  type = "betabinomial",
  beta_init = rep(0, ncol(X)),
  sigma_init = 1,
  epsilon = 1e-6,
  a = 1,
  b = ncol(X),
  temperature = 2
)

set.seed(465)
emvs_5_t3 <- EMVS(
  X = X,
  Y = y,
  #v0 = 0.01,
  v0 = v0_try,
  v1 = 1,
  type = "betabinomial",
  beta_init = rep(0, ncol(X)),
  sigma_init = 1,
  epsilon = 1e-6,
  a = 1,
  b = ncol(X),
  temperature = 5
)

set.seed(465)
emvs_5_t4 <- EMVS(
  X = X,
  Y = y,
  #v0 = 0.01,
  v0 = v0_try,
  v1 = 1,
  type = "betabinomial",
  beta_init = rep(0, ncol(X)),
  sigma_init = 1,
  epsilon = 1e-6,
  a = 1,
  b = ncol(X),
  temperature = 10
)

set.seed(465)
emvs_5_t5 <- EMVS(
  X = X,
  Y = y,
  #v0 = 0.01,
  v0 = v0_try,
  v1 = 1,
  type = "betabinomial",
  beta_init = rep(0, ncol(X)),
  sigma_init = 1,
  epsilon = 1e-6,
  a = 1,
  b = ncol(X),
  temperature = 100
)


emvs_5_t0_error <- Inf
emvs_5_t0_idx <- NA
emvs_5_t0_thresh <- NA
for(i in 1:6){
  temp <- find_optimal_threshold(emvs_5_t0, i, true_betas_5, N_5, P_5)
  if(temp$error < emvs_5_t0_error){
    emvs_5_t0_error <- temp$error
    emvs_5_t0_idx <- i
    emvs_5_t0_thresh <- temp$threshold
  }
}
emvs_5_t0_error
emvs_5_t0_idx
emvs_5_t0_thresh


emvs_5_t1_error <- Inf
emvs_5_t1_idx <- NA
emvs_5_t1_thresh <- NA
for(i in 1:6){
  temp <- find_optimal_threshold(emvs_5_t1, i, true_betas_5, N_5, P_5)
  if(temp$error < emvs_5_t1_error){
    emvs_5_t1_error <- temp$error
    emvs_5_t1_idx <- i
    emvs_5_t1_thresh <- temp$threshold
  }
}
emvs_5_t1_error
emvs_5_t1_idx
emvs_5_t1_thresh


emvs_5_t2_error <- Inf
emvs_5_t2_idx <- NA
emvs_5_t2_thresh <- NA
for(i in 1:6){
  temp <- find_optimal_threshold(emvs_5_t2, i, true_betas_5, N_5, P_5)
  if(temp$error < emvs_5_t2_error){
    emvs_5_t2_error <- temp$error
    emvs_5_t2_idx <- i
    emvs_5_t2_thresh <- temp$threshold
  }
}
emvs_5_t2_error
emvs_5_t2_idx
emvs_5_t2_thresh


emvs_5_t3_error <- Inf
emvs_5_t3_idx <- NA
emvs_5_t3_thresh <- NA
for(i in 1:6){
  temp <- find_optimal_threshold(emvs_5_t3, i, true_betas_5, N_5, P_5)
  if(temp$error < emvs_5_t3_error){
    emvs_5_t3_error <- temp$error
    emvs_5_t3_idx <- i
    emvs_5_t3_thresh <- temp$threshold
  }
}
emvs_5_t3_error
emvs_5_t3_idx
emvs_5_t3_thresh


emvs_5_t4_error <- Inf
emvs_5_t4_idx <- NA
emvs_5_t4_thresh <- NA
for(i in 1:6){
  temp <- find_optimal_threshold(emvs_5_t4, i, true_betas_5, N_5, P_5)
  if(temp$error < emvs_5_t4_error){
    emvs_5_t4_error <- temp$error
    emvs_5_t4_idx <- i
    emvs_5_t4_thresh <- temp$threshold
  }
}
emvs_5_t4_error
emvs_5_t4_idx
emvs_5_t4_thresh


emvs_5_t5_error <- Inf
emvs_5_t5_idx <- NA
emvs_5_t5_thresh <- NA
for(i in 1:6){
  temp <- find_optimal_threshold(emvs_5_t5, i, true_betas_5, N_5, P_5)
  if(temp$error < emvs_5_t5_error){
    emvs_5_t5_error <- temp$error
    emvs_5_t5_idx <- i
    emvs_5_t5_thresh <- temp$threshold
  }
}
emvs_5_t5_error
emvs_5_t5_idx
emvs_5_t5_thresh


emvs_5_t0_thresholded <- as.integer(emvs_5_t0$prob_inclusion[emvs_5_t0_idx,] > emvs_5_t0_thresh)
emvs_5_t0_thresh_cm <- table(true_betas_5, emvs_5_t0_thresholded)

emvs_5_t1_thresholded <- as.integer(emvs_5_t1$prob_inclusion[emvs_5_t1_idx,] > emvs_5_t1_thresh)
emvs_5_t1_thresh_cm <- table(true_betas_5, emvs_5_t1_thresholded)

emvs_5_t2_thresholded <- as.integer(emvs_5_t2$prob_inclusion[emvs_5_t2_idx,] > emvs_5_t2_thresh)
emvs_5_t2_thresh_cm <- table(true_betas_5, emvs_5_t2_thresholded)

emvs_5_t3_thresholded <- as.integer(emvs_5_t3$prob_inclusion[emvs_5_t3_idx,] > emvs_5_t3_thresh)
emvs_5_t3_thresh_cm <- table(true_betas_5, emvs_5_t3_thresholded)

emvs_5_t4_thresholded <- as.integer(emvs_5_t4$prob_inclusion[emvs_5_t4_idx,] > emvs_5_t4_thresh)
emvs_5_t4_thresh_cm <- table(true_betas_5, emvs_5_t4_thresholded)

emvs_5_t5_thresholded <- as.integer(emvs_5_t5$prob_inclusion[emvs_5_t5_idx,] > emvs_5_t5_thresh)
emvs_5_t5_thresh_cm <- table(true_betas_5, emvs_5_t5_thresholded)

emvs_5_t0_thresh_cm
emvs_5_t1_thresh_cm
emvs_5_t2_thresh_cm
emvs_5_t3_thresh_cm
emvs_5_t4_thresh_cm
emvs_5_t5_thresh_cm

emvs_5_t0_est <- emvs_5_t0$betas[1,]
emvs_5_t1_est <- emvs_5_t1$betas[1,]
emvs_5_t2_est <- emvs_5_t2$betas[1,]
emvs_5_t3_est <- emvs_5_t3$betas[1,]
emvs_5_t4_est <- emvs_5_t4$betas[1,]
emvs_5_t5_est <- emvs_5_t5$betas[1,]

# Signal coefficients
mean((emvs_5_t0_est[which(true_betas_5 == 1)] - beta_vals_5[which(true_betas_5 == 1)])^2)
mean((emvs_5_t1_est[which(true_betas_5 == 1)] - beta_vals_5[which(true_betas_5 == 1)])^2)
mean((emvs_5_t2_est[which(true_betas_5 == 1)] - beta_vals_5[which(true_betas_5 == 1)])^2)
mean((emvs_5_t3_est[which(true_betas_5 == 1)] - beta_vals_5[which(true_betas_5 == 1)])^2)
mean((emvs_5_t4_est[which(true_betas_5 == 1)] - beta_vals_5[which(true_betas_5 == 1)])^2)
mean((emvs_5_t5_est[which(true_betas_5 == 1)] - beta_vals_5[which(true_betas_5 == 1)])^2)
# Minimized at t = 1

# All coefficients
mean((emvs_5_t0_est - beta_vals_5)^2)
mean((emvs_5_t1_est - beta_vals_5)^2)
mean((emvs_5_t2_est - beta_vals_5)^2)
mean((emvs_5_t3_est - beta_vals_5)^2)
mean((emvs_5_t4_est - beta_vals_5)^2)
mean((emvs_5_t5_est - beta_vals_5)^2)
# Technically minimized at t = 10, but very indiscernible except for t = 100

# Train MSE
mean(((X_5 %*% as.matrix(c(-1, emvs_5_t0_est))) - y_5)^2)
mean(((X_5 %*% as.matrix(c(-1, emvs_5_t1_est))) - y_5)^2)
mean(((X_5 %*% as.matrix(c(-1, emvs_5_t2_est))) - y_5)^2)
mean(((X_5 %*% as.matrix(c(-1, emvs_5_t3_est))) - y_5)^2)
mean(((X_5 %*% as.matrix(c(-1, emvs_5_t4_est))) - y_5)^2)
mean(((X_5 %*% as.matrix(c(-1, emvs_5_t5_est))) - y_5)^2)
# Minimized at low temperatures

# Test MSE
mean(((X_5_test %*% as.matrix(c(-1, emvs_5_t0_est))) - y_5_test)^2)
mean(((X_5_test %*% as.matrix(c(-1, emvs_5_t1_est))) - y_5_test)^2)
mean(((X_5_test %*% as.matrix(c(-1, emvs_5_t2_est))) - y_5_test)^2)
mean(((X_5_test %*% as.matrix(c(-1, emvs_5_t3_est))) - y_5_test)^2)
mean(((X_5_test %*% as.matrix(c(-1, emvs_5_t4_est))) - y_5_test)^2)
mean(((X_5_test %*% as.matrix(c(-1, emvs_5_t5_est))) - y_5_test)^2)
# Minimized at t = 10
# With this, it outperforms other models
```




# Prior Methods

## SSVS

### Model 0

```{r}
#| label: SSVS 0

y <- model_0$y
X <- model_0$X
data <- data.frame(y = y, X[,-1])

set.seed(465)
start <- Sys.time()
ssvs_0_g <- lm.spike(y ~ .,
                   data = data,
                   niter = N_SAMPLES_2,
                   error.distribution = "gaussian")
print(Sys.time() - start)

# Probability of inclusion
colMeans(ssvs_0_g$beta[BURN_IN:N_SAMPLES_2,-1] != 0)

# Coefficient estimates
colMeans(ssvs_0_g$beta[BURN_IN:N_SAMPLES_2,-1])
```

Correct and reasonably fast. Gets back to the proper coefficients, which is good.


### Model 1

```{r}
#| label: SSVS 1

y <- model_1$y
X <- model_1$X
data <- data.frame(y = y, X[,-1])

set.seed(465)
start <- Sys.time()
ssvs_1_g <- lm.spike(y ~ .,
                   data = data,
                   niter = N_SAMPLES_2,
                   error.distribution = "gaussian")
print(Sys.time() - start)

# Probability of inclusion
colMeans(ssvs_1_g$beta[BURN_IN:N_SAMPLES_2,-1] != 0)

# Coefficient estimates
colMeans(ssvs_1_g$beta[BURN_IN:N_SAMPLES_2,-1])
```

Both seem to miss on some of the variables in our signal, and we aren't super close to the correct coefficients in our estimates.


### Model 2

```{r}
#| label: SSVS 2

y <- model_2$y
X <- model_2$X
data <- data.frame(y = y, X[,-1])

set.seed(465)
ssvs_2_start <- Sys.time()
ssvs_2_g <- lm.spike(y ~ .,
                   data = data,
                   niter = N_SAMPLES_2,
                   error.distribution = "gaussian")
ssvs_2_end <- Sys.time()
print(ssvs_2_end)

save(ssvs_2_g, file = "ssvs_2_g.RData")
ssvs_2_runtime <- c(ssvs_2_start, ssvs_2_end)
save(ssvs_2_runtime, file = "ssvs_2_runtime.RData")

# Probability of inclusion
colMeans(ssvs_2_g$beta[BURN_IN:N_SAMPLES_2,-1] != 0)
which(colMeans(ssvs_2_g$beta[BURN_IN:N_SAMPLES_2,-1] != 0) > 0.5) # Awful job

# Coefficient estimates
colMeans(ssvs_2_g$beta[BURN_IN:N_SAMPLES_2,-1])
```

This does a truly terrible job with the dense signal - it recovers almost nothing as being greater than 0. Could we get it there by tweaking the prior? Probably.


### Model 3

```{r}
#| label: SSVS 3

y <- model_3$y
X <- model_3$X
data <- data.frame(y = y, X[,-1])

set.seed(465)
ssvs_3_start <- Sys.time()
ssvs_3_g <- lm.spike(y ~ .,
                   data = data,
                   niter = N_SAMPLES_2,
                   error.distribution = "gaussian")
ssvs_3_end <- Sys.time()
print(ssvs_3_end)

save(ssvs_3_g, file = "ssvs_3_g.RData")
ssvs_3_runtime <- c(ssvs_3_start, ssvs_3_end)
save(ssvs_3_runtime, file = "ssvs_3_runtime.RData")

# Probability of inclusion
colMeans(ssvs_3_g$beta[BURN_IN:N_SAMPLES_2,-1] != 0)
which(colMeans(ssvs_3_g$beta[BURN_IN:N_SAMPLES_2,-1] != 0) > 0.01) # It doesn't give us many whp, but it definitely IDs the strongest candidates first
# Probably would perform better with a smarter prior specification

# Coefficient estimates
colMeans(ssvs_3_g$beta[BURN_IN:N_SAMPLES_2,-1])
```

Not too bad with the sparse model - we would probably want to adjust the prior to have higher inclusion probabilities, because as of right now it's pushing everything pretty close to 0. It does ID some of the correct variables quickly, and it gets a reasonable interpretation of their coefficients. However, it also gives us some false positives.


### Model 4

```{r}
#| label: SSVS 4

y <- model_4$y
X <- model_4$X
data <- data.frame(y = y, X[,-1])

set.seed(465)
ssvs_4_start <- Sys.time()
ssvs_4_g <- lm.spike(y ~ .,
                   data = data,
                   niter = N_SAMPLES_2,
                   error.distribution = "gaussian")
ssvs_4_end <- Sys.time()
print(ssvs_4_end)

save(ssvs_4_g, file = "ssvs_4_g.RData")
ssvs_4_runtime <- c(ssvs_4_start, ssvs_4_end)
save(ssvs_4_runtime, file = "ssvs_4_runtime.RData")

# Probability of inclusion
colMeans(ssvs_4_g$beta[BURN_IN:N_SAMPLES_2,-1] != 0)
which(colMeans(ssvs_4_g$beta[BURN_IN:N_SAMPLES_2,-1] != 0) > 0.01) # Mixed bag - gets some of the correct variables, still mostly the ones with stronger signal.

# Coefficient estimates
colMeans(ssvs_4_g$beta[BURN_IN:N_SAMPLES_2,-1])
```

Does a pretty okay job, although I feel like better performance is certainly possible. We are getting somewhat close to our true coefficients in the true positives, anyway.


## LASSO

### Model 0

```{r}
#| label: lasso 0

y <- model_0$y
X <- model_0$X[,-1]

set.seed(465)
lasso_0 <- blasso(X, y, T = N_SAMPLES, case = "default")

# Probabilities of inclusion
colMeans(lasso_0$beta != 0)
which(colMeans(lasso_0$beta != 0) > 0.5)

# Est. coefficients
colMeans(lasso_0$beta)

# Lambda stays incredibly low
plot(seq(N_SAMPLES - BURN_IN), lasso_0$lambda2[(BURN_IN + 1):N_SAMPLES])
```

Correct, fast, and restores the correct coefficient values.

### Model 1

```{r}
#| label: lasso 1

y <- model_1$y
X <- model_1$X[,-1]

set.seed(465)
lasso_1 <- blasso(X, y, T = N_SAMPLES, case = "default")

# Probabilities of inclusion
colMeans(lasso_1$beta != 0)
which(colMeans(lasso_1$beta != 0) > 0.5)

# Est. coefficients
colMeans(lasso_1$beta)

# Lambda stays very low
plot(seq(N_SAMPLES - BURN_IN), lasso_1$lambda2[(BURN_IN + 1):N_SAMPLES])
```

This almost recovers the correct values - all of the correct variables, but the coefficients are a little off (because they are shrunk towards zero).

### Model 2

```{r}
#| label: lasso 2

y <- model_2$y
X <- model_2$X[,-1]

set.seed(465)
lasso_2_start <- Sys.time()
lasso_2 <- blasso(X, y, T = N_SAMPLES, case = "default")
lasso_2_end <- Sys.time()
print(lasso_2_end)

save(lasso_2, file = "lasso_2.RData")
lasso_2_runtime <- c(lasso_2_start, lasso_2_end)
save(lasso_2_runtime, file = "lasso_2_runtime.RData")

# Probabilities of inclusion
colMeans(lasso_2$beta != 0)
which(colMeans(lasso_2$beta != 0) > 0.5) # Should be the first 75, so we aren't capturing a bunch of them

# Est. coefficients
colMeans(lasso_2$beta) # Most of these are incredibly close to 0


# Lambda mostly stays low
plot(seq(N_SAMPLES - BURN_IN), lasso_2$lambda2[(BURN_IN + 1):N_SAMPLES])

# Model size stays pretty consistent in that 45-50 range
plot(seq(N_SAMPLES - BURN_IN), lasso_2$m[(BURN_IN + 1):N_SAMPLES])
```

This definitely doesn't do a great job - it gives us a lot of false negatives, but also has some false positives.


### Model 3

```{r}
#| label: lasso 3

y <- model_3$y
X <- model_3$X[,-1]

set.seed(465)
lasso_3_start <- Sys.time()
lasso_3 <- blasso(X, y, T = N_SAMPLES, case = "default")
lasso_3_end <- Sys.time()
print(lasso_3_end)

save(lasso_3, file = "lasso_3.RData")
lasso_3_runtime <- c(lasso_3_start, lasso_3_end)
save(lasso_3_runtime, file = "lasso_3_runtime.RData")

# Probabilities of inclusion
colMeans(lasso_3$beta != 0)
which(colMeans(lasso_3$beta != 0) > 0.65) # Not terrible performance, does well with thresholding. However, there are some very persisitent false positives.

# Est. coefficients
colMeans(lasso_3$beta) # Most of these are incredibly close to 0


# Lambda mostly stays low
plot(seq(N_SAMPLES - BURN_IN), lasso_3$lambda2[(BURN_IN + 1):N_SAMPLES])

# Model size stays pretty consistent in that 45-50 range
plot(seq(N_SAMPLES - BURN_IN), lasso_3$m[(BURN_IN + 1):N_SAMPLES])
```

LASSO performs well here - you have to threshold the probability of inclusion higher, but once you do that, the algorithm has strong performance. Still picking up on some false positives, and we are missing our coefficient estimates.

### Model 4

```{r}
#| label: lasso 4

y <- model_4$y
X <- model_4$X[,-1]

set.seed(465)
lasso_4_start <- Sys.time()
lasso_4 <- blasso(X, y, T = N_SAMPLES, case = "default")
lasso_4_end <- Sys.time()
print(lasso_4_end)

save(lasso_4, file = "lasso_4.RData")
lasso_4_runtime <- c(lasso_4_start, lasso_4_end)
save(lasso_4_runtime, file = "lasso_4_runtime.RData")

# Probabilities of inclusion
colMeans(lasso_4$beta != 0)
which(colMeans(lasso_4$beta != 0) > 0.65) # Thresholding it high gets it pretty close, but not perfect

# Est. coefficients
colMeans(lasso_4$beta) # Most of these are incredibly close to 0


# Lambda mostly stays low
plot(seq(N_SAMPLES - BURN_IN), lasso_4$lambda2[(BURN_IN + 1):N_SAMPLES])

# Model size stays pretty consistent in that 50-55 range
plot(seq(N_SAMPLES - BURN_IN), lasso_4$m[(BURN_IN + 1):N_SAMPLES])
```

Actually slightly better performance here than the previous - I think this must just be a factor of the random runs.


## NG

### Model 0

```{r}
#| label: ng 0

y <- model_0$y
X <- model_0$X[,-1]

set.seed(465)
ng_0 <- blasso(X, y, T = N_SAMPLES, case = "ng")

# Probabilities of inclusion
colMeans(ng_0$beta != 0)
which(colMeans(ng_0$beta != 0) > 0.5)

# Est. coefficients
colMeans(ng_0$beta)

# Lambda stays very low
plot(seq(N_SAMPLES - BURN_IN), ng_0$lambda2[(BURN_IN + 1):N_SAMPLES])
```

Very well done, gets the correct variables and super close to correct coefficients.


### Model 1

```{r}
#| label: ng 1

y <- model_1$y
X <- model_1$X[,-1]

set.seed(465)
ng_1 <- blasso(X, y, T = N_SAMPLES, case = "ng")

# Probabilities of inclusion
colMeans(ng_1$beta != 0)
which(colMeans(ng_1$beta != 0) > 0.55) # Needs a little thresholding

# Est. coefficients
colMeans(ng_1$beta)

# Lambda mostly stays low
plot(seq(N_SAMPLES - BURN_IN), ng_1$lambda2[(BURN_IN + 1):N_SAMPLES])
```

Gets the correct variables, but it does miss on the coefficients a good bit.


### Model 2

```{r}
#| label: ng 2

y <- model_2$y
X <- model_2$X[,-1]

set.seed(465)
ng_2_start <- Sys.time()
ng_2 <- blasso(X, y, T = N_SAMPLES, case = "ng")
ng_2_end <- Sys.time()
print(ng_2_end)

save(ng_2, file = "ng_2.RData")
ng_2_runtime <- c(ng_2_start, ng_2_end)
save(ng_2_runtime, file = "ng_2_runtime.RData")

# Probabilities of inclusion
colMeans(ng_2$beta != 0)
which(colMeans(ng_2$beta != 0) > 0.5) # So we're only capturing some of them

# Est. coefficients
colMeans(ng_2$beta)


# Lambda mostly stays low
plot(seq(N_SAMPLES - BURN_IN), ng_2$lambda2[(BURN_IN + 1):N_SAMPLES])

# Model size stays pretty consistent in that 45-50 range
plot(seq(N_SAMPLES - BURN_IN), ng_2$m[(BURN_IN + 1):N_SAMPLES])
```

This definitely doesn't do a great job - it gives us a lot of false negatives, and only captures about half of the true negatives.


### Model 3

```{r}
#| label: ng 3

y <- model_3$y
X <- model_3$X[,-1]

set.seed(465)
ng_3_start <- Sys.time()
ng_3 <- blasso(X, y, T = N_SAMPLES, case = "ng")
ng_3_end <- Sys.time()
print(ng_3_end)

save(ng_3, file = "ng_3.RData")
ng_3_runtime <- c(ng_3_start, ng_3_end)
save(ng_3_runtime, file = "ng_3_runtime.RData")

# Probabilities of inclusion
colMeans(ng_3$beta != 0)
which(colMeans(ng_3$beta != 0) > 0.7) # Not terrible performance, does a better job if you threshold it high

# Est. coefficients
colMeans(ng_3$beta) # Most of these are incredibly close to 0
# Shrinkage still affecting our coefficients


# Lambda mostly stays low
plot(seq(N_SAMPLES - BURN_IN), ng_3$lambda2[(BURN_IN + 1):N_SAMPLES])

# Model size stays pretty consistent in that 45-50 range
plot(seq(N_SAMPLES - BURN_IN), ng_3$m[(BURN_IN + 1):N_SAMPLES])
```

NG performs well here - you have to threshold the probability of inclusion higher, but once you do that, the algorithm has perfect performance. Slightly better than LASSO, but extremely marginal.

### Model 4

```{r}
#| label: ng 4

y <- model_4$y
X <- model_4$X[,-1]

set.seed(465)
ng_4_start <- Sys.time()
ng_4 <- blasso(X, y, T = N_SAMPLES, case = "ng")
ng_4_end <- Sys.time()
print(ng_4_end)

save(ng_4, file = "ng_4.RData")
ng_4_runtime <- c(ng_4_start, ng_4_end)
save(ng_4_runtime, file = "ng_4_runtime.RData")

# Probabilities of inclusion
colMeans(ng_4$beta != 0)
which(colMeans(ng_4$beta != 0) > 0.65) # Thresholding it high gets it pretty close, but not perfect

# Est. coefficients
colMeans(ng_4$beta) # Most of these are incredibly close to 0


# Lambda mostly stays low
plot(seq(N_SAMPLES - BURN_IN), ng_4$lambda2[(BURN_IN + 1):N_SAMPLES])

# Model size stays pretty consistent in that 50-55 range
plot(seq(N_SAMPLES - BURN_IN), ng_4$m[(BURN_IN + 1):N_SAMPLES])
```

Does pretty well - maybe slightly better than with $n > p$ in the previous example, though again these are close. Still having some issues with the true pos/neg, but at least the coefficients are pretty close here.


## Horseshoe

### Model 0

```{r}
#| label: hs 0

y <- model_0$y
X <- model_0$X[,-1]

set.seed(465)
hs_0 <- blasso(X, y, T = N_SAMPLES, case = "hs")

# Probabilities of inclusion
colMeans(hs_0$beta != 0)
which(colMeans(hs_0$beta != 0) > 0.5)

# Est. coefficients
colMeans(hs_0$beta)

# Lambda stays very low
plot(seq(N_SAMPLES - BURN_IN), hs_0$lambda2[(BURN_IN + 1):N_SAMPLES])
```

Very well done, gets the correct variables and super close to correct coefficients.


### Model 1

```{r}
#| label: hs 1

y <- model_1$y
X <- model_1$X[,-1]

set.seed(465)
hs_1 <- blasso(X, y, T = N_SAMPLES, case = "hs")

# Probabilities of inclusion
colMeans(hs_1$beta != 0)
which(colMeans(ng_1$beta != 0) > 0.55) # Needs a little thresholding

# Est. coefficients
colMeans(hs_1$beta)

# Lambda mostly stays low
plot(seq(N_SAMPLES - BURN_IN), hs_1$lambda2[(BURN_IN + 1):N_SAMPLES])
```

Gets the correct variables, but it does miss on the coefficients a good bit.


### Model 2

```{r}
#| label: hs 2

y <- model_2$y
X <- model_2$X[,-1]

set.seed(465)
hs_2_start <- Sys.time()
hs_2 <- blasso(X, y, T = N_SAMPLES, case = "hs")
hs_2_end <- Sys.time()
print(hs_2_end)

save(hs_2, file = "hs_2.RData")
hs_2_runtime <- c(hs_2_start, hs_2_end)
save(hs_2_runtime, file = "hs_2_runtime.RData")

# Probabilities of inclusion
colMeans(hs_2$beta != 0)
which(colMeans(hs_2$beta != 0) > 0.5) # So we're only capturing some of them

# Est. coefficients
colMeans(hs_2$beta)


# Lambda mostly stays low
plot(seq(N_SAMPLES - BURN_IN), ng_2$lambda2[(BURN_IN + 1):N_SAMPLES])

# Model size stays pretty consistent in that 45-50 range
plot(seq(N_SAMPLES - BURN_IN), hs_2$m[(BURN_IN + 1):N_SAMPLES])
```

Once again, not doing well on dense signals.


### Model 3

```{r}
#| label: hs 3

y <- model_3$y
X <- model_3$X[,-1]

set.seed(465)
hs_3_start <- Sys.time()
hs_3 <- blasso(X, y, T = N_SAMPLES, case = "hs")
hs_3_end <- Sys.time()
print(hs_3_end)

save(hs_3, file = "hs_3.RData")
hs_3_runtime <- c(hs_3_start, hs_3_end)
save(hs_3_runtime, file = "hs_3_runtime.RData")

# Probabilities of inclusion
colMeans(hs_3$beta != 0)
which(colMeans(hs_3$beta != 0) > 0.65) # Not terrible performance, does a better job if you threshold it high

# Est. coefficients
colMeans(hs_3$beta) # Not recovering coefficients super well


# Lambda mostly stays low, looccasional spikes
plot(seq(N_SAMPLES - BURN_IN), hs_3$lambda2[(BURN_IN + 1):N_SAMPLES])

# Model size stays pretty consistent in that 45-50 range
plot(seq(N_SAMPLES - BURN_IN), hs_3$m[(BURN_IN + 1):N_SAMPLES])
```

NG performs well here - you have to threshold the probability of inclusion higher, but once you do that, the algorithm has perfect performance. Slightly better than LASSO, but extremely marginal.

### Model 4

```{r}
#| label: hs 4

y <- model_4$y
X <- model_4$X[,-1]

set.seed(465)
hs_4_start <- Sys.time()
hs_4 <- blasso(X, y, T = N_SAMPLES, case = "hs")
hs_4_end <- Sys.time()
print(hs_4_end)

save(hs_4, file = "hs_4.RData")
hs_4_runtime <- c(hs_4_start, hs_4_end)
save(hs_4_runtime, file = "hs_4_runtime.RData")

# Probabilities of inclusion
colMeans(hs_4$beta != 0)
which(colMeans(hs_4$beta != 0) > 0.7) # Thresholding it high gets it pretty close, but not perfect. Worse performance

# Est. coefficients
colMeans(hs_4$beta) # Closer to accurate


# Lambda mostly stays low
plot(seq(N_SAMPLES - BURN_IN), hs_4$lambda2[(BURN_IN + 1):N_SAMPLES])

# Model size stays pretty consistent in that 50-55 range
plot(seq(N_SAMPLES - BURN_IN), hs_4$m[(BURN_IN + 1):N_SAMPLES])
```

Does alright, though definitely worse than the $n > p$ in the previous example. Captures fewer true positives, but can be thresholded to remove false positives pretty effectively. For the coefficients that it captures correctly, it does a better job of getting the right coefficients (or close to).




# Performance Summary

## Runtime (Order of Magnitude)

### Non-Prior Methods

#### Subset Selection (BBA)

Order of hours.

#### Bayesian Model Selection

Order of seconds-minutes.

#### Bayesian Model Averaging

Order of seconds.

#### EMVS

Order of fractional seconds.

### Prior Methods

#### SSVS

Order of seconds-minutes for normally distributed errors, add in another order of magnitude for student-distributed errors.

#### LASSO

Order of minutes-10s of minutes.

#### NG

Order of minutes-10s of minutes.

#### Horseshoe

Order of minutes-10s of minutes.



## Thresholding

```{r}
#| label: helper function

find_optimal_threshold <- function(emvs, idx, true_betas, N, P){
  roc_obj <- roc(true_betas, emvs$prob_inclusion[idx,])
  tpr <- roc_obj$sensitivities
  fpr <- 1 - roc_obj$specificities
  error <- (fpr * N + (1 - tpr) * P) / (P + N)
  threshold <- roc_obj$thresholds[which.min(error)]
  return(list(threshold = threshold, error = error[which.min(error)]))
}
```


### Model 2

```{r}
#| label: thresholding model 2

true_betas_2 <- model_2$beta_true[-1]
true_betas_2 <- as.integer(true_betas_2 != 0)

N_2 <- sum(true_betas_2 == 0)
P_2 <- sum(true_betas_2 == 1)

# BMA
bma_2_roc <- roc(true_betas_2,
                 estimates.bma(bma_2, order.by.pip = FALSE)[,1])
plot(bma_2_roc)
bma_2_tpr <- bma_2_roc$sensitivities
bma_2_fpr <- 1 - bma_2_roc$specificities
bma_2_error <- (bma_2_fpr * N_2 + (1 - bma_2_tpr) * P_2) / (P_2 + N_2)
bma_2_threshold <- bma_2_roc$thresholds[which.min(bma_2_error)]
bma_2_threshold

# SSVS
ssvs_2_roc <- roc(true_betas_2,
                  colMeans(ssvs_2_g$beta[BURN_IN:N_SAMPLES_2,-1] != 0))
plot(ssvs_2_roc)
ssvs_2_tpr <- ssvs_2_roc$sensitivities
ssvs_2_fpr <- 1 - ssvs_2_roc$specificities
ssvs_2_error <- (ssvs_2_fpr * N_2 + (1 - ssvs_2_tpr) * P_2) / (P_2 + N_2)
ssvs_2_threshold <- ssvs_2_roc$thresholds[which.min(ssvs_2_error)]
ssvs_2_threshold

# LASSO
lasso_2_roc <- roc(true_betas_2,
                  colMeans(lasso_2$beta[(BURN_IN + 1):N_SAMPLES,] != 0))
plot(lasso_2_roc)
lasso_2_tpr <- lasso_2_roc$sensitivities
lasso_2_fpr <- 1 - lasso_2_roc$specificities
lasso_2_error <- (lasso_2_fpr * N_2 + (1 - lasso_2_tpr) * P_2) / (P_2 + N_2)
lasso_2_threshold <- lasso_2_roc$thresholds[which.min(lasso_2_error)]
lasso_2_threshold

# NG
ng_2_roc <- roc(true_betas_2,
                  colMeans(ng_2$beta[(BURN_IN + 1):N_SAMPLES,] != 0))
plot(ng_2_roc)
ng_2_tpr <- ng_2_roc$sensitivities
ng_2_fpr <- 1 - ng_2_roc$specificities
ng_2_error <- (ng_2_fpr * N_2 + (1 - ng_2_tpr) * P_2) / (P_2 + N_2)
ng_2_threshold <- ng_2_roc$thresholds[which.min(ng_2_error)]
ng_2_threshold

# Horseshoe
hs_2_roc <- roc(true_betas_2,
                  colMeans(hs_2$beta[(BURN_IN + 1):N_SAMPLES,] != 0))
plot(hs_2_roc)
hs_2_tpr <- hs_2_roc$sensitivities
hs_2_fpr <- 1 - hs_2_roc$specificities
hs_2_error <- (hs_2_fpr * N_2 + (1 - hs_2_tpr) * P_2) / (P_2 + N_2)
hs_2_threshold <- hs_2_roc$thresholds[which.min(hs_2_error)]
hs_2_threshold



# EMVS
emvs_2_error <- Inf
emvs_2_idx <- NA
emvs_2_thresh <- NA
for(i in 1:6){
  temp <- find_optimal_threshold(emvs_2, i, true_betas_2, N_2, P_2)
  if(temp$error < emvs_2_error){
    emvs_2_error <- temp$error
    emvs_2_idx <- i
    emvs_2_thresh <- temp$threshold
  }
}
emvs_2_error
emvs_2_idx
emvs_2_thresh
```

### Model 3

```{r}
#| label: thresholding model 3

true_betas_3 <- model_3$beta_true[-1]
true_betas_3 <- as.integer(true_betas_3 != 0)

N_3 <- sum(true_betas_3 == 0)
P_3 <- sum(true_betas_3 == 1)

# BMA
bma_3_roc <- roc(true_betas_3,
                 estimates.bma(bma_3, order.by.pip = FALSE)[,1])
plot(bma_3_roc)
bma_3_tpr <- bma_3_roc$sensitivities
bma_3_fpr <- 1 - bma_3_roc$specificities
bma_3_error <- (bma_3_fpr * N_3 + (1 - bma_3_tpr) * P_3) / (P_3 + N_3)
bma_3_threshold <- bma_3_roc$thresholds[which.min(bma_3_error)]
bma_3_threshold

# SSVS
ssvs_3_roc <- roc(true_betas_3,
                  colMeans(ssvs_3_g$beta[BURN_IN:N_SAMPLES_2,-1] != 0))
plot(ssvs_3_roc)
ssvs_3_tpr <- ssvs_3_roc$sensitivities
ssvs_3_fpr <- 1 - ssvs_3_roc$specificities
ssvs_3_error <- (ssvs_3_fpr * N_3 + (1 - ssvs_3_tpr) * P_3) / (P_3 + N_3)
ssvs_3_threshold <- ssvs_3_roc$thresholds[which.min(ssvs_3_error)]
ssvs_3_threshold

# LASSO
lasso_3_roc <- roc(true_betas_3,
                  colMeans(lasso_3$beta[(BURN_IN + 1):N_SAMPLES,] != 0))
plot(lasso_3_roc)
lasso_3_tpr <- lasso_3_roc$sensitivities
lasso_3_fpr <- 1 - lasso_3_roc$specificities
lasso_3_error <- (lasso_3_fpr * N_3 + (1 - lasso_3_tpr) * P_3) / (P_3 + N_3)
lasso_3_threshold <- lasso_3_roc$thresholds[which.min(lasso_3_error)]
lasso_3_threshold

# NG
ng_3_roc <- roc(true_betas_3,
                  colMeans(ng_3$beta[(BURN_IN + 1):N_SAMPLES,] != 0))
plot(ng_3_roc)
ng_3_tpr <- ng_3_roc$sensitivities
ng_3_fpr <- 1 - ng_3_roc$specificities
ng_3_error <- (ng_3_fpr * N_3 + (1 - ng_3_tpr) * P_3) / (P_3 + N_3)
ng_3_threshold <- ng_3_roc$thresholds[which.min(ng_3_error)]
ng_3_threshold

# Horseshoe
hs_3_roc <- roc(true_betas_3,
                  colMeans(hs_3$beta[(BURN_IN + 1):N_SAMPLES,] != 0))
plot(hs_3_roc)
hs_3_tpr <- hs_3_roc$sensitivities
hs_3_fpr <- 1 - hs_3_roc$specificities
hs_3_error <- (hs_3_fpr * N_3 + (1 - hs_3_tpr) * P_3) / (P_3 + N_3)
hs_3_threshold <- hs_3_roc$thresholds[which.min(hs_3_error)]
hs_3_threshold

# EMVS
emvs_3_error <- Inf
emvs_3_idx <- NA
emvs_3_thresh <- NA
for(i in 1:6){
  temp <- find_optimal_threshold(emvs_3, i, true_betas_3, N_3, P_3)
  if(temp$error < emvs_3_error){
    emvs_3_error <- temp$error
    emvs_3_idx <- i
    emvs_3_thresh <- temp$threshold
  }
}
emvs_3_error
emvs_3_idx
emvs_3_thresh
```

### Model 4

```{r}
#| label: thresholding model 4

true_betas_4 <- model_4$beta_true[-1]
true_betas_4 <- as.integer(true_betas_4 != 0)

N_4 <- sum(true_betas_4 == 0)
P_4 <- sum(true_betas_4 == 1)

# BMA
bma_4_roc <- roc(true_betas_4,
                 estimates.bma(bma_4, order.by.pip = FALSE)[,1])
plot(bma_4_roc)
bma_4_tpr <- bma_4_roc$sensitivities
bma_4_fpr <- 1 - bma_4_roc$specificities
bma_4_error <- (bma_4_fpr * N_4 + (1 - bma_4_tpr) * P_4) / (P_4 + N_4)
bma_4_threshold <- bma_4_roc$thresholds[which.min(bma_4_error)]
bma_4_threshold

# SSVS
ssvs_4_roc <- roc(true_betas_4,
                  colMeans(ssvs_4_g$beta[BURN_IN:N_SAMPLES_2,-1] != 0))
plot(ssvs_4_roc)
ssvs_4_tpr <- ssvs_4_roc$sensitivities
ssvs_4_fpr <- 1 - ssvs_4_roc$specificities
ssvs_4_error <- (ssvs_4_fpr * N_4 + (1 - ssvs_4_tpr) * P_4) / (P_4 + N_4)
ssvs_4_threshold <- ssvs_4_roc$thresholds[which.min(ssvs_4_error)]
ssvs_4_threshold

# LASSO
lasso_4_roc <- roc(true_betas_4,
                  colMeans(lasso_4$beta[(BURN_IN + 1):N_SAMPLES,] != 0))
plot(lasso_4_roc)
lasso_4_tpr <- lasso_4_roc$sensitivities
lasso_4_fpr <- 1 - lasso_4_roc$specificities
lasso_4_error <- (lasso_4_fpr * N_4 + (1 - lasso_4_tpr) * P_4) / (P_4 + N_4)
lasso_4_threshold <- lasso_4_roc$thresholds[which.min(lasso_4_error)]
lasso_4_threshold

# NG
ng_4_roc <- roc(true_betas_4,
                  colMeans(ng_4$beta[(BURN_IN + 1):N_SAMPLES,] != 0))
plot(ng_4_roc)
ng_4_tpr <- ng_4_roc$sensitivities
ng_4_fpr <- 1 - ng_4_roc$specificities
ng_4_error <- (ng_4_fpr * N_4 + (1 - ng_4_tpr) * P_4) / (P_4 + N_4)
ng_4_threshold <- ng_4_roc$thresholds[which.min(ng_4_error)]
ng_4_threshold

# Horseshoe
hs_4_roc <- roc(true_betas_4,
                  colMeans(hs_4$beta[(BURN_IN + 1):N_SAMPLES,] != 0))
plot(hs_4_roc)
hs_4_tpr <- hs_4_roc$sensitivities
hs_4_fpr <- 1 - hs_4_roc$specificities
hs_4_error <- (hs_4_fpr * N_4 + (1 - hs_4_tpr) * P_4) / (P_4 + N_4)
hs_4_threshold <- hs_4_roc$thresholds[which.min(hs_4_error)]
hs_4_threshold

# EMVS
emvs_4_error <- Inf
emvs_4_idx <- NA
emvs_4_thresh <- NA
for(i in 1:6){
  temp <- find_optimal_threshold(emvs_4, i, true_betas_4, N_4, P_4)
  if(temp$error < emvs_4_error){
    emvs_4_error <- temp$error
    emvs_4_idx <- i
    emvs_4_thresh <- temp$threshold
  }
}
emvs_4_error
emvs_4_idx
emvs_4_thresh
```

### Model 5

```{r}
#| label: thresholding model 5

true_betas_5 <- model_5$beta_true[-1]
true_betas_5 <- as.integer(true_betas_5 != 0)

N_5 <- sum(true_betas_5 == 0)
P_5 <- sum(true_betas_5 == 1)

# BMA
bma_5_roc <- roc(true_betas_5,
                 estimates.bma(bma_5, order.by.pip = FALSE)[,1])
plot(bma_5_roc)
bma_5_tpr <- bma_5_roc$sensitivities
bma_5_fpr <- 1 - bma_5_roc$specificities
bma_5_error <- (bma_5_fpr * N_5 + (1 - bma_5_tpr) * P_5) / (P_5 + N_5)
bma_5_threshold <- bma_5_roc$thresholds[which.min(bma_5_error)]
bma_5_threshold

# SSVS
ssvs_5_roc <- roc(true_betas_5,
                  colMeans(ssvs_5_g$beta[BURN_IN:N_SAMPLES,-1] != 0))
plot(ssvs_5_roc)
ssvs_5_tpr <- ssvs_5_roc$sensitivities
ssvs_5_fpr <- 1 - ssvs_5_roc$specificities
ssvs_5_error <- (ssvs_5_fpr * N_5 + (1 - ssvs_5_tpr) * P_5) / (P_5 + N_5)
ssvs_5_threshold <- ssvs_5_roc$thresholds[which.min(ssvs_5_error)]
ssvs_5_threshold

# LASSO
lasso_5_roc <- roc(true_betas_5,
                  colMeans(lasso_5$beta[(BURN_IN + 1):N_SAMPLES,] != 0))
plot(lasso_5_roc)
lasso_5_tpr <- lasso_5_roc$sensitivities
lasso_5_fpr <- 1 - lasso_5_roc$specificities
lasso_5_error <- (lasso_5_fpr * N_5 + (1 - lasso_5_tpr) * P_5) / (P_5 + N_5)
lasso_5_threshold <- lasso_5_roc$thresholds[which.min(lasso_5_error)]
lasso_5_threshold

# NG
ng_5_roc <- roc(true_betas_5,
                  colMeans(ng_5$beta[(BURN_IN + 1):N_SAMPLES,] != 0))
plot(ng_5_roc)
ng_5_tpr <- ng_5_roc$sensitivities
ng_5_fpr <- 1 - ng_5_roc$specificities
ng_5_error <- (ng_5_fpr * N_5 + (1 - ng_5_tpr) * P_5) / (P_5 + N_5)
ng_5_threshold <- ng_5_roc$thresholds[which.min(ng_5_error)]
ng_5_threshold

# Horseshoe
hs_5_roc <- roc(true_betas_5,
                  colMeans(hs_5$beta[(BURN_IN + 1):N_SAMPLES,] != 0))
plot(hs_5_roc)
hs_5_tpr <- hs_5_roc$sensitivities
hs_5_fpr <- 1 - hs_5_roc$specificities
hs_5_error <- (hs_5_fpr * N_5 + (1 - hs_5_tpr) * P_5) / (P_5 + N_5)
hs_5_threshold <- hs_5_roc$thresholds[which.min(hs_5_error)]
hs_5_threshold


# EMVS
emvs_5_error <- Inf
emvs_5_idx <- NA
emvs_5_thresh <- NA
for(i in 1:6){
  temp <- find_optimal_threshold(emvs_5, i, true_betas_5, N_5, P_5)
  if(temp$error < emvs_5_error){
    emvs_5_error <- temp$error
    emvs_5_idx <- i
    emvs_5_thresh <- temp$threshold
  }
}
emvs_5_error
emvs_5_idx
emvs_5_thresh
```




## Recovery of True Coefficients (MSE)

BMS does not actually give us coefficient estimates - it just tells us what coefficients it thinks should be included.

### Model 0


```{r}
#| label: true coefficients model 0

true_betas_0 <- model_0$beta_true[2:11]
true_betas_0 <- as.integer(true_betas_0 != 0)

bma_0_est <- estimates.bma(bma_0, order.by.pip = FALSE)[,2]
emvs_0_est <- emvs_0$betas[1,]
ssvs_0_est <- colMeans(ssvs_0_g$beta[BURN_IN:N_SAMPLES_2,-1])
lasso_0_est <- colMeans(lasso_0$beta[BURN_IN:N_SAMPLES,])
ng_0_est <- colMeans(ng_0$beta[BURN_IN:N_SAMPLES,])
hs_0_est <- colMeans(hs_0$beta[BURN_IN:N_SAMPLES,])

true_betas_0
beta_vals_0 <- model_0$beta_true[-1]

data.frame(true_values = beta_vals_0[which(true_betas_0 == 1)],
           BMA = bma_0_est[which(true_betas_0 == 1)],
           EMVS = emvs_0_est[which(true_betas_0 == 1)],
           SSVS = ssvs_0_est[which(true_betas_0 == 1)],
           Truth = beta_vals_0[which(true_betas_0 == 1)],
           LASSO = lasso_0_est[which(true_betas_0 == 1)],
           Normal.Gamma = ng_0_est[which(true_betas_0 == 1)],
           Horseshoe = hs_0_est[which(true_betas_0 == 1)]) %>% 
  pivot_longer(-true_values, names_to = "Model", values_to = "Estimate") %>% 
  mutate(Model = if_else(Model == "Normal.Gamma", "Normal-Gamma", Model),
         true_values = factor(true_values)) %>% 
  group_by(Model, true_values) %>%
  summarize(Estimate = mean(Estimate)) %>% 
  ungroup() %>% 
  ggplot(aes(x = true_values, y = Estimate, fill = Model)) +
  geom_col(position = "dodge") + 
  theme_bw() +
  labs(x = "True Coefficient", title = "Model 0 Estimates") +
  theme(plot.title = element_text(hjust = 0.5),
        panel.grid.minor.x = element_blank(),
        panel.grid.major.x = element_blank()) +
  scale_fill_viridis(discrete = TRUE, option = "A", direction = -1)

data.frame(true_values = beta_vals_0[which(true_betas_0 == 1)],
           BMA = bma_0_est[which(true_betas_0 == 1)],
           EMVS = emvs_0_est[which(true_betas_0 == 1)],
           SSVS = ssvs_0_est[which(true_betas_0 == 1)],
           LASSO = lasso_0_est[which(true_betas_0 == 1)],
           Normal.Gamma = ng_0_est[which(true_betas_0 == 1)],
           Horseshoe = hs_0_est[which(true_betas_0 == 1)]) %>% 
  pivot_longer(-true_values, names_to = "Model", values_to = "Estimate") %>% 
  mutate(Model = if_else(Model == "Normal.Gamma", "Normal-Gamma", Model)) %>% 
  group_by(Model) %>% 
  summarize(MSE = mean((Estimate - true_values)^2)) %>% 
  ungroup() %>% 
  arrange(MSE) %>% 
  kable(digits = 3)
# Just looking at true coefficients, BMA is the best

data.frame(true_values = beta_vals_0,
           BMA = bma_0_est,
           EMVS = emvs_0_est,
           SSVS = ssvs_0_est,
           LASSO = lasso_0_est,
           Normal.Gamma = ng_0_est,
           Horseshoe = hs_0_est) %>% 
  pivot_longer(-true_values, names_to = "Model", values_to = "Estimate") %>% 
  mutate(Model = if_else(Model == "Normal.Gamma", "Normal-Gamma", Model)) %>% 
  group_by(Model) %>% 
  summarize(MSE = mean((Estimate - true_values)^2)) %>% 
  ungroup() %>% 
  arrange(MSE) %>% 
  kable(digits = 3)
# Looking at all coefficients, BMA is still the best


data.frame(true_values = beta_vals_0,
           BMA = bma_0_est,
           EMVS = emvs_0_est,
           SSVS = ssvs_0_est,
           Truth = beta_vals_0,
           LASSO = lasso_0_est,
           Normal.Gamma = ng_0_est,
           Horseshoe = hs_0_est) %>% 
  pivot_longer(-true_values, names_to = "Model", values_to = "Estimate") %>% 
  mutate(Model = if_else(Model == "Normal.Gamma", "Normal-Gamma", Model),
         true_values = factor(true_values)) %>% 
  group_by(Model, true_values) %>%
  summarize(Estimate = mean(Estimate)) %>% 
  ungroup() %>% 
  ggplot(aes(x = true_values, y = Estimate, fill = Model)) +
  geom_col(position = "dodge") + 
  theme_bw() +
  labs(x = "True Coefficient", title = "Model 0 Estimates") +
  theme(plot.title = element_text(hjust = 0.5),
        panel.grid.minor.x = element_blank(),
        panel.grid.major.x = element_blank()) +
  scale_fill_viridis(discrete = TRUE, option = "A", direction = -1)
```


### Model 1

```{r}
#| label: true coefficients model 1

true_betas_1 <- model_1$beta_true[2:11]
true_betas_1 <- as.integer(true_betas_1 != 0)

bma_1_est <- estimates.bma(bma_1, order.by.pip = FALSE)[,2]
emvs_1_est <- emvs_1$betas[1,]
ssvs_1_est <- colMeans(ssvs_1_g$beta[BURN_IN:N_SAMPLES_2,-1])
lasso_1_est <- colMeans(lasso_1$beta[BURN_IN:N_SAMPLES,])
ng_1_est <- colMeans(ng_1$beta[BURN_IN:N_SAMPLES,])
hs_1_est <- colMeans(hs_1$beta[BURN_IN:N_SAMPLES,])

beta_vals_1 <- model_1$beta_true[-1]

data.frame(true_values = beta_vals_1[which(true_betas_1 == 1)],
           BMA = bma_1_est[which(true_betas_1 == 1)],
           EMVS = emvs_1_est[which(true_betas_1 == 1)],
           Truth = beta_vals_1[which(true_betas_1 == 1)],
           SSVS = ssvs_1_est[which(true_betas_1 == 1)],
           LASSO = lasso_1_est[which(true_betas_1 == 1)],
           Normal.Gamma = ng_1_est[which(true_betas_1 == 1)],
           Horseshoe = hs_1_est[which(true_betas_1 == 1)]) %>% 
  pivot_longer(-true_values, names_to = "Model", values_to = "Estimate") %>% 
  mutate(Model = if_else(Model == "Normal.Gamma", "Normal-Gamma", Model),
         true_values = factor(true_values)) %>% 
  group_by(Model, true_values) %>%
  summarize(Estimate = mean(Estimate)) %>% 
  ungroup() %>% 
  ggplot(aes(x = true_values, y = Estimate, fill = Model)) +
  geom_col(position = "dodge") + 
  theme_bw() +
  labs(x = "True Coefficient", title = "Model 1 Estimates") +
  theme(plot.title = element_text(hjust = 0.5),
        panel.grid.minor.x = element_blank(),
        panel.grid.major.x = element_blank()) +
  scale_fill_viridis(discrete = TRUE, option = "A", direction = -1)
# Everything is having shrinkage issues except for coefficients at -1
# Is this possibly an issue with the intercept?

data.frame(true_values = beta_vals_1[which(true_betas_1 == 1)],
           BMA = bma_1_est[which(true_betas_1 == 1)],
           EMVS = emvs_1_est[which(true_betas_1 == 1)],
           SSVS = ssvs_1_est[which(true_betas_1 == 1)],
           LASSO = lasso_1_est[which(true_betas_1 == 1)],
           Normal.Gamma = ng_1_est[which(true_betas_1 == 1)],
           Horseshoe = hs_1_est[which(true_betas_1 == 1)]) %>% 
  pivot_longer(-true_values, names_to = "Model", values_to = "Estimate") %>% 
  mutate(Model = if_else(Model == "Normal.Gamma", "Normal-Gamma", Model)) %>% 
  group_by(Model) %>% 
  summarize(MSE = mean((Estimate - true_values)^2)) %>% 
  ungroup() %>% 
  arrange(MSE) %>% 
  kable(digits = 3)
# EMVS gets the closest

data.frame(true_values = beta_vals_1,
           BMA = bma_1_est,
           EMVS = emvs_1_est,
           SSVS = ssvs_1_est,
           LASSO = lasso_1_est,
           Normal.Gamma = ng_1_est,
           Horseshoe = hs_1_est) %>% 
  pivot_longer(-true_values, names_to = "Model", values_to = "Estimate") %>% 
  mutate(Model = if_else(Model == "Normal.Gamma", "Normal-Gamma", Model)) %>% 
  group_by(Model) %>% 
  summarize(MSE = mean((Estimate - true_values)^2)) %>% 
  ungroup() %>% 
  arrange(MSE) %>% 
  kable(digits = 3)
# EVMS remains the best

data.frame(true_values = beta_vals_1,
           BMA = bma_1_est,
           EMVS = emvs_1_est,
           SSVS = ssvs_1_est,
           Truth = beta_vals_1,
           LASSO = lasso_1_est,
           Normal.Gamma = ng_1_est,
           Horseshoe = hs_1_est) %>% 
  pivot_longer(-true_values, names_to = "Model", values_to = "Estimate") %>% 
  mutate(Model = if_else(Model == "Normal.Gamma", "Normal-Gamma", Model),
         true_values = factor(true_values)) %>% 
  group_by(Model, true_values) %>%
  summarize(Estimate = mean(Estimate)) %>% 
  ungroup() %>% 
  ggplot(aes(x = true_values, y = Estimate, fill = Model)) +
  geom_col(position = "dodge") + 
  theme_bw() +
  labs(x = "True Coefficient", title = "Model 1 Estimates") +
  theme(plot.title = element_text(hjust = 0.5),
        panel.grid.minor.x = element_blank(),
        panel.grid.major.x = element_blank()) +
  scale_fill_viridis(discrete = TRUE, option = "A", direction = -1)
```

### Model 2

```{r}
#| label: true coefficients model 2

bma_2_est <- estimates.bma(bma_2, order.by.pip = FALSE)[,2]
emvs_2_est <- emvs_2$betas[1,]
ssvs_2_est <- colMeans(ssvs_2_g$beta[BURN_IN:N_SAMPLES_2,-1])
lasso_2_est <- colMeans(lasso_2$beta[BURN_IN:N_SAMPLES,])
ng_2_est <- colMeans(ng_2$beta[BURN_IN:N_SAMPLES,])
hs_2_est <- colMeans(hs_2$beta[BURN_IN:N_SAMPLES,])

beta_vals_2 <- model_2$beta_true[-1]

data.frame(true_values = beta_vals_2[which(true_betas_2 == 1)],
           BMA = bma_2_est[which(true_betas_2 == 1)],
           EMVS = emvs_2_est[which(true_betas_2 == 1)],
           SSVS = ssvs_2_est[which(true_betas_2 == 1)],
           Truth = beta_vals_2[which(true_betas_2 == 1)],
           LASSO = lasso_2_est[which(true_betas_2 == 1)],
           Normal.Gamma = ng_2_est[which(true_betas_2 == 1)],
           Horseshoe = hs_2_est[which(true_betas_2 == 1)]) %>% 
  pivot_longer(-true_values, names_to = "Model", values_to = "Estimate") %>% 
  mutate(Model = if_else(Model == "Normal.Gamma", "Normal-Gamma", Model),
         true_values = factor(true_values)) %>%
  group_by(Model, true_values) %>%
  summarize(Estimate = mean(Estimate)) %>% 
  ungroup() %>% 
  ggplot(aes(x = true_values, y = Estimate, fill = Model)) +
  geom_col(position = "dodge") + 
  theme_bw() +
  labs(x = "True Coefficient", title = "Model 2 Estimates") +
  theme(plot.title = element_text(hjust = 0.5),
        panel.grid.minor.x = element_blank(),
        panel.grid.major.x = element_blank()) +
  scale_fill_viridis(discrete = TRUE, option = "A", direction = -1)

data.frame(true_values = beta_vals_2[which(true_betas_2 == 1)],
           BMA = bma_2_est[which(true_betas_2 == 1)],
           EMVS = emvs_2_est[which(true_betas_2 == 1)],
           SSVS = ssvs_2_est[which(true_betas_2 == 1)],
           LASSO = lasso_2_est[which(true_betas_2 == 1)],
           Normal.Gamma = ng_2_est[which(true_betas_2 == 1)],
           Horseshoe = hs_2_est[which(true_betas_2 == 1)]) %>%
  pivot_longer(-true_values, names_to = "Model", values_to = "Estimate") %>% 
  mutate(Model = if_else(Model == "Normal.Gamma", "Normal-Gamma", Model)) %>% 
  group_by(Model) %>% 
  summarize(MSE = mean((Estimate - true_values)^2)) %>% 
  ungroup() %>% 
  arrange(MSE) %>% 
  kable(digits = 3)
# LASSO gets the closest


data.frame(true_values = beta_vals_2,
           BMA = bma_2_est,
           EMVS = emvs_2_est,
           SSVS = ssvs_2_est,
           LASSO = lasso_2_est,
           Normal.Gamma = ng_2_est,
           Horseshoe = hs_2_est) %>% 
  pivot_longer(-true_values, names_to = "Model", values_to = "Estimate") %>% 
  mutate(Model = if_else(Model == "Normal.Gamma", "Normal-Gamma", Model)) %>% 
  group_by(Model) %>% 
  summarize(MSE = mean((Estimate - true_values)^2)) %>% 
  ungroup() %>% 
  arrange(MSE) %>% 
  kable(digits = 3)
# LASSO remains the best

data.frame(true_values = beta_vals_2,
           BMA = bma_2_est,
           EMVS = emvs_2_est,
           SSVS = ssvs_2_est,
           Truth = beta_vals_2,
           LASSO = lasso_2_est,
           Normal.Gamma = ng_2_est,
           Horseshoe = hs_2_est) %>% 
  pivot_longer(-true_values, names_to = "Model", values_to = "Estimate") %>% 
  mutate(Model = if_else(Model == "Normal.Gamma", "Normal-Gamma", Model),
         true_values = factor(true_values)) %>%
  group_by(Model, true_values) %>%
  summarize(Estimate = mean(Estimate)) %>% 
  ungroup() %>% 
  ggplot(aes(x = true_values, y = Estimate, fill = Model)) +
  geom_col(position = "dodge") + 
  theme_bw() +
  labs(x = "True Coefficient", title = "Model 2 Estimates") +
  theme(plot.title = element_text(hjust = 0.5),
        panel.grid.minor.x = element_blank(),
        panel.grid.major.x = element_blank()) +
  scale_fill_viridis(discrete = TRUE, option = "A", direction = -1)
```

### Model 3

```{r}
#| label: true coefficients model 3

bma_3_est <- estimates.bma(bma_3, order.by.pip = FALSE)[,2]
emvs_3_est <- emvs_3$betas[1,]
ssvs_3_est <- colMeans(ssvs_3_g$beta[BURN_IN:N_SAMPLES_2,-1])
lasso_3_est <- colMeans(lasso_3$beta[BURN_IN:N_SAMPLES,])
ng_3_est <- colMeans(ng_3$beta[BURN_IN:N_SAMPLES,])
hs_3_est <- colMeans(hs_3$beta[BURN_IN:N_SAMPLES,])

beta_vals_3 <- model_3$beta_true[-1]

data.frame(true_values = beta_vals_3[which(true_betas_3 == 1)],
           BMA = bma_3_est[which(true_betas_3 == 1)],
           EMVS = emvs_3_est[which(true_betas_3 == 1)],
           Truth = beta_vals_3[which(true_betas_3 == 1)],
           SSVS = ssvs_3_est[which(true_betas_3 == 1)],
           LASSO = lasso_3_est[which(true_betas_3 == 1)],
           Normal.Gamma = ng_3_est[which(true_betas_3 == 1)],
           Horseshoe = hs_3_est[which(true_betas_3 == 1)]) %>% 
  pivot_longer(-true_values, names_to = "Model", values_to = "Estimate") %>% 
  mutate(Model = if_else(Model == "Normal.Gamma", "Normal-Gamma", Model),
         true_values = factor(true_values)) %>% 
  group_by(Model, true_values) %>%
  summarize(Estimate = mean(Estimate)) %>% 
  ungroup() %>% 
  ggplot(aes(x = true_values, y = Estimate, fill = Model)) +
  geom_col(position = "dodge") + 
  theme_bw() +
  labs(x = "True Coefficient", title = "Model 3 Estimates") +
  theme(plot.title = element_text(hjust = 0.5),
        panel.grid.minor.x = element_blank(),
        panel.grid.major.x = element_blank()) +
  scale_fill_viridis(discrete = TRUE, option = "A", direction = -1)

data.frame(true_values = beta_vals_3[which(true_betas_3 == 1)],
           BMA = bma_3_est[which(true_betas_3 == 1)],
           EMVS = emvs_3_est[which(true_betas_3 == 1)],
           SSVS = ssvs_3_est[which(true_betas_3 == 1)],
           LASSO = lasso_3_est[which(true_betas_3 == 1)],
           Normal.Gamma = ng_3_est[which(true_betas_3 == 1)],
           Horseshoe = hs_3_est[which(true_betas_3 == 1)]) %>% 
  pivot_longer(-true_values, names_to = "Model", values_to = "Estimate") %>% 
  mutate(Model = if_else(Model == "Normal.Gamma", "Normal-Gamma", Model)) %>% 
  group_by(Model) %>% 
  summarize(MSE = mean((Estimate - true_values)^2)) %>% 
  ungroup() %>% 
  arrange(MSE) %>% 
  kable(digits = 3)
# EMVS gets the closest


data.frame(true_values = beta_vals_3,
           BMA = bma_3_est,
           EMVS = emvs_3_est,
           SSVS = ssvs_3_est,
           LASSO = lasso_3_est,
           Normal.Gamma = ng_3_est,
           Horseshoe = hs_3_est) %>% 
  pivot_longer(-true_values, names_to = "Model", values_to = "Estimate") %>% 
  mutate(Model = if_else(Model == "Normal.Gamma", "Normal-Gamma", Model)) %>% 
  group_by(Model) %>% 
  summarize(MSE = mean((Estimate - true_values)^2)) %>% 
  ungroup() %>% 
  arrange(MSE) %>% 
  kable(digits = 3)
# Horseshoe is the best when we include everything


data.frame(true_values = beta_vals_3,
           BMA = bma_3_est,
           EMVS = emvs_3_est,
           SSVS = ssvs_3_est,
           Truth = beta_vals_3,
           LASSO = lasso_3_est,
           Normal.Gamma = ng_3_est,
           Horseshoe = hs_3_est) %>% 
  pivot_longer(-true_values, names_to = "Model", values_to = "Estimate") %>% 
  mutate(Model = if_else(Model == "Normal.Gamma", "Normal-Gamma", Model),
         true_values = factor(true_values)) %>% 
  group_by(Model, true_values) %>%
  summarize(Estimate = mean(Estimate)) %>% 
  ungroup() %>% 
  ggplot(aes(x = true_values, y = Estimate, fill = Model)) +
  geom_col(position = "dodge") + 
  theme_bw() +
  labs(x = "True Coefficient", title = "Model 3 Estimates") +
  theme(plot.title = element_text(hjust = 0.5),
        panel.grid.minor.x = element_blank(),
        panel.grid.major.x = element_blank()) +
  scale_fill_viridis(discrete = TRUE, option = "A", direction = -1)
```

### Model 4

```{r}
#| label: true coefficients model 4

bma_4_est <- estimates.bma(bma_4, order.by.pip = FALSE)[,2]
emvs_4_est <- emvs_4$betas[1,]
ssvs_4_est <- colMeans(ssvs_4_g$beta[BURN_IN:N_SAMPLES_2,-1])
lasso_4_est <- colMeans(lasso_4$beta[BURN_IN:N_SAMPLES,])
ng_4_est <- colMeans(ng_4$beta[BURN_IN:N_SAMPLES,])
hs_4_est <- colMeans(hs_4$beta[BURN_IN:N_SAMPLES,])

beta_vals_4 <- model_4$beta_true[-1]

data.frame(true_values = beta_vals_4[which(true_betas_4 == 1)],
           BMA = bma_4_est[which(true_betas_4 == 1)],
           EMVS = emvs_4_est[which(true_betas_4 == 1)],
           Truth = beta_vals_4[which(true_betas_4 == 1)],
           SSVS = ssvs_4_est[which(true_betas_4 == 1)],
           LASSO = lasso_4_est[which(true_betas_4 == 1)],
           Normal.Gamma = ng_4_est[which(true_betas_4 == 1)],
           Horseshoe = hs_4_est[which(true_betas_4 == 1)]) %>% 
  pivot_longer(-true_values, names_to = "Model", values_to = "Estimate") %>% 
  mutate(Model = if_else(Model == "Normal.Gamma", "Normal-Gamma", Model),
         true_values = factor(true_values)) %>% 
  group_by(Model, true_values) %>%
  summarize(Estimate = mean(Estimate)) %>% 
  ungroup() %>% 
  ggplot(aes(x = true_values, y = Estimate, fill = Model)) +
  geom_col(position = "dodge") + 
  theme_bw() +
  labs(x = "True Coefficient", title = "Model 4 Estimates") +
  theme(plot.title = element_text(hjust = 0.5),
        panel.grid.minor.x = element_blank(),
        panel.grid.major.x = element_blank()) +
  scale_fill_viridis(discrete = TRUE, option = "A", direction = -1)

data.frame(true_values = beta_vals_4[which(true_betas_4 == 1)],
           BMA = bma_4_est[which(true_betas_4 == 1)],
           EMVS = emvs_4_est[which(true_betas_4 == 1)],
           SSVS = ssvs_4_est[which(true_betas_4 == 1)],
           LASSO = lasso_4_est[which(true_betas_4 == 1)],
           Normal.Gamma = ng_4_est[which(true_betas_4 == 1)],
           Horseshoe = hs_4_est[which(true_betas_4 == 1)]) %>% 
  pivot_longer(-true_values, names_to = "Model", values_to = "Estimate") %>% 
  mutate(Model = if_else(Model == "Normal.Gamma", "Normal-Gamma", Model)) %>% 
  group_by(Model) %>% 
  summarize(MSE = mean((Estimate - true_values)^2)) %>% 
  ungroup() %>% 
  arrange(MSE) %>% 
  kable(digits = 3)
# EMVS gets the closest


data.frame(true_values = beta_vals_4,
           BMA = bma_4_est,
           EMVS = emvs_4_est,
           SSVS = ssvs_4_est,
           LASSO = lasso_4_est,
           Normal.Gamma = ng_4_est,
           Horseshoe = hs_4_est) %>% 
  pivot_longer(-true_values, names_to = "Model", values_to = "Estimate") %>% 
  mutate(Model = if_else(Model == "Normal.Gamma", "Normal-Gamma", Model)) %>% 
  group_by(Model) %>% 
  summarize(MSE = mean((Estimate - true_values)^2)) %>% 
  ungroup() %>% 
  arrange(MSE) %>% 
  kable(digits = 3)
# Horseshoe is the best when we include everything


data.frame(true_values = beta_vals_4,
           BMA = bma_4_est,
           EMVS = emvs_4_est,
           SSVS = ssvs_4_est,
           Truth = beta_vals_4,
           LASSO = lasso_4_est,
           Normal.Gamma = ng_4_est,
           Horseshoe = hs_4_est) %>% 
  pivot_longer(-true_values, names_to = "Model", values_to = "Estimate") %>% 
  mutate(Model = if_else(Model == "Normal.Gamma", "Normal-Gamma", Model),
         true_values = factor(true_values)) %>% 
  group_by(Model, true_values) %>%
  summarize(Estimate = mean(Estimate)) %>% 
  ungroup() %>% 
  ggplot(aes(x = true_values, y = Estimate, fill = Model)) +
  geom_col(position = "dodge") + 
  theme_bw() +
  labs(x = "True Coefficient", title = "Model 4 Estimates") +
  theme(plot.title = element_text(hjust = 0.5),
        panel.grid.minor.x = element_blank(),
        panel.grid.major.x = element_blank()) +
  scale_fill_viridis(discrete = TRUE, option = "A", direction = -1)
```

### Model 5

```{r}
#| label: true coefficients model 5

bma_5_est <- estimates.bma(bma_5, order.by.pip = FALSE)[,2]
emvs_5_est <- emvs_5$betas[1,]
ssvs_5_est <- colMeans(ssvs_5_g$beta[BURN_IN:N_SAMPLES,-1])
lasso_5_est <- colMeans(lasso_5$beta[BURN_IN:N_SAMPLES,])
ng_5_est <- colMeans(ng_5$beta[BURN_IN:N_SAMPLES,])
hs_5_est <- colMeans(hs_5$beta[BURN_IN:N_SAMPLES,])

beta_vals_5 <- model_5$beta_true[-1]

data.frame(true_values = beta_vals_5[which(true_betas_5 == 1)],
           BMA = bma_5_est[which(true_betas_5 == 1)],
           EMVS = emvs_5_est[which(true_betas_5 == 1)],
           Truth = beta_vals_5[which(true_betas_5 == 1)],
           SSVS = ssvs_5_est[which(true_betas_5 == 1)],
           LASSO = lasso_5_est[which(true_betas_5 == 1)],
           Normal.Gamma = ng_5_est[which(true_betas_5 == 1)],
           Horseshoe = hs_5_est[which(true_betas_5 == 1)]) %>% 
  pivot_longer(-true_values, names_to = "Model", values_to = "Estimate") %>% 
  mutate(Model = if_else(Model == "Normal.Gamma", "Normal-Gamma", Model),
         true_values = factor(true_values)) %>% 
  group_by(Model, true_values) %>%
  summarize(Estimate = mean(Estimate)) %>% 
  ungroup() %>% 
  ggplot(aes(x = true_values, y = Estimate, fill = Model)) +
  geom_col(position = "dodge") + 
  theme_bw() +
  labs(x = "True Coefficient", title = "Model 5 Estimates") +
  theme(plot.title = element_text(hjust = 0.5),
        panel.grid.minor.x = element_blank(),
        panel.grid.major.x = element_blank()) +
  scale_fill_viridis(discrete = TRUE, option = "A", direction = -1)

data.frame(true_values = beta_vals_5[which(true_betas_5 == 1)],
           BMA = bma_5_est[which(true_betas_5 == 1)],
           EMVS = emvs_5_est[which(true_betas_5 == 1)],
           SSVS = ssvs_5_est[which(true_betas_5 == 1)],
           LASSO = lasso_5_est[which(true_betas_5 == 1)],
           Normal.Gamma = ng_5_est[which(true_betas_5 == 1)],
           Horseshoe = hs_5_est[which(true_betas_5 == 1)]) %>% 
  pivot_longer(-true_values, names_to = "Model", values_to = "Estimate") %>% 
  mutate(Model = if_else(Model == "Normal.Gamma", "Normal-Gamma", Model)) %>% 
  group_by(Model) %>% 
  summarize(MSE = mean((Estimate - true_values)^2)) %>% 
  ungroup() %>% 
  arrange(MSE) %>% 
  kable(digits = 3)
# EMVS gets the closest


data.frame(true_values = beta_vals_5,
           BMA = bma_5_est,
           EMVS = emvs_5_est,
           SSVS = ssvs_5_est,
           LASSO = lasso_5_est,
           Normal.Gamma = ng_5_est,
           Horseshoe = hs_5_est) %>% 
  pivot_longer(-true_values, names_to = "Model", values_to = "Estimate") %>% 
  mutate(Model = if_else(Model == "Normal.Gamma", "Normal-Gamma", Model)) %>% 
  group_by(Model) %>% 
  summarize(MSE = mean((Estimate - true_values)^2)) %>% 
  ungroup() %>% 
  arrange(MSE) %>% 
  kable(digits = 3)
# Pretty much a four-way tie


data.frame(true_values = beta_vals_5,
           BMA = bma_5_est,
           EMVS = emvs_5_est,
           SSVS = ssvs_5_est,
           Truth = beta_vals_5,
           LASSO = lasso_5_est,
           Normal.Gamma = ng_5_est,
           Horseshoe = hs_5_est) %>% 
  pivot_longer(-true_values, names_to = "Model", values_to = "Estimate") %>% 
  mutate(Model = if_else(Model == "Normal.Gamma", "Normal-Gamma", Model),
         true_values = factor(true_values)) %>% 
  group_by(Model, true_values) %>%
  summarize(Estimate = mean(Estimate)) %>% 
  ungroup() %>% 
  ggplot(aes(x = true_values, y = Estimate, fill = Model)) +
  geom_col(position = "dodge") + 
  theme_bw() +
  labs(x = "True Coefficient", title = "Model 5 Estimates") +
  theme(plot.title = element_text(hjust = 0.5),
        panel.grid.minor.x = element_blank(),
        panel.grid.major.x = element_blank()) +
  scale_fill_viridis(discrete = TRUE, option = "A", direction = -1)
```




## True/False Positive Rates

```{r}
#| label: function to calculate rates

calc_rates <- function(cm){
  fpr <- cm[1,2] / sum(cm[1,])
  fnr <- cm[2,1] / sum(cm[2,])
  #tpr <- 1 - fpr
  #tnr <- 1 - fnr
  tpr <- cm[2,2] / sum(cm[2,])
  tnr <- cm[1,1] / sum(cm[1,])
  return(c(TPR = tpr, FNR = fnr, FPR = fpr, TNR = tnr))
}
```


### Model 0

```{r}
#| label: tpr fpr model 0

true_betas_0 <- model_0$beta_true[2:11]
true_betas_0 <- as.integer(true_betas_0 != 0)

bvs_0_cm <- table(true_betas_0, bvs_0$HPMbin)

bma_0_out <- as.integer(estimates.bma(bma_0, order.by.pip = FALSE)[,1] > 0.5)
bma_0_cm <- table(true_betas_0, bma_0_out)

ssvs_0_out <- as.integer(colMeans(ssvs_0_g$beta[BURN_IN:N_SAMPLES_2,-1] != 0) > 0.5)
ssvs_0_cm <- table(true_betas_0, ssvs_0_out)

lasso_0_out <- as.integer(colMeans(lasso_0$beta[BURN_IN:N_SAMPLES,] != 0) > 0.5)
lasso_0_cm <- table(true_betas_0, lasso_0_out)

ng_0_out <- as.integer(colMeans(ng_0$beta[BURN_IN:N_SAMPLES,] != 0) > 0.5)
ng_0_cm <- table(true_betas_0, ng_0_out)

hs_0_out <- as.integer(colMeans(hs_0$beta[BURN_IN:N_SAMPLES,] != 0) > 0.5)
hs_0_cm <- table(true_betas_0, hs_0_out)

# No real need for thresholding here, it's obvious what performs best
emvs_0_out <- as.integer(emvs_0$prob_inclusion[1,] > 0.5)
emvs_0_cm <- table(true_betas_0, emvs_0_out)


X_0 <- model_0$X
y_0 <- model_0$y

X_0_test <- model_0_test$X
y_0_test <- model_0_test$y


model_0_rates <- rbind(
      c(Name = "BMS", Size = sum(bvs_0$HPMbin),
        calc_rates(bvs_0_cm), Runtime = "S",
        MSE_Nonzero = NA, MSE_All = NA, MSE_Resp = NA, MSE_Resp_Test = NA),
      c(Name = "BMA", Size = sum(bma_0_out),
        calc_rates(bma_0_cm), Runtime = "S",
        MSE_Nonzero = mean((bma_0_est[which(true_betas_0 == 1)] -
                             beta_vals_0[which(true_betas_0 == 1)])^2),
        MSE_All = mean((bma_0_est - beta_vals_0)^2),
        MSE_Resp = mean(((X_0 %*% as.matrix(c(-1, bma_0_est))) - y_0)^2),
        MSE_Resp_Test = mean(((X_0_test %*% as.matrix(c(-1, bma_0_est))) - y_0_test)^2)),
      c(Name = "SSVS", Size = sum(ssvs_0_out),
        calc_rates(ssvs_0_cm), Runtime = "S",
        MSE_Nonzero = mean((ssvs_0_est[which(true_betas_0 == 1)] -
                             beta_vals_0[which(true_betas_0 == 1)])^2),
        MSE_All = mean((ssvs_0_est - beta_vals_0)^2),
        MSE_Resp = mean(((X_0 %*% as.matrix(c(-1, ssvs_0_est))) - y_0)^2),
        MSE_Resp_Test = mean(((X_0_test %*% as.matrix(c(-1, ssvs_0_est))) - y_0_test)^2)),
      c(Name = "LASSO", Size = sum(lasso_0_out),
        calc_rates(lasso_0_cm), Runtime = "S",
        MSE_Nonzero = mean((lasso_0_est[which(true_betas_0 == 1)] -
                             beta_vals_0[which(true_betas_0 == 1)])^2),
        MSE_All = mean((lasso_0_est - beta_vals_0)^2),
        MSE_Resp = mean(((X_0 %*% as.matrix(c(-1, lasso_0_est))) - y_0)^2),
        MSE_Resp_Test = mean(((X_0_test %*% as.matrix(c(-1, lasso_0_est))) - y_0_test)^2)),
      c(Name = "Normal-Gamma", Size = sum(ng_0_out),
        calc_rates(ng_0_cm), Runtime = "S",
        MSE_Nonzero = mean((ng_0_est[which(true_betas_0 == 1)] -
                             beta_vals_0[which(true_betas_0 == 1)])^2),
        MSE_All = mean((ng_0_est - beta_vals_0)^2),
        MSE_Resp = mean(((X_0 %*% as.matrix(c(-1, ng_0_est))) - y_0)^2),
        MSE_Resp_Test = mean(((X_0_test %*% as.matrix(c(-1, ng_0_est))) - y_0_test)^2)),
      c(Name = "Horseshoe", Size = sum(hs_0_out),
        calc_rates(hs_0_cm), Runtime = "S",
        MSE_Nonzero = mean((hs_0_est[which(true_betas_0 == 1)] -
                             beta_vals_0[which(true_betas_0 == 1)])^2),
        MSE_All = mean((hs_0_est - beta_vals_0)^2),
        MSE_Resp = mean(((X_0 %*% as.matrix(c(-1, hs_0_est))) - y_0)^2),
        MSE_Resp_Test = mean(((X_0_test %*% as.matrix(c(-1, hs_0_est))) - y_0_test)^2)),
      c(Name = "EMVS", Size = sum(emvs_0_out),
        calc_rates(emvs_0_cm), Runtime = "Ms",
        MSE_Nonzero = mean((emvs_0_est[which(true_betas_0 == 1)] -
                             beta_vals_0[which(true_betas_0 == 1)])^2),
        MSE_All = mean((emvs_0_est - beta_vals_0)^2),
        MSE_Resp = mean(((X_0 %*% as.matrix(c(-1, emvs_0_est))) - y_0)^2),
        MSE_Resp_Test = mean(((X_0_test %*% as.matrix(c(-1, emvs_0_est))) - y_0_test)^2))
  )

save(model_0_rates, file = "model_0_rates.RData")
kable(model_0_rates, digits = 3)
```

### Model 1

```{r}
#| label: tpr fpr model 1

true_betas_1 <- model_1$beta_true[2:11]
true_betas_1 <- as.integer(true_betas_1 != 0)

bvs_1_cm <- table(true_betas_1, bvs_1$HPMbin)

bma_1_out <- as.integer(estimates.bma(bma_1, order.by.pip = FALSE)[,1] > 0.5)
bma_1_cm <- table(true_betas_1, bma_1_out)

ssvs_1_out <- as.integer(colMeans(ssvs_1_g$beta[BURN_IN:N_SAMPLES_2,-1] != 0) > 0.5)
ssvs_1_cm <- table(true_betas_1, ssvs_1_out)

lasso_1_out <- as.integer(colMeans(lasso_1$beta[BURN_IN:N_SAMPLES,] != 0) > 0.5)
lasso_1_cm <- table(true_betas_1, lasso_1_out)

ng_1_out <- as.integer(colMeans(ng_1$beta[BURN_IN:N_SAMPLES,] != 0) > 0.5)
ng_1_cm <- table(true_betas_1, ng_1_out)

hs_1_out <- as.integer(colMeans(hs_1$beta[BURN_IN:N_SAMPLES,] != 0) > 0.5)
hs_1_cm <- table(true_betas_1, hs_1_out)

emvs_1_out <- as.integer(emvs_1$prob_inclusion[1,] > 0.5)
emvs_1_cm <- table(true_betas_1, emvs_1_out)


model_1_rates <- rbind(
      c(Name = "BMS", Size = sum(bvs_1$HPMbin),
        calc_rates(bvs_1_cm), Runtime = "S",
        MSE_Nonzero = NA,
        MSE_All = NA),
      c(Name = "BMA", Size = sum(bma_1_out),
        calc_rates(bma_1_cm), Runtime = "S",
        MSE_Nonzero = mean((bma_1_est[which(true_betas_1 == 1)] -
                             beta_vals_1[which(true_betas_1 == 1)])^2),
        MSE_All = mean((bma_1_est - beta_vals_1)^2)),
      c(Name = "SSVS", Size = sum(ssvs_1_out),
        calc_rates(ssvs_1_cm), Runtime = "S",
        MSE_Nonzero = mean((ssvs_1_est[which(true_betas_1 == 1)] -
                             beta_vals_1[which(true_betas_1 == 1)])^2),
        MSE_All = mean((ssvs_1_est - beta_vals_1)^2)),
      c(Name = "LASSO", Size = sum(lasso_1_out),
        calc_rates(lasso_1_cm), Runtime = "S",
        MSE_Nonzero = mean((lasso_1_est[which(true_betas_1 == 1)] -
                             beta_vals_1[which(true_betas_1 == 1)])^2),
        MSE_All = mean((lasso_1_est - beta_vals_1)^2)),
      c(Name = "Normal-Gamma", Size = sum(ng_1_out),
        calc_rates(ng_1_cm), Runtime = "S",
        MSE_Nonzero = mean((ng_1_est[which(true_betas_1 == 1)] -
                             beta_vals_1[which(true_betas_1 == 1)])^2),
        MSE_All = mean((ng_1_est - beta_vals_1)^2)),
      c(Name = "Horseshoe", Size = sum(hs_1_out),
        calc_rates(hs_1_cm), Runtime = "S",
        MSE_Nonzero = mean((hs_1_est[which(true_betas_1 == 1)] -
                             beta_vals_1[which(true_betas_1 == 1)])^2),
        MSE_All = mean((hs_1_est - beta_vals_1)^2)),
      c(Name = "EMVS", Size = sum(emvs_1_out),
        calc_rates(emvs_1_cm), Runtime = "Ms",
        MSE_Nonzero = mean((emvs_1_est[which(true_betas_1 == 1)] -
                             beta_vals_1[which(true_betas_1 == 1)])^2),
        MSE_All = mean((emvs_1_est - beta_vals_1)^2))
  )

save(model_1_rates, file = "model_1_rates.RData")
kable(model_1_rates, digits = 3)
```

### Model 2

```{r}
#| label: tpr fpr model 2

true_betas_2 <- model_2$beta_true[-1]
true_betas_2 <- as.integer(true_betas_2 != 0)

bvs_2_cm <- table(true_betas_2, bvs_2$HPMbin)

bma_2_out <- as.integer(estimates.bma(bma_2, order.by.pip = FALSE)[,1] > 0.5)
bma_2_cm <- table(true_betas_2, bma_2_out)

ssvs_2_out <- as.integer(colMeans(ssvs_2_g$beta[BURN_IN:N_SAMPLES_2,-1] != 0) > 0.5)
ssvs_2_cm <- table(true_betas_2, ssvs_2_out)
ssvs_2_cm <- cbind(ssvs_2_cm, c(0, 0))

lasso_2_out <- as.integer(colMeans(lasso_2$beta[BURN_IN:N_SAMPLES,] != 0) > 0.5)
lasso_2_cm <- table(true_betas_2, lasso_2_out)

ng_2_out <- as.integer(colMeans(ng_2$beta[BURN_IN:N_SAMPLES,] != 0) > 0.5)
ng_2_cm <- table(true_betas_2, ng_2_out)

hs_2_out <- as.integer(colMeans(hs_2$beta[BURN_IN:N_SAMPLES,] != 0) > 0.5)
hs_2_cm <- table(true_betas_2, hs_2_out)

emvs_2_out <- as.integer(emvs_2$prob_inclusion[1,] > 0.5)
emvs_2_cm <- table(true_betas_2, emvs_2_out)


model_2_rates <- rbind(
      c(Name = "BMS", Size = sum(bvs_2$HPMbin),
        calc_rates(bvs_2_cm), Runtime = "Seconds",
        MSE_Nonzero = NA, MSE_All = NA),
      c(Name = "BMA", Size = sum(bma_2_out),
        calc_rates(bma_2_cm), Runtime = "Seconds",
        MSE_Nonzero = mean((bma_2_est[which(true_betas_2 == 1)] -
                             beta_vals_2[which(true_betas_2 == 1)])^2),
        MSE_All = mean((bma_2_est - beta_vals_2)^2)),
      c(Name = "SSVS", Size = sum(ssvs_2_out),
        calc_rates(ssvs_2_cm), Runtime = "Seconds",
        MSE_Nonzero = mean((ssvs_2_est[which(true_betas_2 == 1)] -
                             beta_vals_2[which(true_betas_2 == 1)])^2),
        MSE_All = mean((ssvs_2_est - beta_vals_2)^2)),
      c(Name = "LASSO", Size = sum(lasso_2_out),
        calc_rates(lasso_2_cm), Runtime = "Minutes",
        MSE_Nonzero = mean((lasso_2_est[which(true_betas_2 == 1)] -
                             beta_vals_2[which(true_betas_2 == 1)])^2),
        MSE_All = mean((lasso_2_est - beta_vals_2)^2)),
      c(Name = "Normal-Gamma", Size = sum(ng_2_out),
        calc_rates(ng_2_cm), Runtime = "Minutes",
        MSE_Nonzero = mean((ng_2_est[which(true_betas_2 == 1)] -
                             beta_vals_2[which(true_betas_2 == 1)])^2),
        MSE_All = mean((ng_2_est - beta_vals_2)^2)),
      c(Name = "Horseshoe", Size = sum(hs_2_out),
        calc_rates(hs_2_cm), Runtime = "Minutes",
        MSE_Nonzero = mean((hs_2_est[which(true_betas_2 == 1)] -
                             beta_vals_2[which(true_betas_2 == 1)])^2),
        MSE_All = mean((hs_2_est - beta_vals_2)^2)),
      c(Name = "EMVS", Size = sum(emvs_2_out),
        calc_rates(emvs_2_cm), runtime = "Milliseconds",
        MSE_Nonzero = mean((emvs_2_est[which(true_betas_2 == 1)] -
                             beta_vals_2[which(true_betas_2 == 1)])^2),
        MSE_All = mean((emvs_2_est - beta_vals_2)^2))
  )

save(model_2_rates, file = "model_2_rates.RData")
kable(model_2_rates, digits = 3)
```

I'm leaving out the thresholding here - they just want to pick all or none of the variables, which is kinda boring. Once again, it's just bad performance.

### Model 3

```{r}
#| label: tpr fpr model 3

true_betas_3 <- model_3$beta_true[-1]
true_betas_3 <- as.integer(true_betas_3 != 0)

bvs_3_cm <- table(true_betas_3, bvs_3$HPMbin)

bma_3_out <- as.integer(estimates.bma(bma_3, order.by.pip = FALSE)[,1] > 0.5)
bma_3_cm <- table(true_betas_3, bma_3_out)

ssvs_3_out <- as.integer(colMeans(ssvs_3_g$beta[BURN_IN:N_SAMPLES_2,-1] != 0) > 0.5)
ssvs_3_cm <- table(true_betas_3, ssvs_3_out)
ssvs_3_thresholded <- as.integer(colMeans(ssvs_3_g$beta[BURN_IN:N_SAMPLES_2,-1] != 0) > ssvs_3_threshold)
ssvs_3_thresh_cm <- table(true_betas_3, ssvs_3_thresholded)

lasso_3_out <- as.integer(colMeans(lasso_3$beta[BURN_IN:N_SAMPLES,] != 0) > 0.5)
lasso_3_cm <- table(true_betas_3, lasso_3_out)
lasso_3_thresholded <- as.integer(colMeans(lasso_3$beta[BURN_IN:N_SAMPLES,] != 0) > lasso_3_threshold)
lasso_3_thresh_cm <- table(true_betas_3, lasso_3_thresholded)

ng_3_out <- as.integer(colMeans(ng_3$beta[BURN_IN:N_SAMPLES,] != 0) > 0.5)
ng_3_cm <- table(true_betas_3, ng_3_out)
ng_3_thresholded <- as.integer(colMeans(ng_3$beta[BURN_IN:N_SAMPLES,] != 0) > ng_3_threshold)
ng_3_thresh_cm <- table(true_betas_3, ng_3_thresholded)

hs_3_out <- as.integer(colMeans(hs_3$beta[BURN_IN:N_SAMPLES,] != 0) > 0.5)
hs_3_cm <- table(true_betas_3, hs_3_out)
hs_3_thresholded <- as.integer(colMeans(hs_3$beta[BURN_IN:N_SAMPLES,] != 0) > hs_3_threshold)
hs_3_thresh_cm <- table(true_betas_3, hs_3_thresholded)

emvs_3_out <- as.integer(emvs_3$prob_inclusion[emvs_3_idx,] > 0.5)
emvs_3_cm <- table(true_betas_3, emvs_3_out)
emvs_3_thresholded <- as.integer(emvs_3$prob_inclusion[emvs_3_idx,] > emvs_3_thresh)
emvs_3_thresh_cm <- table(true_betas_3, emvs_3_thresholded)

X_3 <- model_3$X
y_3 <- model_3$y

X_3_test <- model_3_test$X
y_3_test <- model_3_test$y

model_3_rates <- rbind(
      c(Name = "BMS", Size = sum(bvs_3$HPMbin),
        calc_rates(bvs_3_cm), Runtime = "S",
        MSE_Nonzero = NA, MSE_All = NA,
        MSE_Resp = NA, MSE_Resp_Test = NA),
      c(Name = "BMA", Size = sum(bma_3_out),
        calc_rates(bma_3_cm), Runtime = "S",
        MSE_Nonzero = mean((bma_3_est[which(true_betas_3 == 1)] -
                             beta_vals_3[which(true_betas_3 == 1)])^2),
        MSE_All = mean((bma_3_est - beta_vals_3)^2),
        MSE_Resp = mean(((X_3 %*% as.matrix(c(-1, bma_3_est))) - y_3)^2),
        MSE_Resp_Test = mean(((X_3_test %*% as.matrix(c(-1, bma_3_est))) - y_3_test)^2)),
      c(Name = paste("SSVS, p =", round(ssvs_3_threshold, 3)),
        Size = sum(ssvs_3_thresholded),
        calc_rates(ssvs_3_thresh_cm), runtime = "S",
        MSE_Nonzero = mean((ssvs_3_est[which(true_betas_3 == 1)] -
                             beta_vals_3[which(true_betas_3 == 1)])^2),
        MSE_All = mean((ssvs_3_est - beta_vals_3)^2),
        MSE_Resp = mean(((X_3 %*% as.matrix(c(-1, ssvs_3_est))) - y_3)^2),
        MSE_Resp_Test = mean(((X_3_test %*% as.matrix(c(-1, ssvs_3_est))) - y_3_test)^2)),
      c(Name = paste("LASSO, p =", round(lasso_3_threshold, 3)),
        Size = sum(lasso_3_thresholded),
        calc_rates(lasso_3_thresh_cm), runtime = "M",
        MSE_Nonzero = mean((lasso_3_est[which(true_betas_3 == 1)] -
                             beta_vals_3[which(true_betas_3 == 1)])^2),
        MSE_All = mean((lasso_3_est - beta_vals_3)^2),
        MSE_Resp = mean(((X_3 %*% as.matrix(c(-1, lasso_3_est))) - y_3)^2),
        MSE_Resp_Test = mean(((X_3_test %*% as.matrix(c(-1, lasso_3_est))) - y_3_test)^2)),
      c(Name = paste("Normal-Gamma, p =", round(ng_3_threshold, 3)),
        Size = sum(ng_3_thresholded),
        calc_rates(ng_3_thresh_cm), runtime = "M",
        MSE_Nonzero = mean((ng_3_est[which(true_betas_3 == 1)] -
                             beta_vals_3[which(true_betas_3 == 1)])^2),
        MSE_All = mean((ng_3_est - beta_vals_3)^2),
        MSE_Resp = mean(((X_3 %*% as.matrix(c(-1, ng_3_est))) - y_3)^2),
        MSE_Resp_Test = mean(((X_3_test %*% as.matrix(c(-1, ng_3_est))) - y_3_test)^2)),
      c(Name = paste("Horseshoe, p =", round(hs_3_threshold, 3)),
        Size = sum(hs_3_thresholded),
        calc_rates(hs_3_thresh_cm), runtime = "M",
        MSE_Nonzero = mean((hs_3_est[which(true_betas_3 == 1)] -
                             beta_vals_3[which(true_betas_3 == 1)])^2),
        MSE_All = mean((hs_3_est - beta_vals_3)^2),
        MSE_Resp = mean(((X_3 %*% as.matrix(c(-1, hs_3_est))) - y_3)^2),
        MSE_Resp_Test = mean(((X_3_test %*% as.matrix(c(-1, hs_3_est))) - y_3_test)^2)),
      c(Name = paste("EMVS, p =", round(emvs_3_thresh, 3)),
        Size = sum(emvs_3_thresholded),
        calc_rates(emvs_3_thresh_cm), runtime = "Ms",
        MSE_Nonzero = mean((emvs_3_est[which(true_betas_3 == 1)] -
                             beta_vals_3[which(true_betas_3 == 1)])^2),
        MSE_All = mean((emvs_3_est - beta_vals_3)^2),
        MSE_Resp = mean(((X_3 %*% as.matrix(c(-1, emvs_3_est))) - y_3)^2),
        MSE_Resp_Test = mean(((X_3_test %*% as.matrix(c(-1, emvs_3_est))) - y_3_test)^2))
  )

save(model_3_rates, file = "model_3_rates.RData")
kable(model_3_rates, digits = 3)
```

### Model 4

```{r}
#| label: tpr fpr model 4

true_betas_4 <- model_4$beta_true[-1]
true_betas_4 <- as.integer(true_betas_4 != 0)

bvs_4_cm <- table(true_betas_4, bvs_4$HPMbin)

bma_4_out <- as.integer(estimates.bma(bma_4, order.by.pip = FALSE)[,1] > 0.5)
bma_4_cm <- table(true_betas_4, bma_4_out)

ssvs_4_out <- as.integer(colMeans(ssvs_4_g$beta[BURN_IN:N_SAMPLES_2,-1] != 0) > 0.5)
ssvs_4_cm <- table(true_betas_4, ssvs_4_out)
ssvs_4_thresholded <- as.integer(colMeans(ssvs_4_g$beta[BURN_IN:N_SAMPLES_2,-1] != 0) > ssvs_4_threshold)
ssvs_4_thresh_cm <- table(true_betas_4, ssvs_4_thresholded)

lasso_4_out <- as.integer(colMeans(lasso_4$beta[BURN_IN:N_SAMPLES,] != 0) > 0.5)
lasso_4_cm <- table(true_betas_4, lasso_4_out)
lasso_4_thresholded <- as.integer(colMeans(lasso_4$beta[BURN_IN:N_SAMPLES,] != 0) > lasso_4_threshold)
lasso_4_thresh_cm <- table(true_betas_4, lasso_4_thresholded)

ng_4_out <- as.integer(colMeans(ng_4$beta[BURN_IN:N_SAMPLES,] != 0) > 0.5)
ng_4_cm <- table(true_betas_4, ng_4_out)
ng_4_thresholded <- as.integer(colMeans(ng_4$beta[BURN_IN:N_SAMPLES,] != 0) > ng_4_threshold)
ng_4_thresh_cm <- table(true_betas_4, ng_4_thresholded)

hs_4_out <- as.integer(colMeans(hs_4$beta[BURN_IN:N_SAMPLES,] != 0) > 0.5)
hs_4_cm <- table(true_betas_4, hs_4_out)
hs_4_thresholded <- as.integer(colMeans(hs_4$beta[BURN_IN:N_SAMPLES,] != 0) > hs_4_threshold)
hs_4_thresh_cm <- table(true_betas_4, hs_4_thresholded)

emvs_4_out <- as.integer(emvs_4$prob_inclusion[emvs_4_idx,] > 0.5)
emvs_4_cm <- table(true_betas_4, emvs_4_out)
emvs_4_thresholded <- as.integer(emvs_4$prob_inclusion[emvs_4_idx,] > emvs_4_thresh)
emvs_4_thresh_cm <- table(true_betas_4, emvs_4_thresholded)

X_4 <- model_4$X
y_4 <- model_4$y

X_4_test <- model_4_test$X
y_4_test <- model_4_test$y

model_4_rates <- rbind(
      c(Name = "BMS", Size = sum(bvs_4$HPMbin),
        calc_rates(bvs_4_cm), Runtime = "S",
        MSE_Nonzero = NA, MSE_All = NA,
        MSE_Resp = NA, MSE_Resp_Test = NA),
      c(Name = "BMA", Size = sum(bma_4_out),
        calc_rates(bma_4_cm), Runtime = "S",
        MSE_Nonzero = mean((bma_4_est[which(true_betas_4 == 1)] -
                             beta_vals_4[which(true_betas_4 == 1)])^2),
        MSE_All = mean((bma_4_est - beta_vals_4)^2),
        MSE_Resp = mean(((X_4 %*% as.matrix(c(-1, bma_4_est))) - y_4)^2),
        MSE_Resp_Test = mean(((X_4_test %*% as.matrix(c(-1, bma_4_est))) - y_4_test)^2)),
      c(Name = paste("SSVS, p =", round(ssvs_4_threshold, 3)),
        Size = sum(ssvs_4_thresholded),
        calc_rates(ssvs_4_thresh_cm), runtime = "S",
        MSE_Nonzero = mean((ssvs_4_est[which(true_betas_4 == 1)] -
                             beta_vals_4[which(true_betas_4 == 1)])^2),
        MSE_All = mean((ssvs_4_est - beta_vals_4)^2),
        MSE_Resp = mean(((X_4 %*% as.matrix(c(-1, ssvs_4_est))) - y_4)^2),
        MSE_Resp_Test = mean(((X_4_test %*% as.matrix(c(-1, ssvs_4_est))) - y_4_test)^2)),
      c(Name = paste("LASSO, p =", round(lasso_4_threshold, 3)),
        Size = sum(lasso_4_thresholded),
        calc_rates(lasso_4_thresh_cm), runtime = "M",
        MSE_Nonzero = mean((lasso_4_est[which(true_betas_4 == 1)] -
                             beta_vals_4[which(true_betas_4 == 1)])^2),
        MSE_All = mean((lasso_4_est - beta_vals_4)^2),
        MSE_Resp = mean(((X_4 %*% as.matrix(c(-1, lasso_4_est))) - y_4)^2),
        MSE_Resp_Test = mean(((X_4_test %*% as.matrix(c(-1, lasso_4_est))) - y_4_test)^2)),
      c(Name = paste("Normal-Gamma, p =", round(ng_4_threshold, 3)),
        Size = sum(ng_4_thresholded),
        calc_rates(ng_4_thresh_cm), runtime = "M",
        MSE_Nonzero = mean((ng_4_est[which(true_betas_4 == 1)] -
                             beta_vals_4[which(true_betas_4 == 1)])^2),
        MSE_All = mean((ng_4_est - beta_vals_4)^2),
        MSE_Resp = mean(((X_4 %*% as.matrix(c(-1, ng_4_est))) - y_4)^2),
        MSE_Resp_Test = mean(((X_4_test %*% as.matrix(c(-1, ng_4_est))) - y_4_test)^2)),
      c(Name = paste("Horseshoe, p =", round(hs_4_threshold, 3)),
        Size = sum(hs_4_thresholded),
        calc_rates(hs_4_thresh_cm), runtime = "M",
        MSE_Nonzero = mean((hs_4_est[which(true_betas_4 == 1)] -
                             beta_vals_4[which(true_betas_4 == 1)])^2),
        MSE_All = mean((hs_4_est - beta_vals_4)^2),
        MSE_Resp = mean(((X_4 %*% as.matrix(c(-1, hs_4_est))) - y_4)^2),
        MSE_Resp_Test = mean(((X_4_test %*% as.matrix(c(-1, hs_4_est))) - y_4_test)^2)),
      c(Name = paste("EMVS, p =", round(emvs_4_thresh, 3)),
        Size = sum(emvs_4_thresholded),
        calc_rates(emvs_4_thresh_cm), runtime = "Ms",
        MSE_Nonzero = mean((emvs_4_est[which(true_betas_4 == 1)] -
                             beta_vals_4[which(true_betas_4 == 1)])^2),
        MSE_All = mean((emvs_4_est - beta_vals_4)^2),
        MSE_Resp = mean(((X_4 %*% as.matrix(c(-1, emvs_4_est))) - y_4)^2),
        MSE_Resp_Test = mean(((X_4_test %*% as.matrix(c(-1, emvs_4_est))) - y_4_test)^2))
  )

save(model_4_rates, file = "model_4_rates.RData")
kable(model_4_rates, digits = 3)
```


### Model 5

```{r}
#| label: tpr fpr model 5

true_betas_5 <- model_5$beta_true[-1]
true_betas_5 <- as.integer(true_betas_5 != 0)

bvs_5_cm <- table(true_betas_5, bvs_5$HPMbin)

bma_5_out <- as.integer(estimates.bma(bma_5, order.by.pip = FALSE)[,1] > 0.5)
bma_5_cm <- table(true_betas_5, bma_5_out)

ssvs_5_out <- as.integer(colMeans(ssvs_5_g$beta[BURN_IN:N_SAMPLES,-1] != 0) > 0.5)
ssvs_5_cm <- table(true_betas_5, ssvs_5_out)
ssvs_5_thresholded <- as.integer(colMeans(ssvs_5_g$beta[BURN_IN:N_SAMPLES,-1] != 0) > ssvs_5_threshold)
ssvs_5_thresh_cm <- table(true_betas_5, ssvs_5_thresholded)

lasso_5_out <- as.integer(colMeans(lasso_5$beta[BURN_IN:N_SAMPLES,] != 0) > 0.5)
lasso_5_cm <- table(true_betas_5, lasso_5_out)
lasso_5_thresholded <- as.integer(colMeans(lasso_5$beta[BURN_IN:N_SAMPLES,] != 0) > lasso_5_threshold)
lasso_5_thresh_cm <- table(true_betas_5, lasso_5_thresholded)

ng_5_out <- as.integer(colMeans(ng_5$beta[BURN_IN:N_SAMPLES,] != 0) > 0.5)
ng_5_cm <- table(true_betas_5, ng_5_out)
ng_5_thresholded <- as.integer(colMeans(ng_5$beta[BURN_IN:N_SAMPLES,] != 0) > ng_5_threshold)
ng_5_thresh_cm <- table(true_betas_5, ng_5_thresholded)

hs_5_out <- as.integer(colMeans(hs_5$beta[BURN_IN:N_SAMPLES,] != 0) > 0.5)
hs_5_cm <- table(true_betas_5, hs_5_out)
hs_5_thresholded <- as.integer(colMeans(hs_5$beta[BURN_IN:N_SAMPLES,] != 0) > hs_5_threshold)
hs_5_thresh_cm <- table(true_betas_5, hs_5_thresholded)

emvs_5_out <- as.integer(emvs_5$prob_inclusion[emvs_5_idx,] > 0.5)
emvs_5_cm <- table(true_betas_5, emvs_5_out)
emvs_5_thresholded <- as.integer(emvs_5$prob_inclusion[emvs_5_idx,] > emvs_5_thresh)
emvs_5_thresh_cm <- table(true_betas_5, emvs_5_thresholded)

X_5 <- model_5$X
y_5 <- model_5$y

X_5_test <- model_5_test$X
y_5_test <- model_5_test$y


model_5_rates <- rbind(
      c(Name = "BMS", Size = sum(bvs_5$HPMbin),
        calc_rates(bvs_5_cm), Runtime = "M",
        MSE_Nonzero = NA, MSE_All = NA,
        MSE_Resp = NA, MSE_Resp_Test = NA),
      c(Name = "BMA", Size = sum(bma_5_out),
        calc_rates(bma_5_cm), Runtime = "S",
        MSE_Nonzero = mean((bma_5_est[which(true_betas_5 == 1)] -
                             beta_vals_5[which(true_betas_5 == 1)])^2),
        MSE_All = mean((bma_5_est - beta_vals_5)^2),
        MSE_Resp = mean(((X_5 %*% as.matrix(c(-1, bma_5_est))) - y_5)^2),
        MSE_Resp_Test = mean(((X_5_test %*% as.matrix(c(-1, bma_5_est))) - y_5_test)^2)),
      c(Name = paste("SSVS, p =", round(ssvs_5_threshold, 3)),
        Size = sum(ssvs_5_thresholded),
        calc_rates(ssvs_5_thresh_cm), runtime = "M",
        MSE_Nonzero = mean((ssvs_5_est[which(true_betas_5 == 1)] -
                             beta_vals_5[which(true_betas_5 == 1)])^2),
        MSE_All = mean((ssvs_5_est - beta_vals_5)^2),
        MSE_Resp = mean(((X_5 %*% as.matrix(c(-1, ssvs_5_est))) - y_5)^2),
        MSE_Resp_Test = mean(((X_5_test %*% as.matrix(c(-1, ssvs_5_est))) - y_5_test)^2)),
      c(Name = paste("LASSO, p =", round(lasso_5_threshold, 3)),
        Size = sum(lasso_5_thresholded),
        calc_rates(lasso_5_thresh_cm), runtime = "H",
        MSE_Nonzero = mean((lasso_5_est[which(true_betas_5 == 1)] -
                             beta_vals_5[which(true_betas_5 == 1)])^2),
        MSE_All = mean((lasso_5_est - beta_vals_5)^2),
        MSE_Resp = mean(((X_5 %*% as.matrix(c(-1, lasso_5_est))) - y_5)^2),
        MSE_Resp_Test = mean(((X_5_test %*% as.matrix(c(-1, lasso_5_est))) - y_5_test)^2)),
      c(Name = paste("Normal-Gamma, p =", round(ng_5_threshold, 3)),
        Size = sum(ng_5_thresholded),
        calc_rates(ng_5_thresh_cm), runtime = "H",
        MSE_Nonzero = mean((ng_5_est[which(true_betas_5 == 1)] -
                             beta_vals_5[which(true_betas_5 == 1)])^2),
        MSE_All = mean((ng_5_est - beta_vals_5)^2),
        MSE_Resp = mean(((X_5 %*% as.matrix(c(-1, ng_5_est))) - y_5)^2),
        MSE_Resp_Test = mean(((X_5_test %*% as.matrix(c(-1, ng_5_est))) - y_5_test)^2)),
      c(Name = paste("Horseshoe, p =", round(hs_5_threshold, 3)),
        Size = sum(hs_5_thresholded),
        calc_rates(hs_5_thresh_cm), runtime = "H",
        MSE_Nonzero = mean((hs_5_est[which(true_betas_5 == 1)] -
                             beta_vals_5[which(true_betas_5 == 1)])^2),
        MSE_All = mean((hs_5_est - beta_vals_5)^2),
        MSE_Resp = mean(((X_5 %*% as.matrix(c(-1, hs_5_est))) - y_5)^2),
        MSE_Resp_Test = mean(((X_5_test %*% as.matrix(c(-1, hs_5_est))) - y_5_test)^2)),
      c(Name = paste("EMVS, p =", round(emvs_5_thresh, 3)),
        Size = sum(emvs_5_thresholded),
        calc_rates(emvs_5_thresh_cm), runtime = "Ms",
        MSE_Nonzero = mean((emvs_5_est[which(true_betas_5 == 1)] -
                             beta_vals_5[which(true_betas_5 == 1)])^2),
        MSE_All = mean((emvs_5_est - beta_vals_5)^2),
        MSE_Resp = mean(((X_5 %*% as.matrix(c(-1, emvs_5_est))) - y_5)^2),
        MSE_Resp_Test = mean(((X_5_test %*% as.matrix(c(-1, emvs_5_est))) - y_5_test)^2))
  )

save(model_5_rates, file = "model_5_rates.RData")
kable(model_5_rates, digits = 3)
```






# Long-Term Code Runs

```{r}
#| label: long-term code runs

y <- model_5$y
X <- model_5$X
data <- data.frame(y = y, X[,-1])

# SSVS
set.seed(465)
ssvs_start <- Sys.time()
ssvs_5_g <- lm.spike(y ~ .,
                   data = data,
                   niter = N_SAMPLES,
                   error.distribution = "gaussian")
ssvs_end <- Sys.time()

save(ssvs_5_g, file = "ssvs_5_g.RData")
ssvs_5_runtime <- c(ssvs_start, ssvs_end)
save(ssvs_5_runtime, file = "ssvs_5_runtime.RData")
print(ssvs_end)


X <- model_5$X[,-1]

# LASSO
set.seed(465)
lasso_start <- Sys.time()
lasso_5 <- blasso(X, y, T = N_SAMPLES, case = "default")
lasso_end <- Sys.time()

save(lasso_5, file = "lasso_5.RData")
lasso_5_runtime <- c(lasso_start, lasso_end)
save(lasso_5_runtime, file = "lasso_5_runtime.RData")
print(lasso_end)


# NG
set.seed(465)
ng_start <- Sys.time()
ng_5 <- blasso(X, y, T = N_SAMPLES, case = "ng")
ng_end <- Sys.time()

save(ng_5, file = "ng_5.RData")
ng_5_runtime <- c(ng_start, ng_end)
save(ng_5_runtime, file = "ng_5_runtime.RData")
print(ng_end)


# Horseshoe
set.seed(465)
hs_start <- Sys.time()
hs_5 <- blasso(X, y, T = N_SAMPLES, case = "hs")
hs_end <- Sys.time()

save(hs_5, file = "hs_5.RData")
hs_5_runtime <- c(hs_start, hs_end)
save(hs_5_runtime, file = "hs_5_runtime.RData")
print(hs_end)


# Subset Selection
# Model 2
set.seed(465)
X <- model_2$X
y <- model_2$y
model_2_fit <- bayeslm(y ~ X[,-1],
                       N = N_SAMPLES,
                       burnin = BURN_IN,
                       prior = "ridge")
temp <- post_predict(post_y_hat = tcrossprod(model_2_fit$beta, X),
                    post_sigma = model_2_fit$sigma,
                    yy = y)
post_y_pred = temp$post_y_pred
post_lpd = temp$post_lpd

bss_2_start <- Sys.time()
indicators_2 <- branch_and_bound(yy = fitted(model_2_fit),
                              XX = X)
bss_2_midpoint <- Sys.time()
accept_info_2 <- accept_family(post_y_pred = post_y_pred,
                            post_lpd = post_lpd,
                            XX = X,
                            indicators = indicators_2,
                            yy = y,
                            post_y_hat = tcrossprod(model_2_fit$beta, X))
bss_2_end <- Sys.time()

save(indicators_2, file = "indicators_2.RData")
save(accept_info_2, file = "accept_info_2.RData")
bss_2_runtime <- c(bss_2_start, bss_2_midpoint, bss_2_end)
save(bss_2_runtime, file = "bss_2_runtime.RData")
print(bss_2_end)


# Model 3
set.seed(465)
X <- model_3$X
y <- model_3$y
model_3_fit <- bayeslm(y ~ X[,-1],
                       N = N_SAMPLES,
                       burnin = BURN_IN,
                       prior = "ridge")
temp <- post_predict(post_y_hat = tcrossprod(model_3_fit$beta, X),
                    post_sigma = model_3_fit$sigma,
                    yy = y)
post_y_pred = temp$post_y_pred
post_lpd = temp$post_lpd

bss_3_start <- Sys.time()
indicators_3 <- branch_and_bound(yy = fitted(model_3_fit),
                              XX = X)
bss_3_midpoint <- Sys.time()
accept_info_3 <- accept_family(post_y_pred = post_y_pred,
                            post_lpd = post_lpd,
                            XX = X,
                            indicators = indicators_3,
                            yy = y,
                            post_y_hat = tcrossprod(model_3_fit$beta, X))
bss_3_end <- Sys.time()

save(indicators_3, file = "indicators_3.RData")
save(accept_info_3, file = "accept_info_3.RData")
bss_3_runtime <- c(bss_3_start, bss_3_midpoint, bss_3_end)
save(bss_3_runtime, file = "bss_3_runtime.RData")
print(bss_3_end)


# Model 4
set.seed(465)
X <- model_4$X
y <- model_4$y
model_4_fit <- bayeslm(y ~ X[,-1],
                       N = N_SAMPLES,
                       burnin = BURN_IN,
                       prior = "ridge")
temp <- post_predict(post_y_hat = tcrossprod(model_4_fit$beta, X),
                    post_sigma = model_4_fit$sigma,
                    yy = y)
post_y_pred = temp$post_y_pred
post_lpd = temp$post_lpd

bss_4_start <- Sys.time()
indicators_4 <- branch_and_bound(yy = fitted(model_4_fit),
                              XX = X)
bss_4_midpoint <- Sys.time()
accept_info_4 <- accept_family(post_y_pred = post_y_pred,
                            post_lpd = post_lpd,
                            XX = X,
                            indicators = indicators_4,
                            yy = y,
                            post_y_hat = tcrossprod(model_4_fit$beta, X))
bss_4_end <- Sys.time()

save(indicators_4, file = "indicators_4.RData")
save(accept_info_4, file = "accept_info_4.RData")
bss_4_runtime <- c(bss_4_start, bss_4_midpoint, bss_4_end)
save(bss_4_runtime, file = "bss_4_runtime.RData")
print(bss_4_end)


# Model 5
set.seed(465)
X <- model_5$X
y <- model_5$y
model_5_fit <- bayeslm(y ~ X[,-1],
                       N = N_SAMPLES,
                       burnin = BURN_IN,
                       prior = "ridge")
temp <- post_predict(post_y_hat = tcrossprod(model_5_fit$beta, X),
                    post_sigma = model_5_fit$sigma,
                    yy = y)
post_y_pred = temp$post_y_pred
post_lpd = temp$post_lpd

bss_5_start <- Sys.time()
indicators_5 <- branch_and_bound(yy = fitted(model_5_fit),
                              XX = X)
bss_5_midpoint <- Sys.time()
accept_info_5 <- accept_family(post_y_pred = post_y_pred,
                            post_lpd = post_lpd,
                            XX = X,
                            indicators = indicators_5,
                            yy = y,
                            post_y_hat = tcrossprod(model_5_fit$beta, X))
bss_5_end <- Sys.time()

save(indicators_5, file = "indicators_5.RData")
save(accept_info_5, file = "accept_info_5.RData")
bss_5_runtime <- c(bss_5_start, bss_5_midpoint, bss_5_end)
save(bss_5_runtime, file = "bss_5_runtime.RData")
print(bss_5_end)
```



