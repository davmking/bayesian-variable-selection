---
title: "Theory Notes"
author: "Dav King"
format: pdf
bibliography: references.bib
csl: apa.csl
---

```{r}
library(VGAM)
library(tidyverse)
library(viridis)
library(patchwork)
```


# Overview/Background

Variable selection is important in high-dimensional settings: We often expect that only a small handful of the predictors are actually associated with the outcome, but when we include many in the model, we have issues with computational complexity, sparse solutions, and potentially issues with finding variables significant which are not actually meaningful predictors.

Why is a Bayesian approach potentially better? Bayesian methods allow us to introduce prior information on the betas, which can help us introduce known structure into the variable selection setting and also stabilize inferences in high-dimensional settings [@lu2022].

I note that the introduction to [@rockova2013] has a lot of good background on this topic, and would be useful in explaining the setup for this presentation.

Another merit in this Bayesian approach: We can actually estimate the model parameter coefficients along with the estimate of which parameters should be included. This avoids the tempting but potentially problematic approach of simply selecting the most meaningful variables and removing the rest before actually computing the model, which eliminates some of the uncertainty from the model selection process [@rockova2013].



# Priors

## Spike-and-Slab Priors

The spike-and-slab prior is a two-point mixture on the $\beta_j$, which forces some of the $\beta_j$ to zero and estimates the coefficients of the others. The generic form of the spike-and-slab prior is

$$
  \beta_j | \gamma_j \sim \gamma_j \phi_1(\beta_j) + (1 - \gamma_j) \phi_0(\beta_j), \quad \mathbf{\gamma} \sim \pi(\mathbf{\gamma})
$$

In this case, $\phi_1(\beta_j)$ is a diffuse "slab distribution" so that the $\beta_j$ can reach their true coefficients, and $\phi_0(\beta_j)$ is a concentrated "spike distribution" pulling effects to 0, and $\gamma_j$ is a binary latent indicator representing the $2^p$ possible models [@lu2022].

### Stochastic Search Variable Selection (SSVS)

SSVS embeds the entire regression setup in a hierarchical Bayes normal mixture model, using latent variables to identify subset choices. Promising subsets of predictors have higher posterior probability, and Gibbs sampling can indirectly sample from the multinomial posterior distribution on the set of possible subset choices. Subsets with higher probability are identified by their more frequent appearance in the Gibbs sample, avoiding the problem of calculating the posterior probabilities for all $2^p$ subsets. Frequently, this converges quickly to near-optimal solutions [@george1993].

According to George (1993), this can be represented using latent variable $\gamma_j \in \{0, 1\}$ with

$$
  \beta_j | \gamma_j \sim (1 - \gamma_j) N(0, \tau_j^2) + \gamma_j N(0, c_j^2 \tau_j^2)
$$


```{r}
tau <- 1
c <- 4

slab <- rnorm(100000, 0, tau^2)
spike <- rnorm(100000, 0, c^2 * tau^2)

data.frame(slab, spike) %>% 
  pivot_longer(everything()) %>% 
  mutate(name = factor(name, levels = c("spike", "slab"))) %>% 
  ggplot(aes(x = value, fill = name)) +
  geom_density(alpha = 0.5) +
  theme_bw() +
  labs(x = "Beta", y = "Density", fill = "Distribution",
       title = "Plot of SSVS Spike-and-Slab Distribution",
       subtitle = "tau = 1, c = 4") +
  theme(plot.title = element_text(hjust = 0.5),
        plot.subtitle = element_text(hjust = 0.5),
        panel.grid = element_blank()) +
  scale_fill_viridis(discrete = TRUE, direction = -1) +
  scale_y_continuous(labels = scales::percent_format())
```


### Normal Mixture of Inverse Gamma (NMIG)

Given in [@fahrmeir2010] and Ishwaran and Rao (2003, 2005, find citations), The hierarchical prior for $\beta_j | \tau_j^2$ is

$$
  \beta_j | \tau_j^2 \sim N(0, \tau_j^2) \\
  \tau_j | \gamma_j \sim (1 - \gamma_j) \text{IG}(a_\tau, \nu_0 b_\tau) + \gamma_j \text{IG}(a_\tau, \nu_1 b_\tau)
$$

By placing the spike and slab priors on the variances instead of the coefficients themselves, we can have some robustness against tuning parameters [@lu2022].

```{r}
nu_0 <- 1e-2
nu_1 <- 0.2

a_tau <- 1
b_tau <- 1

N_samples <- 100000

spike_tau <- 1 / rgamma(N_samples, a_tau, nu_0 * b_tau)
slab_tau <- 1 / rgamma(N_samples, a_tau, nu_1 * b_tau)

spike_beta <- rnorm(N_samples, 0, spike_tau)
slab_beta <- rnorm(N_samples, 0, slab_tau)

#data.frame(spike_tau, slab_tau, spike_beta, slab_beta) %>% 
  #pivot_longer(everything()) %>% 
  #separate(name, into = c("distribution", "level"), sep = "_") %>% 
  #ggplot(aes(x = value, fill = distribution)) +
  #geom_density(alpha = 0.3) +
  #facet_wrap(~level, scales = "free") +
  #coord_cartesian(xlim = c(-100, 100))


nmig_tau <- data.frame(spike_tau, slab_tau) %>% 
  pivot_longer(everything()) %>% 
  mutate(name = substring(name, 1, nchar(name) - 4)) %>% 
  mutate(name = factor(name, levels = c("spike", "slab"))) %>% 
  ggplot(aes(x = value, fill = name)) +
  geom_density(alpha = 0.5) +
  coord_cartesian(xlim = c(0, 200)) +
  theme_bw() +
  labs(x = "Tau", y = "Density", fill = "Distribution",
       title = "Plot of NMIG Tau Spike-and-Slab Distribution",
       subtitle = "nu0 = 0.01, nu1 = 0.2") +
  theme(plot.title = element_text(hjust = 0.5),
        plot.subtitle = element_text(hjust = 0.5),
        panel.grid = element_blank()) +
  scale_fill_viridis(discrete = TRUE, direction = -1)


nmig_beta <- data.frame(spike_beta, slab_beta) %>% 
  pivot_longer(everything()) %>% 
  mutate(name = substring(name, 1, nchar(name) - 5)) %>% 
  mutate(name = factor(name, levels = c("spike", "slab"))) %>% 
  ggplot(aes(x = value, fill = name)) +
  geom_density(alpha = 0.5) +
  coord_cartesian(xlim = c(-150, 150)) +
  theme_bw() +
  labs(x = "Beta", y = "Density", fill = "Distribution",
       title = "Plot of NMIG Beta Spike-and-Slab Distribution",
       subtitle = "nu0 = 0.01, nu1 = 0.2") +
  theme(plot.title = element_text(hjust = 0.5),
        plot.subtitle = element_text(hjust = 0.5),
        panel.grid = element_blank()) +
  scale_fill_viridis(discrete = TRUE, direction = -1)

nmig_tau + nmig_beta
```


## Shrinkage Priors

Shrinkage priors focus on simply pulling some of the coefficients towards zero, while retaining the stron geffects with minimum penalization. Shrinkage priors are continuous, as opposed to the spike-and-slab prior. As noted in [@bhadra2017], there are many settings (such as genomics) where many effects are negligible but not zero; this creates an argument for one-group global-local shrinkage priors, rather than two-group spike-and-slab priors.

### LASSO Prior

Originally written about in [@park2008], we can write the Laplace prior as

$$
  \beta_j | \tau_j \sim N(0, \sigma^2 \tau_j^2) \\
  \tau_j^2 | \lambda \sim \text{exp}(\lambda^2 / 2)
$$

We can Gibbs sample the $\lambda$ parameter, instead of having to do cross-validation.

```{r}
lambda <- c(1, 3, 5, 10)
sigma_2 <- 1

N_samples <- 100000

tau_values <- matrix(NA, nrow = N_samples, ncol = length(lambda))
beta_values <- matrix(NA, nrow = N_samples, ncol = length(lambda))

for(l in 1:length(lambda)){
  tau_values[,l] <- rexp(N_samples, lambda[l]^2 / 2)
}

for(l in 1:length(lambda)){
  beta_values[,l] <- rnorm(N_samples, 0, sigma_2 * tau_values[,l])
}

lasso_tau <- data.frame(tau_values) %>%
  pivot_longer(everything(), names_to = "lambda_number") %>% 
  mutate(lambda_number = as.integer(substring(lambda_number, 2))) %>% 
  mutate(lambda = lambda[lambda_number]) %>% 
  mutate(lambda = factor(lambda)) %>% 
  ggplot(aes(x = value, fill = lambda)) +
  geom_density(alpha = 0.5) +
  coord_cartesian(xlim = c(0, .5)) +
  theme_bw() +
  labs(x = "Tau", y = "Density", fill = "Lambda",
       title = "LASSO Tau by Lambda") +
  theme(plot.title = element_text(hjust = 0.5),
        panel.grid = element_blank()) +
  scale_fill_viridis(discrete = TRUE, option = "B", direction = -1)


lasso_beta <- data.frame(beta_values) %>% 
  pivot_longer(everything(), names_to = "lambda_number") %>% 
  mutate(lambda_number = as.integer(substring(lambda_number, 2))) %>% 
  mutate(lambda = lambda[lambda_number]) %>% 
  mutate(lambda = factor(lambda)) %>% 
  ggplot(aes(x = value, fill = lambda)) +
  geom_density(alpha = 0.5) +
  coord_cartesian(xlim = c(-1, 1)) +
  theme_bw() +
  labs(x = "Beta", y = "Density", fill = "Lambda",
       title = "LASSO Beta by Lambda") +
  theme(plot.title = element_text(hjust = 0.5),
        panel.grid = element_blank()) +
  scale_fill_viridis(discrete = TRUE, option = "B", direction = -1)

lasso_tau + lasso_beta
```

Note to myself: This might actually be why our sampler isn't working. Let's incorporate this into the actual thesis.


### Normal-Gamma Prior

This is similar to the LASSO regression, but solves some of the problems. Bayesian LASSO is suboptimal: the shrinkage effect is too weak for small coefficients. The Normal-Gamma assumes the mxing distribution in the scale normal of mixture has a gamma distribution, given by:

$$
  \beta_j | \tau_j \sim N(0, \tau_j^2) \\
  \tau_j^2 \sim \text{gamma}(\lambda, 1/(2\xi^2))
$$

In this case, both hyperparameters are important. Per [@griffin2010], this results in a distribution that places a lot of mass close to 0, but also has heavy tails, particularly as $\lambda$ decreases. In many ways, this can be considered similar to a spike-and-slab prior with the right formulation.


### Horseshoe Prior

The horseshoe class of priors uses a global hyperparameter to shrink all coefficients towards zero, with a local hyperparameter to allow some coefficients to adjust the scale of shrinkage at the local level. These are known as **global-local** shrinkage priors. It can be represented as a scale mixture of normals,

$$
  \beta_j | \kappa_j \sim N(0, \kappa_j^2) \\
  \kappa_j | \tau \sim C^+(0, \tau) \\
  \tau | \sigma \sim C^+(0, \sigma)
$$

In this case, $C^+(0, \tau)$ is a half-Cauchy distribution for the standard deviation $\kappa_j$. The $\kappa_j$ is referred to as the local shrinkage parameter, while $\tau$ is gthe global shrinkage parameter.

```{r}
tau <- 0.1

N_samples <- 100000

kappa <- rcauchy(N_samples * 2, 0, tau)
kappa <- kappa[kappa > 0]

beta <- rnorm(length(kappa), 0, kappa^2)

horseshoe_kappa <- ggplot(mapping = aes(x = kappa)) +
  geom_density(fill = "grey") +
  coord_cartesian(xlim = c(0, quantile(kappa, 0.9975))) +
  theme_bw() +
  labs(x = "Kappa", y = "Density", title = "Horseshoe Kappa",
       subtitle = "Tau = 0.1") +
  theme(plot.title = element_text(hjust = 0.5),
        plot.subtitle = element_text(hjust = 0.5),
        panel.grid = element_blank())

horseshoe_beta <- ggplot(mapping = aes(x = beta)) +
  geom_density(fill = "grey") +
  coord_cartesian(xlim = quantile(beta, c(.00001, .99999))) +
  theme_bw() +
  labs(x = "Beta", y = "Density", title = "Horseshoe Beta",
       subtitle = "Tau = 0.1") +
  theme(plot.title = element_text(hjust = 0.5),
        plot.subtitle = element_text(hjust = 0.5),
        panel.grid = element_blank())

horseshoe_kappa + horseshoe_beta
```

Going to need to set a seed for this one, it has absurdly different outcomes based on the run.

Per [@carvalho2010], this model has benefits in not needing to select any hyperparameters, since all unknowns have fully specified priors. This is different from most similar approaches in shrinkage models. The prior remains robust and highly adaptive even in the absence of these hyperparameters. The other benefit, compared to other models, is that it converges to the correct answer in sparse situations extremely quickly, while still demonstrating strong robustness against noise for obvious signals, which it leaves unshrunk.


### Horseshoe+ Prior

An extension of the horseshoe prior is the horseshoe+ prior,

$$
  \beta_j | \kappa_j \sim N(0, \kappa_j^2) \\
  \kappa_j | \tau, \eta_j \sim C^+(0, \tau \eta_j) \\
  \eta_j \sim C^+(0, 1)
$$

This extra latent variable provides an extra layer of local shrinkage, which is (in practice) often better in performance in terms of both MSE and computation time when dealing with ultra-sparse signals.

In terms of signals, the regression setting for noisy signals differentiates signals ($\kappa_i = 0$) from noise ($\kappa_i = 1$). The horseshoe+ prior has a "horseshoe" shape pushing posterior mass to either $\kappa_i = 0$ or $\kappa_i = 1$, which the horseshoe prior does not.


### Dirichlet-Laplace Prior

This is another global-local shrinkage prior:

$$
  \beta_j | \sigma^2, \phi_j, \psi_j, \tau \sim N(0, \sigma^2 \phi_j^2 \psi_j \tau^2) \\
  \psi_j \sim \text{exp}(1/2) \\
  (\phi_1, \dots, \phi_p) \sim \text{Dir}(a, \dots, a) \\
  \tau \sim \text{gamma}(pa, 1/2)
$$

In this case, $\phi_j$ is the local shrinkage parameter, whereas $\tau$ is the global shrinkage parameter. Dir$(a, \dots, a)$ is the Dirichlet distribution with concentration vector $(a, \dots, a)$. This can be a hyperprior, but we can also choose: $a = 1/n$ if $p > n$ or there is a strong correlation between covariates; $a = 1/2$ when $p$ is small and there is only moderate correlation between covariates.

```{r}
p <- 10
a <- 1/2
sigma_2 <- 1

N_samples <- 100000

psi <- rexp(N_samples, 1/2)
phi <- rdiric(N_samples, a)
tau <- rgamma(N_samples, p * a, 1/2)
beta <- rnorm(N_samples, 0, sigma_2 * psi * phi * tau^2)

dl_psi <- ggplot(mapping = aes(x = psi)) +
  geom_density(fill = "grey") +
  theme_bw() +
  labs(x = "Psi", y = "Density", title = "Psi",
       subtitle = "Dirichlet-Laplace Prior") +
  theme(plot.title = element_text(hjust = 0.5),
        plot.subtitle = element_text(hjust = 0.5),
        panel.grid = element_blank())

ggplot(mapping = aes(x = phi)) +
  geom_density(fill = "grey")
```

This might not be super possible to plot, due to the nature of a Dirichlet distribution in high dimensions. We will see. There is a good paper [@bhattacharya2014], but I am not going to explain the nonsense happening with this distribution unless it somehow becomes demonstrably the best performing test.


## Hybrid Priors

### SSLASSO

The SSLASSO process considers a mixture of two Laplace distributions, where $\phi_1(\beta_j) = \frac{\lambda_1}{2} \text{exp} \{-\lambda_1 |\beta_j| \}$ with small $\lambda_1$ and $\lambda_0(\beta_j) = \frac{\lambda_0}{2} \text{exp}\{-\lambda_0 |\beta_j|\}$ with large $\lambda_0$. Using a binomial prior on $\gamma$,

$$
  \pi(\mathbf{\beta} | \mathbf{\gamma}) = \prod_{i = 1}^{p} \left [ \gamma_j \phi_1 (\beta_j) + (1 - \gamma_j) \phi_0(\beta_j) \right ]
$$

If $\phi_1(\beta_j) = \phi_0(\beta_j)$, we have the standard Lasso penalty.


# Other Methods

## Bayesian Model Averaging

In BMA, you essentially have a vector of latent variables that says whether or not each beta should be included in the model, and you sample this along with your other variables in the MCMC sampler. Remarkably, averaging over the outputs from these has shockingly good performance, and often provides a good estimate for the true model parameters without doing the full $2^p$ search. Many descriptions of this and similar approaches can be found in [@hoeting1999].


## Best Subset Selection

This is talked about extensively in [@kowal2022]. This approach de-empasizes *best* subset selection, which is often unstable under permutations of the data, and instead emphasizes *acceptable* or *good enough* subset selection, which is more stable and also simplifies the computation time. They use a modified Branch-and-Bound Algorithm (BBA) to implement this in the Bayesian setting. This approach focuses on the $\ell_0$ norm, rather than $\ell_1$ or $\ell_2$.


# EMVS

The EM Variable Selection process is a deterministic alternative to MCMC stochastic search which rapidly identifies promising candidate models. Useful in the $p > n$ setting, it frequently outperforms stochastic search from MCMC greatly in terms of computation time [@rockova2013]. EMVS uses a spike-and-slab prior, but focuses on two steps: Estimating the expected value of the latent variables given current parameter estimates, and then maximizing the likelihood with respect to the parameters using this expectation.

EMVS is also useful because of its computational efficiency: It is possible to search over a much wider set of parameters effectively. You can change what your priors look like, putting (for example) more weight on the spike.

Sometimes, the EM algorithm fails to converge to the global maximum if it is initialized near local modes. A recommendation is to run the algorithm for a wide choice of starting values. Additionally, you can try the deterministic annealing EM algorithm (DEAM), which uses the free energy function and starts with high temperatures to smooth away local modes, only lowering the temperature later to expose the actual posterior.

## Algorithm

ChatGPT prompt: "Explain, in detail, the EMVS algorithm for Bayesian variable selection. Assume I am generally familiar with concepts in Bayesian statistics, but am entirely unfamiliar with the EM algorithm."

### Setup

Consider the basic linear regression setting:

$$
  y = X\beta + \epsilon, \quad \epsilon \sim N(0, \sigma^2I)
$$

In this setting, $y$ is the $n \times 1$ vector of responses, $X$ is the $n \times p$ matrix of inputs, $\beta$ is the $p \times 1$ vector of coefficients, and $\sigma^2$ is the error variance. Then we can assign a spike-and-slab prior to each $\beta_j$:

$$
  \beta_j \sim (1 - \gamma_j) \phi_0(\beta_j) + \gamma_j \phi_1(\beta_j), \quad \gamma_j \in \{0, 1\}
$$

where $\gamma_j$ is an inclusion indicator for the $j$-th variable, $\phi_0$ is some function governing the spike (maybe expressed $N(0, \tau_0^2)$), and $\phi_1$ is some function governing the slab (maybe expressed $N(0, \tau_1^2)$). The prior for $\gamma_j$ is typically Bernoulli with parameter $\pi$.


### Initialization

Initialize parameters, including $\gamma_j$, $\beta$, and $\sigma^2$.


### Expectation (E) Step

Calculate the posterior inclusion probabilities for each predictor:

$$
  \omega_j = \mathbb{P}(\gamma_j = 1 | y, \theta)
$$

where $\theta$ denotes the current parameter estiamtes. Using Bayes' rule, this involves

$$
  \omega_j = \frac{\pi \phi_1(\beta_j)}{\pi \phi_1(\beta_j) + (1 - \pi)\phi_0(\beta_j)}
$$

### Maximization (M) Step

Update the parameter estimates by maximizing the expected complete data log-likelihood:

$$
  \hat{\beta} = \text{arg max}_\beta \mathbb{E}[\ell(\beta, \gamma | y)]
$$

### Conclusion

Iterate until convergence, probably based on minimal changes in the log-likelihood. After convergence, select variables with high posterior inclusion probabilities.




\newpage

# References


